{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox as mdpt, numpy as np\n",
    "import mdptoolbox.example\n",
    "import MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, given a transition function and discount rate, we generate a random reward function over all transitions. We then sparsify the reward function by setting some proportion (e.g. 10%) of the transition values to 0. We then generate the optimal policy for said reward function (using, for instance, policy iteration). We now attempt to build a model that can predict the sparsity used to generate the optimal policy given the transition function, discount rate, and policy itself, but *not* the reward function, as otherwise the problem would be trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a bunch of MDPs with different parameters, sparsity\n",
    "\n",
    "NUM_MDPs = 100\n",
    "NUM_STATES = 10\n",
    "NUM_ACTIONS = 4\n",
    "\n",
    "def get_transition_matrix(num_states, num_actions, generator = np.random.dirichlet):\n",
    "    P = np.zeros((num_actions, num_states, num_states)) # (A, S, S) shape\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            P[a, s, :] = generator(np.ones(num_states))\n",
    "    return P\n",
    "\n",
    "def get_reward_matrix(num_states, num_actions, sparsity = 0.0, generator = np.random.normal):\n",
    "    R = np.zeros((num_states, num_actions))\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            if np.random.rand() < sparsity:\n",
    "                R[s, a] = 0\n",
    "            else:\n",
    "                R[s, a] = generator()\n",
    "    return R\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "EPSILON = 0.01\n",
    "MAX_ITER = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparsity levels generated by generate_tests are divided using arange from 0 to 1 and then scrambled randomly, meaning that in effect each sparsity level in the training and test sets is sampled uniformly from [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests(num_mdps = NUM_MDPs, sparsity_levels = None, mdp_generator = mdpt.mdp.PolicyIteration):\n",
    "    \"\"\"\n",
    "    Generate a bunch of MDPs with different sparsity levels, and return the sparsity levels and the MDPs\n",
    "\n",
    "    Args:\n",
    "        sparsity_levels: a list of sparsity levels to generate MDPs with\n",
    "    Returns:\n",
    "        sparsity_levels: the sparsity levels used to generate the MDPs, in the same order as the MDPs\n",
    "        MDPS: an array of MDPs\n",
    "    \"\"\"\n",
    "    sparsity_levels = sparsity_levels if sparsity_levels is not None else np.arange(num_mdps) / num_mdps\n",
    "    sparsity_copy = sparsity_levels.copy() # defensive copy\n",
    "    np.random.shuffle(sparsity_copy)\n",
    "    MDPS = np.array([mdp_generator(\n",
    "        get_transition_matrix(NUM_STATES, NUM_ACTIONS), \n",
    "        get_reward_matrix(NUM_STATES, NUM_ACTIONS, sparsity_copy[i]), \n",
    "        DISCOUNT, max_iter = MAX_ITER) \n",
    "        for i in range(num_mdps)\n",
    "    ])\n",
    "    return sparsity_copy, MDPS\n",
    "\n",
    "sparsity_levels, MDPS = generate_tests()\n",
    "for mdp in MDPS:\n",
    "    mdp.run()\n",
    "    # print(mdp.policy) # debug\n",
    "# print(MDPS[0].policy) # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a classifier to predict sparsity level from a policy\n",
    "### Idea 1: hack-y heuristics\n",
    "\n",
    "def heuristic_classifier(MDP, policy):\n",
    "    \"\"\"\n",
    "    A heuristic classifier that predicts the sparsity level of an MDP's reward function given its \n",
    "    optimal policy\n",
    "    1. \n",
    "    \"\"\"\n",
    "    # TODO: implement this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 0.0408 - mae: 0.1630 - val_loss: 0.0357 - val_mae: 0.1572\n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 0.0351 - mae: 0.1513 - val_loss: 0.0381 - val_mae: 0.1652\n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 12s 5ms/step - loss: 0.0341 - mae: 0.1486 - val_loss: 0.0372 - val_mae: 0.1641\n",
      "Epoch 4/100\n",
      "2500/2500 [==============================] - 9s 4ms/step - loss: 0.0336 - mae: 0.1472 - val_loss: 0.0353 - val_mae: 0.1573\n",
      "Epoch 5/100\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 0.0330 - mae: 0.1455 - val_loss: 0.0333 - val_mae: 0.1490\n",
      "Epoch 6/100\n",
      "2500/2500 [==============================] - 7s 3ms/step - loss: 0.0326 - mae: 0.1445 - val_loss: 0.0345 - val_mae: 0.1547\n",
      "Epoch 7/100\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 0.0323 - mae: 0.1440 - val_loss: 0.0364 - val_mae: 0.1616\n",
      "Epoch 8/100\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 0.0319 - mae: 0.1427 - val_loss: 0.0346 - val_mae: 0.1546\n",
      "Mean squared error: 0.033806095979522444\n",
      "Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\n"
     ]
    }
   ],
   "source": [
    "### Idea 2: neural network\n",
    "# Thanks again ChatGPT for outlining the code structure\n",
    "\n",
    "sparsity, MDPs = generate_tests(100000)\n",
    "# print(np.array(MDPs[0].P).shape)\n",
    "training_data = [(np.array(mdp.P), mdp.discount, mdp.policy, sparsity[i]) for i, mdp in enumerate(MDPs)]\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Step 1: Feature extraction function\n",
    "def extract_features(transition_function, discount_rate, optimal_policy):\n",
    "    \"\"\"\n",
    "    Extract features from the MDP's transition function, discount rate, and optimal policy\n",
    "    \"\"\"\n",
    "    # opt_policy = optimal_policy.reshape(-1, 1)  # Reshape for sklearn which expects 2D input\n",
    "\n",
    "    # # Initialize the OneHotEncoder\n",
    "    # encoder = OneHotEncoder(sparse=False)  # Use sparse=False to get a dense array\n",
    "\n",
    "    # # Fit and transform\n",
    "    # opt_policy_one_hot = encoder.fit_transform(opt_policy)\n",
    "    features = np.concatenate((transition_function.flatten(), [discount_rate], optimal_policy.flatten()))\n",
    "    # print(features.shape)\n",
    "    # length 10*10*4 + 1 + 10 = 411\n",
    "\n",
    "    # Placeholder features\n",
    "    # features = np.random.rand(411)\n",
    "    return features\n",
    "\n",
    "# Step 2: Data preparation (assuming you have your data in an appropriate format)\n",
    "# This is a placeholder function - you would replace it with actual data loading and processing\n",
    "def prepare_data(training_data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for transition_function, discount_rate, optimal_policy, sparsity_level in training_data:\n",
    "        features.append(extract_features(transition_function, discount_rate, optimal_policy))\n",
    "        labels.append(sparsity_level)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Step 3: Model selection\n",
    "\n",
    "def build_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='linear')  # Linear activation for regression output\n",
    "    ])\n",
    "\n",
    "    # Num parameters: 411*64 + 64 + 64*64 + 64 + 64*64 + 64 + 64*1 + 1 = 26497\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='mean_squared_error',  # Suitable for regression\n",
    "                  metrics=['mae'])  # Mean Absolute Error as an additional metric\n",
    "    # ``loss\" refers to training data, ``val_loss\" refers to validation data\n",
    "    return model\n",
    "\n",
    "features, labels = prepare_data(training_data)\n",
    "# Example: features shape is (num_samples, num_features), adjust 'input_dim' accordingly\n",
    "input_dim = features.shape[1]  # Assuming 'features' is already defined and preprocessed\n",
    "\n",
    "model = build_model(input_dim)\n",
    "\n",
    "# Training the model\n",
    "model.fit(features, labels, epochs=100, validation_split=0.2, verbose = 1, \n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])\n",
    "\n",
    "# Don't forget to preprocess your new data before making predictions\n",
    "# predicted_sparsity = model.predict(new_features)\n",
    "\n",
    "# Step 4: Training the model (placeholder for training data)\n",
    "# training_data = load_your_data_somehow()\n",
    "# features, labels = prepare_data(training_data)\n",
    "# model.fit(features, labels)\n",
    "\n",
    "# Step 5: Prediction function\n",
    "def predict_sparsity(transition_function, discount_rate, optimal_policy):\n",
    "    features = extract_features(transition_function, discount_rate, optimal_policy).reshape(1, -1)\n",
    "    predicted_sparsity = model(features) # more efficient than .predict() for single samples\n",
    "    return predicted_sparsity\n",
    "\n",
    "# Note: The actual training step and data preparation would depend on your specific dataset and environment setup.\n",
    "test_sparsity, test_MDPs = generate_tests()\n",
    "test_data = [(np.array(mdp.P), mdp.discount, mdp.policy) for mdp in (test_MDPs)]\n",
    "NUM_TESTS = 1000\n",
    "mse = np.zeros(NUM_TESTS)\n",
    "\n",
    "for i in range(min(NUM_TESTS, len(test_data))):\n",
    "    transition_function, discount_rate, optimal_policy = test_data[i]\n",
    "    prediction = predict_sparsity(transition_function, discount_rate, optimal_policy)\n",
    "    mse[i] = (prediction - test_sparsity[i])**2\n",
    "    # print(f\"Predicted sparsity level for MDP {i}: {prediction}, actual sparsity level: {test_sparsity[i]}, Squared error: {mse[i]}\")\n",
    "\n",
    "print(f\"Mean squared error: {np.mean(mse)}\")\n",
    "print(\"Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a control, when the input layer (with same dimension as transition_function + discount rate + optimal policy) is randomized, MSE = ~0.115\n",
    "- I should also note that I'm choosing hyperparameters here in a rather unprincipled way by guess-timating their effects on the model\n",
    "- The loss seems to settle around 0.033 after ~20% into each epoch when given 10^5 training points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30965 0.36155 0.52631 ... 0.76905 0.8053  0.19016]\n",
      "Mean squared error: 0.03445141010174199\n",
      "Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\n",
      "Mean absolute error: 0.14673268830908204\n"
     ]
    }
   ],
   "source": [
    "### Idea 3: Multiple linear regression \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "sparsity, MDPs = generate_tests(100000)\n",
    "# print(np.array(MDPs[0].P).shape)\n",
    "training_data = [(np.array(mdp.P), mdp.discount, mdp.policy, sparsity[i]) for i, mdp in enumerate(MDPs)]\n",
    "print(sparsity)\n",
    "features, labels = prepare_data(training_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean squared error: {mse}\")\n",
    "print(\"Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\")\n",
    "print(f\"Mean absolute error: {mae}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
