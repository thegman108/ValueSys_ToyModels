{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox as mdpt, numpy as np\n",
    "import mdptoolbox.example\n",
    "import MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, given a transition function and discount rate, we generate a random reward function over all transitions. We then sparsify the reward function by setting some proportion (e.g. 10%) of the transition values to 0. We then generate the optimal policy for said reward function (using, for instance, policy iteration). We now attempt to build a model that can predict the sparsity used to generate the optimal policy given the transition function, discount rate, and policy itself, but *not* the reward function, as otherwise the problem would be trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a bunch of MDPs with different parameters, sparsity\n",
    "\n",
    "NUM_MDPs = 100\n",
    "NUM_STATES = 100\n",
    "NUM_ACTIONS = 4\n",
    "\n",
    "def get_transition_matrix(num_states, num_actions, generator = np.random.dirichlet):\n",
    "    \"\"\"\n",
    "    Returns a transition matrix for a given number of states and actions\n",
    "    \n",
    "    Returns:\n",
    "        P: (num_actions, num_states, num_states) array, where P[a, s, s'] is the probability of \n",
    "        transitioning from state s to state s' given action a\n",
    "    \"\"\"\n",
    "    P = np.zeros((num_actions, num_states, num_states)) # (A, S, S) shape\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            P[a, s, :] = generator(np.ones(num_states))\n",
    "    return P\n",
    "\n",
    "def get_reward_matrix(num_states, num_actions, sparsity = 0.0, generator = np.random.normal):\n",
    "    \"\"\"\n",
    "    Returns a reward matrix for a given number of states and actions\n",
    "    \"\"\"\n",
    "    R = np.zeros((num_states, num_actions))\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            if np.random.rand() < sparsity:\n",
    "                R[s, a] = 0\n",
    "            else:\n",
    "                R[s, a] = generator()\n",
    "    return R\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "EPSILON = 0.01\n",
    "MAX_ITER = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparsity levels generated by generate_tests are divided using arange from 0 to 1 and then scrambled randomly, meaning that in effect each sparsity level in the training and test sets is sampled uniformly from [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests(num_mdps = NUM_MDPs, sparsity_levels = None, mdp_generator = mdpt.mdp.PolicyIteration):\n",
    "    \"\"\"\n",
    "    Generate a bunch of MDPs with different sparsity levels, and return the sparsity levels and the MDPs\n",
    "\n",
    "    Args:\n",
    "        sparsity_levels: a list of sparsity levels to generate MDPs with\n",
    "    Returns:\n",
    "        sparsity_levels: the sparsity levels used to generate the MDPs, in the same order as the MDPs\n",
    "        MDPS: an array of MDPs\n",
    "    \"\"\"\n",
    "    sparsity_levels = sparsity_levels if sparsity_levels is not None else np.arange(num_mdps) / num_mdps\n",
    "    sparsity_copy = sparsity_levels.copy() # defensive copy\n",
    "    np.random.shuffle(sparsity_copy)\n",
    "    MDPS = np.array([mdp_generator(\n",
    "        get_transition_matrix(NUM_STATES, NUM_ACTIONS), \n",
    "        get_reward_matrix(NUM_STATES, NUM_ACTIONS, sparsity_copy[i]), \n",
    "        DISCOUNT, max_iter = MAX_ITER) \n",
    "        for i in range(num_mdps)\n",
    "    ])\n",
    "    return sparsity_copy, MDPS\n",
    "\n",
    "sparsity_levels, MDPS = generate_tests()\n",
    "for mdp in MDPS:\n",
    "    mdp.run()\n",
    "    # print(mdp.policy) # debug\n",
    "# print(MDPS[0].policy) # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a classifier to predict sparsity level from a policy\n",
    "### Idea 1: hack-y heuristics\n",
    "\n",
    "def heuristic_classifier(MDP, policy):\n",
    "    \"\"\"\n",
    "    A heuristic classifier that predicts the sparsity level of an MDP's reward function given its \n",
    "    optimal policy\n",
    "    1. \n",
    "    \"\"\"\n",
    "    # TODO: implement this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25/25 [==============================] - 3s 47ms/step - loss: 0.2056 - mae: 0.3619 - val_loss: 0.0569 - val_mae: 0.2046\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 1s 44ms/step - loss: 0.0539 - mae: 0.1812 - val_loss: 0.0118 - val_mae: 0.0895\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.0376 - mae: 0.1530 - val_loss: 0.0189 - val_mae: 0.1217\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 2s 77ms/step - loss: 0.0285 - mae: 0.1315 - val_loss: 0.0273 - val_mae: 0.1485\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 2s 63ms/step - loss: 0.0249 - mae: 0.1252 - val_loss: 0.0180 - val_mae: 0.1167\n",
      "Predicted sparsity level for MDP 0: 0.26282399892807007, actual sparsity level: 0.09, Squared error: 0.02986813336610794\n",
      "Predicted sparsity level for MDP 1: 0.30783629417419434, actual sparsity level: 0.4, Squared error: 0.008494149893522263\n",
      "Predicted sparsity level for MDP 2: 0.2889615595340729, actual sparsity level: 0.04, Squared error: 0.06198186054825783\n",
      "Predicted sparsity level for MDP 3: 0.31293389201164246, actual sparsity level: 0.33, Squared error: 0.00029125247965566814\n",
      "Predicted sparsity level for MDP 4: 0.38812950253486633, actual sparsity level: 0.56, Squared error: 0.02953946962952614\n",
      "Predicted sparsity level for MDP 5: 0.639725923538208, actual sparsity level: 0.86, Squared error: 0.04852067679166794\n",
      "Predicted sparsity level for MDP 6: 0.4345489740371704, actual sparsity level: 0.55, Squared error: 0.013328942470252514\n",
      "Predicted sparsity level for MDP 7: 0.8153687715530396, actual sparsity level: 0.93, Squared error: 0.013140319846570492\n",
      "Predicted sparsity level for MDP 8: 0.4179702699184418, actual sparsity level: 0.61, Squared error: 0.03687542304396629\n",
      "Predicted sparsity level for MDP 9: 0.45985904335975647, actual sparsity level: 0.68, Squared error: 0.048462044447660446\n",
      "Predicted sparsity level for MDP 10: 0.31923559308052063, actual sparsity level: 0.11, Squared error: 0.043779533356428146\n",
      "Predicted sparsity level for MDP 11: 0.5897519588470459, actual sparsity level: 0.79, Squared error: 0.040099285542964935\n",
      "Predicted sparsity level for MDP 12: 0.7162778377532959, actual sparsity level: 0.76, Squared error: 0.0019116266630589962\n",
      "Predicted sparsity level for MDP 13: 0.32191553711891174, actual sparsity level: 0.34, Squared error: 0.00032704792101867497\n",
      "Predicted sparsity level for MDP 14: 0.4142102301120758, actual sparsity level: 0.6, Squared error: 0.03451784700155258\n",
      "Predicted sparsity level for MDP 15: 0.27253258228302, actual sparsity level: 0.06, Squared error: 0.045170098543167114\n",
      "Predicted sparsity level for MDP 16: 0.2475004494190216, actual sparsity level: 0.02, Squared error: 0.05175645649433136\n",
      "Predicted sparsity level for MDP 17: 0.6135768890380859, actual sparsity level: 0.71, Squared error: 0.009297411888837814\n",
      "Predicted sparsity level for MDP 18: 0.23438674211502075, actual sparsity level: 0.07, Squared error: 0.02702300250530243\n",
      "Predicted sparsity level for MDP 19: 0.30786579847335815, actual sparsity level: 0.21, Squared error: 0.009577715769410133\n",
      "Predicted sparsity level for MDP 20: 0.3304733335971832, actual sparsity level: 0.18, Squared error: 0.022642221301794052\n",
      "Predicted sparsity level for MDP 21: 0.34028974175453186, actual sparsity level: 0.28, Squared error: 0.003634852822870016\n",
      "Predicted sparsity level for MDP 22: 0.4564008116722107, actual sparsity level: 0.73, Squared error: 0.07485652714967728\n",
      "Predicted sparsity level for MDP 23: 0.24149158596992493, actual sparsity level: 0.08, Squared error: 0.026079533621668816\n",
      "Predicted sparsity level for MDP 24: 0.33173710107803345, actual sparsity level: 0.42, Squared error: 0.007790336851030588\n",
      "Predicted sparsity level for MDP 25: 0.6460754871368408, actual sparsity level: 0.8, Squared error: 0.02369275875389576\n",
      "Predicted sparsity level for MDP 26: 0.302883118391037, actual sparsity level: 0.44, Squared error: 0.018801039084792137\n",
      "Predicted sparsity level for MDP 27: 0.8130319118499756, actual sparsity level: 0.94, Squared error: 0.016120893880724907\n",
      "Predicted sparsity level for MDP 28: 0.5774786472320557, actual sparsity level: 0.74, Squared error: 0.026413192972540855\n",
      "Predicted sparsity level for MDP 29: 0.8404673337936401, actual sparsity level: 0.96, Squared error: 0.014288052916526794\n",
      "Predicted sparsity level for MDP 30: 0.2457513064146042, actual sparsity level: 0.54, Squared error: 0.08658229559659958\n",
      "Predicted sparsity level for MDP 31: 0.1736740916967392, actual sparsity level: 0.22, Squared error: 0.002146089682355523\n",
      "Predicted sparsity level for MDP 32: 0.2050599902868271, actual sparsity level: 0.17, Squared error: 0.0012292028404772282\n",
      "Predicted sparsity level for MDP 33: 0.6654868125915527, actual sparsity level: 0.85, Squared error: 0.03404512628912926\n",
      "Predicted sparsity level for MDP 34: 0.5357959270477295, actual sparsity level: 0.5, Squared error: 0.0012813484063372016\n",
      "Predicted sparsity level for MDP 35: 0.2729713022708893, actual sparsity level: 0.2, Squared error: 0.005324810743331909\n",
      "Predicted sparsity level for MDP 36: 0.8188894987106323, actual sparsity level: 0.91, Squared error: 0.008301128633320332\n",
      "Predicted sparsity level for MDP 37: 0.5873534679412842, actual sparsity level: 0.77, Squared error: 0.033359747380018234\n",
      "Predicted sparsity level for MDP 38: 0.42671212553977966, actual sparsity level: 0.47, Squared error: 0.0018738399958238006\n",
      "Predicted sparsity level for MDP 39: 0.2918894588947296, actual sparsity level: 0.35, Squared error: 0.003376834327355027\n",
      "Predicted sparsity level for MDP 40: 0.2519782483577728, actual sparsity level: 0.51, Squared error: 0.06657522171735764\n",
      "Predicted sparsity level for MDP 41: 0.273001492023468, actual sparsity level: 0.13, Squared error: 0.020449427887797356\n",
      "Predicted sparsity level for MDP 42: 0.26281502842903137, actual sparsity level: 0.49, Squared error: 0.051613014191389084\n",
      "Predicted sparsity level for MDP 43: 0.8005454540252686, actual sparsity level: 0.89, Squared error: 0.008002113550901413\n",
      "Predicted sparsity level for MDP 44: 0.2670184373855591, actual sparsity level: 0.43, Squared error: 0.026562992483377457\n",
      "Predicted sparsity level for MDP 45: 0.2957821190357208, actual sparsity level: 0.36, Squared error: 0.0041239382699131966\n",
      "Predicted sparsity level for MDP 46: 0.35634514689445496, actual sparsity level: 0.05, Squared error: 0.09384734183549881\n",
      "Predicted sparsity level for MDP 47: 0.7588995695114136, actual sparsity level: 0.84, Squared error: 0.0065772756934165955\n",
      "Predicted sparsity level for MDP 48: 0.2617136836051941, actual sparsity level: 0.03, Squared error: 0.053691230714321136\n",
      "Predicted sparsity level for MDP 49: 0.5958538055419922, actual sparsity level: 0.75, Squared error: 0.023761048913002014\n",
      "Predicted sparsity level for MDP 50: 0.4792356789112091, actual sparsity level: 0.64, Squared error: 0.025845162570476532\n",
      "Predicted sparsity level for MDP 51: 0.5457843542098999, actual sparsity level: 0.65, Squared error: 0.010860895738005638\n",
      "Predicted sparsity level for MDP 52: 0.3928687870502472, actual sparsity level: 0.59, Squared error: 0.03886070474982262\n",
      "Predicted sparsity level for MDP 53: 0.2746593952178955, actual sparsity level: 0.1, Squared error: 0.03050590679049492\n",
      "Predicted sparsity level for MDP 54: 0.23788630962371826, actual sparsity level: 0.0, Squared error: 0.056589897722005844\n",
      "Predicted sparsity level for MDP 55: 0.3290081322193146, actual sparsity level: 0.32, Squared error: 8.11465724837035e-05\n",
      "Predicted sparsity level for MDP 56: 0.4088995158672333, actual sparsity level: 0.48, Squared error: 0.005055277142673731\n",
      "Predicted sparsity level for MDP 57: 0.4209960699081421, actual sparsity level: 0.57, Squared error: 0.02220216952264309\n",
      "Predicted sparsity level for MDP 58: 0.36347952485084534, actual sparsity level: 0.37, Squared error: 4.251665814081207e-05\n",
      "Predicted sparsity level for MDP 59: 0.2871771454811096, actual sparsity level: 0.25, Squared error: 0.0013821400934830308\n",
      "Predicted sparsity level for MDP 60: 0.3440357744693756, actual sparsity level: 0.29, Squared error: 0.002919865772128105\n",
      "Predicted sparsity level for MDP 61: 0.5329834222793579, actual sparsity level: 0.67, Squared error: 0.018773546442389488\n",
      "Predicted sparsity level for MDP 62: 0.35543277859687805, actual sparsity level: 0.45, Squared error: 0.008942957036197186\n",
      "Predicted sparsity level for MDP 63: 0.8332253694534302, actual sparsity level: 0.97, Squared error: 0.018707307055592537\n",
      "Predicted sparsity level for MDP 64: 0.21569669246673584, actual sparsity level: 0.24, Squared error: 0.000590650481171906\n",
      "Predicted sparsity level for MDP 65: 0.6723513603210449, actual sparsity level: 0.82, Squared error: 0.021800119429826736\n",
      "Predicted sparsity level for MDP 66: 0.6591418981552124, actual sparsity level: 0.78, Squared error: 0.014606674201786518\n",
      "Predicted sparsity level for MDP 67: 0.40601646900177, actual sparsity level: 0.19, Squared error: 0.04666311666369438\n",
      "Predicted sparsity level for MDP 68: 0.9078514575958252, actual sparsity level: 0.98, Squared error: 0.0052054147236049175\n",
      "Predicted sparsity level for MDP 69: 0.821958065032959, actual sparsity level: 0.9, Squared error: 0.00609053997322917\n",
      "Predicted sparsity level for MDP 70: 0.7352935075759888, actual sparsity level: 0.87, Squared error: 0.018145840615034103\n",
      "Predicted sparsity level for MDP 71: 0.34295961260795593, actual sparsity level: 0.41, Squared error: 0.0044944132678210735\n",
      "Predicted sparsity level for MDP 72: 0.8319724798202515, actual sparsity level: 0.95, Squared error: 0.013930493034422398\n",
      "Predicted sparsity level for MDP 73: 0.49252820014953613, actual sparsity level: 0.7, Squared error: 0.04304454103112221\n",
      "Predicted sparsity level for MDP 74: 0.25823384523391724, actual sparsity level: 0.26, Squared error: 3.1192689675663132e-06\n",
      "Predicted sparsity level for MDP 75: 0.5363197326660156, actual sparsity level: 0.72, Squared error: 0.033738452941179276\n",
      "Predicted sparsity level for MDP 76: 0.21124324202537537, actual sparsity level: 0.23, Squared error: 0.0003518161247484386\n",
      "Predicted sparsity level for MDP 77: 0.8022581338882446, actual sparsity level: 0.92, Squared error: 0.013863150961697102\n",
      "Predicted sparsity level for MDP 78: 0.3505115211009979, actual sparsity level: 0.14, Squared error: 0.04431509971618652\n",
      "Predicted sparsity level for MDP 79: 0.3375578224658966, actual sparsity level: 0.58, Squared error: 0.058778200298547745\n",
      "Predicted sparsity level for MDP 80: 0.2966967821121216, actual sparsity level: 0.27, Squared error: 0.0007127175922505558\n",
      "Predicted sparsity level for MDP 81: 0.4992775321006775, actual sparsity level: 0.53, Squared error: 0.0009438683046028018\n",
      "Predicted sparsity level for MDP 82: 0.4407864212989807, actual sparsity level: 0.52, Squared error: 0.006274788174778223\n",
      "Predicted sparsity level for MDP 83: 0.7781403064727783, actual sparsity level: 0.88, Squared error: 0.010375396348536015\n",
      "Predicted sparsity level for MDP 84: 0.24783125519752502, actual sparsity level: 0.01, Squared error: 0.05656370520591736\n",
      "Predicted sparsity level for MDP 85: 0.24403169751167297, actual sparsity level: 0.15, Squared error: 0.008841958828270435\n",
      "Predicted sparsity level for MDP 86: 0.2551506459712982, actual sparsity level: 0.3, Squared error: 0.0020114656072109938\n",
      "Predicted sparsity level for MDP 87: 0.31549885869026184, actual sparsity level: 0.38, Squared error: 0.004160396754741669\n",
      "Predicted sparsity level for MDP 88: 0.6315219402313232, actual sparsity level: 0.83, Squared error: 0.039393533021211624\n",
      "Predicted sparsity level for MDP 89: 0.5131158828735352, actual sparsity level: 0.63, Squared error: 0.013661895878612995\n",
      "Predicted sparsity level for MDP 90: 0.5121912956237793, actual sparsity level: 0.62, Squared error: 0.0116227176040411\n",
      "Predicted sparsity level for MDP 91: 0.4938715100288391, actual sparsity level: 0.69, Squared error: 0.03846638277173042\n",
      "Predicted sparsity level for MDP 92: 0.41993358731269836, actual sparsity level: 0.46, Squared error: 0.0016053180443122983\n",
      "Predicted sparsity level for MDP 93: 0.48172274231910706, actual sparsity level: 0.39, Squared error: 0.008413064293563366\n",
      "Predicted sparsity level for MDP 94: 0.9084053039550781, actual sparsity level: 0.99, Squared error: 0.006657695863395929\n",
      "Predicted sparsity level for MDP 95: 0.6304821968078613, actual sparsity level: 0.66, Squared error: 0.0008713022689335048\n",
      "Predicted sparsity level for MDP 96: 0.22961238026618958, actual sparsity level: 0.16, Squared error: 0.004845884162932634\n",
      "Predicted sparsity level for MDP 97: 0.6738916635513306, actual sparsity level: 0.81, Squared error: 0.01852547936141491\n",
      "Predicted sparsity level for MDP 98: 0.25447940826416016, actual sparsity level: 0.31, Squared error: 0.003082536393776536\n",
      "Predicted sparsity level for MDP 99: 0.25879520177841187, actual sparsity level: 0.12, Squared error: 0.019264107570052147\n",
      "Mean squared error: 0.021436910873603665\n",
      "Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\n"
     ]
    }
   ],
   "source": [
    "### Idea 2: neural network\n",
    "# Thanks again ChatGPT for outlining the code structure\n",
    "\n",
    "sparsity, MDPs = generate_tests(1000)\n",
    "# print(np.array(MDPs[0].P).shape)\n",
    "training_data = [(np.array(mdp.P), mdp.discount, mdp.policy, sparsity[i]) for i, mdp in enumerate(MDPs)]\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Step 1: Feature extraction function\n",
    "def extract_features(transition_function, discount_rate, optimal_policy):\n",
    "    \"\"\"\n",
    "    Extract features from the MDP's transition function, discount rate, and optimal policy\n",
    "    \"\"\"\n",
    "    # opt_policy = optimal_policy.reshape(-1, 1)  # Reshape for sklearn which expects 2D input\n",
    "\n",
    "    # # Initialize the OneHotEncoder\n",
    "    # encoder = OneHotEncoder(sparse=False)  # Use sparse=False to get a dense array\n",
    "\n",
    "    # # Fit and transform\n",
    "    # opt_policy_one_hot = encoder.fit_transform(opt_policy)\n",
    "    features = np.concatenate((transition_function.flatten(), [discount_rate], optimal_policy.flatten()))\n",
    "    # print(features.shape)\n",
    "    # length 10*10*4 + 1 + 10 = 411\n",
    "\n",
    "    # Placeholder features\n",
    "    # features = np.random.rand(411)\n",
    "    return features\n",
    "\n",
    "# Step 2: Data preparation (assuming you have your data in an appropriate format)\n",
    "# This is a placeholder function - you would replace it with actual data loading and processing\n",
    "def prepare_data(training_data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for transition_function, discount_rate, optimal_policy, sparsity_level in training_data:\n",
    "        features.append(extract_features(transition_function, discount_rate, optimal_policy))\n",
    "        labels.append(sparsity_level)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Step 3: Model selection\n",
    "\n",
    "def build_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='linear')  # Linear activation for regression output\n",
    "    ])\n",
    "\n",
    "    # Num parameters: 4011*64 + 64 + 64*64 + 64 + 64*64 + 64 + 64*1 + 1 = 265,153\n",
    "    # Num data points: 10000\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='mean_squared_error',  # Suitable for regression\n",
    "                  metrics=['mae'])  # Mean Absolute Error as an additional metric\n",
    "    # ``loss\" refers to training data, ``val_loss\" refers to validation data\n",
    "    return model\n",
    "\n",
    "features, labels = prepare_data(training_data)\n",
    "# Example: features shape is (num_samples, num_features), adjust 'input_dim' accordingly\n",
    "input_dim = features.shape[1]  # Assuming 'features' is already defined and preprocessed\n",
    "\n",
    "model = build_model(input_dim)\n",
    "\n",
    "# Training the model\n",
    "model.fit(features, labels, epochs=100, validation_split=0.2, verbose = 1, \n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])\n",
    "\n",
    "# Don't forget to preprocess your new data before making predictions\n",
    "# predicted_sparsity = model.predict(new_features)\n",
    "\n",
    "# Step 4: Training the model (placeholder for training data)\n",
    "# training_data = load_your_data_somehow()\n",
    "# features, labels = prepare_data(training_data)\n",
    "# model.fit(features, labels)\n",
    "\n",
    "# Step 5: Prediction function\n",
    "def predict_sparsity(transition_function, discount_rate, optimal_policy):\n",
    "    features = extract_features(transition_function, discount_rate, optimal_policy).reshape(1, -1)\n",
    "    predicted_sparsity = model(features) # more efficient than .predict() for single samples\n",
    "    return predicted_sparsity\n",
    "\n",
    "# Testing model\n",
    "test_sparsity, test_MDPs = generate_tests()\n",
    "test_data = [(np.array(mdp.P), mdp.discount, mdp.policy) for mdp in (test_MDPs)]\n",
    "NUM_TESTS = 1000\n",
    "mse = np.zeros(min(NUM_TESTS, len(test_data)))\n",
    "\n",
    "for i in range(min(NUM_TESTS, len(test_data))):\n",
    "    transition_function, discount_rate, optimal_policy = test_data[i]\n",
    "    prediction = predict_sparsity(transition_function, discount_rate, optimal_policy)[0][0]\n",
    "    mse[i] = (prediction - test_sparsity[i])**2\n",
    "    print(f\"Predicted sparsity level for MDP {i}: {prediction}, actual sparsity level: {test_sparsity[i]}, Squared error: {mse[i]}\")\n",
    "\n",
    "print(f\"Mean squared error: {np.mean(mse)}\")\n",
    "print(\"Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ten actions:\n",
    "- As a control, when the input layer (with same dimension as transition_function + discount rate + optimal policy) is randomized, MSE = ~0.115\n",
    "- I should also note that I'm choosing hyperparameters here in a rather unprincipled way by guess-timating their effects on the model\n",
    "- The loss seems to settle around 0.033 after ~20% into each epoch when given 10^5 training points \n",
    "\n",
    "With 100 actions:\n",
    "- Model does slightly better (now ~0.028); maybe patterns in MDP/sparsity become more apparent with more states?\n",
    "- In terms of computation, generating the MDPs takes a lot longer than training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.103 0.369 0.111 0.719 0.327 0.034 0.525 0.414 0.193 0.026 0.998 0.905\n",
      " 0.294 0.841 0.114 0.519 0.944 0.005 0.014 0.697 0.118 0.123 0.594 0.415\n",
      " 0.829 0.624 0.792 0.777 0.32  0.676 0.106 0.421 0.893 0.456 0.602 0.776\n",
      " 0.312 0.495 0.405 0.176 0.5   0.05  0.137 0.686 0.044 0.206 0.825 0.87\n",
      " 0.44  0.437 0.64  0.241 0.404 0.145 0.151 0.217 0.806 0.128 0.945 0.949\n",
      " 0.451 0.692 0.989 0.936 0.231 0.423 0.319 0.51  0.856 0.372 0.723 0.371\n",
      " 0.117 0.435 0.886 0.373 0.887 0.827 0.313 0.671 0.75  0.452 0.937 0.504\n",
      " 0.446 0.916 0.465 0.959 0.168 0.309 0.375 0.66  0.115 0.007 0.629 0.076\n",
      " 0.121 0.74  0.59  0.025 0.253 0.668 0.974 0.416 0.18  0.926 0.917 0.374\n",
      " 0.902 0.731 0.342 0.049 0.275 0.971 0.367 0.636 0.052 0.984 0.718 0.346\n",
      " 0.506 0.251 0.635 0.45  0.673 0.918 0.209 0.615 0.337 0.234 0.964 0.127\n",
      " 0.659 0.287 0.746 0.13  0.767 0.996 0.029 0.544 0.302 0.757 0.281 0.116\n",
      " 0.71  0.868 0.646 0.889 0.575 0.857 0.675 0.448 0.34  0.666 0.872 0.037\n",
      " 0.993 0.227 0.616 0.381 0.388 0.427 0.264 0.412 0.224 0.543 0.192 0.842\n",
      " 0.869 0.623 0.761 0.897 0.061 0.239 0.789 0.909 0.985 0.184 0.338 0.819\n",
      " 0.086 0.564 0.977 0.238 0.61  0.358 0.063 0.627 0.419 0.724 0.246 0.915\n",
      " 0.074 0.91  0.158 0.793 0.835 0.164 0.235 0.833 0.276 0.401 0.83  0.379\n",
      " 0.791 0.747 0.299 0.102 0.492 0.883 0.082 0.522 0.778 0.722 0.957 0.295\n",
      " 0.965 0.621 0.683 0.252 0.988 0.898 0.288 0.813 0.376 0.322 0.557 0.98\n",
      " 0.17  0.881 0.568 0.347 0.771 0.283 0.216 0.742 0.877 0.073 0.713 0.786\n",
      " 0.744 0.626 0.488 0.764 0.824 0.196 0.057 0.133 0.146 0.351 0.956 0.707\n",
      " 0.538 0.882 0.991 0.002 0.622 0.53  0.15  0.212 0.422 0.52  0.126 0.899\n",
      " 0.695 0.39  0.716 0.403 0.47  0.65  0.498 0.89  0.178 0.874 0.46  0.225\n",
      " 0.554 0.987 0.109 0.848 0.507 0.143 0.047 0.9   0.906 0.384 0.814 0.385\n",
      " 0.105 0.644 0.849 0.79  0.341 0.663 0.35  0.983 0.831 0.181 0.885 0.528\n",
      " 0.805 0.402 0.04  0.82  0.667 0.173 0.711 0.324 0.259 0.535 0.549 0.497\n",
      " 0.107 0.762 0.317 0.505 0.604 0.438 0.031 0.583 0.198 0.449 0.153 0.22\n",
      " 0.961 0.328 0.672 0.531 0.642 0.992 0.318 0.409 0.795 0.743 0.023 0.844\n",
      " 0.54  0.698 0.144 0.459 0.947 0.424 0.125 0.584 0.768 0.749 0.464 0.12\n",
      " 0.807 0.765 0.473 0.799 0.021 0.323 0.596 0.561 0.361 0.758 0.933 0.478\n",
      " 0.86  0.378 0.922 0.732 0.316 0.019 0.652 0.878 0.155 0.613 0.817 0.682\n",
      " 0.353 0.854 0.093 0.946 0.186 0.938 0.07  0.895 0.908 0.685 0.837 0.362\n",
      " 0.307 0.755 0.097 0.262 0.864 0.517 0.311 0.72  0.188 0.738 0.055 0.175\n",
      " 0.285 0.641 0.511 0.154 0.383 0.693 0.059 0.894 0.499 0.447 0.274 0.785\n",
      " 0.709 0.745 0.782 0.136 0.689 0.973 0.147 0.289 0.888 0.204 0.851 0.028\n",
      " 0.054 0.766 0.293 0.931 0.336 0.334 0.306 0.329 0.605 0.348 0.335 0.739\n",
      " 0.95  0.442 0.649 0.911 0.578 0.521 0.134 0.939 0.921 0.529 0.038 0.73\n",
      " 0.714 0.657 0.296 0.249 0.715 0.496 0.466 0.06  0.975 0.845 0.228 0.756\n",
      " 0.36  0.363 0.677 0.247 0.03  0.553 0.139 0.305 0.444 0.243 0.129 0.485\n",
      " 0.847 0.112 0.1   0.254 0.441 0.364 0.491 0.457 0.417 0.349 0.032 0.551\n",
      " 0.85  0.699 0.701 0.597 0.873 0.219 0.774 0.822 0.185 0.948 0.157 0.574\n",
      " 0.339 0.14  0.784 0.919 0.935 0.094 0.084 0.811 0.7   0.463 0.366 0.205\n",
      " 0.214 0.927 0.161 0.213 0.748 0.808 0.036 0.548 0.558 0.356 0.482 0.587\n",
      " 0.821 0.242 0.875 0.691 0.282 0.064 0.48  0.586 0.955 0.942 0.994 0.662\n",
      " 0.608 0.589 0.818 0.016 0.753 0.138 0.562 0.954 0.31  0.122 0.314 0.454\n",
      " 0.471 0.089 0.656 0.901 0.999 0.733 0.035 0.539 0.859 0.199 0.969 0.679\n",
      " 0.222 0.265 0.2   0.581 0.25  0.245 0.256 0.11  0.879 0.067 0.536 0.469\n",
      " 0.503 0.065 0.966 0.298 0.389 0.141 0.387 0.614 0.759 0.095 0.637 0.377\n",
      " 0.179 0.001 0.434 0.846 0.477 0.533 0.325 0.654 0.734 0.42  0.816 0.58\n",
      " 0.315 0.823 0.84  0.665 0.591 0.28  0.773 0.237 0.876 0.162 0.394 0.555\n",
      " 0.775 0.37  0.518 0.932 0.474 0.333 0.725 0.229 0.066 0.169 0.645 0.475\n",
      " 0.407 0.708 0.131 0.687 0.171 0.152 0.291 0.202 0.236 0.041 0.413 0.99\n",
      " 0.633 0.083 0.344 0.436 0.952 0.08  0.443 0.871 0.943 0.16  0.979 0.62\n",
      " 0.058 0.297 0.232 0.861 0.834 0.576 0.483 0.735 0.515 0.221 0.516 0.891\n",
      " 0.4   0.396 0.892 0.532 0.63  0.981 0.651 0.552 0.658 0.279 0.569 0.008\n",
      " 0.053 0.514 0.643 0.803 0.925 0.399 0.257 0.599 0.703 0.729 0.425 0.046\n",
      " 0.763 0.595 0.512 0.078 0.832 0.619 0.223 0.997 0.967 0.215 0.207 0.38\n",
      " 0.132 0.304 0.547 0.489 0.098 0.267 0.502 0.461 0.705 0.728 0.244 0.048\n",
      " 0.77  0.101 0.096 0.077 0.93  0.838 0.809 0.913 0.26  0.527 0.391 0.867\n",
      " 0.345 0.042 0.556 0.182 0.567 0.726 0.664 0.108 0.545 0.565 0.368 0.261\n",
      " 0.903 0.541 0.611 0.962 0.563 0.78  0.839 0.934 0.826 0.751 0.445 0.266\n",
      " 0.278 0.76  0.815 0.628 0.481 0.017 0.68  0.92  0.003 0.248 0.069 0.736\n",
      " 0.033 0.982 0.513 0.308 0.071 0.386 0.092 0.727 0.617 0.653 0.57  0.189\n",
      " 0.392 0.802 0.431 0.191 0.559 0.484 0.603 0.721 0.268 0.156 0.29  0.201\n",
      " 0.852 0.914 0.406 0.357 0.043 0.332 0.428 0.97  0.359 0.273 0.865 0.661\n",
      " 0.794 0.159 0.453 0.218 0.458 0.797 0.398 0.292 0.88  0.355 0.537 0.598\n",
      " 0.674 0.011 0.352 0.836 0.798 0.712 0.099 0.382 0.068 0.062 0.607 0.55\n",
      " 0.625 0.669 0.941 0.571 0.609 0.211 0.479 0.01  0.812 0.572 0.978 0.024\n",
      " 0.678 0.638 0.075 0.119 0.039 0.255 0.81  0.004 0.524 0.258 0.462 0.546\n",
      " 0.195 0.741 0.455 0.508 0.486 0.013 0.855 0.542 0.862 0.468 0.303 0.3\n",
      " 0.579 0.408 0.783 0.487 0.828 0.472 0.012 0.177 0.592 0.6   0.263 0.172\n",
      " 0.573 0.365 0.929 0.951 0.534 0.41  0.197 0.96  0.439 0.702 0.208 0.326\n",
      " 0.953 0.277 0.09  0.079 0.853 0.56  0.056 0.779 0.433 0.321 0.968 0.648\n",
      " 0.737 0.015 0.69  0.426 0.165 0.67  0.647 0.769 0.924 0.493 0.226 0.286\n",
      " 0.843 0.696 0.501 0.94  0.976 0.395 0.706 0.006 0.194 0.187 0.788 0.523\n",
      " 0.21  0.091 0.632 0.163 0.203 0.476 0.566 0.588 0.694 0.923 0.113 0.907\n",
      " 0.601 0.717 0.972 0.343 0.681 0.606 0.135 0.804 0.704 0.995 0.866 0.23\n",
      " 0.183 0.331 0.858 0.411 0.27  0.33  0.8   0.684 0.088 0.585 0.801 0.284\n",
      " 0.272 0.301 0.618 0.577 0.    0.19  0.051 0.418 0.148 0.884 0.49  0.081\n",
      " 0.863 0.639 0.166 0.085 0.393 0.796 0.509 0.593 0.958 0.045 0.104 0.24\n",
      " 0.582 0.752 0.149 0.612 0.963 0.655 0.429 0.896 0.432 0.634 0.271 0.027\n",
      " 0.174 0.467 0.986 0.072 0.494 0.018 0.912 0.124 0.233 0.928 0.009 0.631\n",
      " 0.787 0.781 0.397 0.022 0.688 0.087 0.354 0.904 0.754 0.269 0.772 0.43\n",
      " 0.526 0.167 0.02  0.142]\n",
      "Mean squared error: 0.012474614421711565\n",
      "Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\n",
      "Mean absolute error: 0.0872336239293905\n"
     ]
    }
   ],
   "source": [
    "### Idea 3: Multiple linear regression \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "sparsity, MDPs = generate_tests(1000)\n",
    "# print(np.array(MDPs[0].P).shape)\n",
    "training_data = [(np.array(mdp.P), mdp.discount, mdp.policy, sparsity[i]) for i, mdp in enumerate(MDPs)]\n",
    "print(sparsity)\n",
    "features, labels = prepare_data(training_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean squared error: {mse}\")\n",
    "print(\"Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\")\n",
    "print(f\"Mean absolute error: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interesting! When I increased the number of states from 10 to 100 and *decreased* the number of training data points from 10^5 to 10^4, test loss *decreased* from ~0.033 to ~0.014 and stayed that way with training data = 10^3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
