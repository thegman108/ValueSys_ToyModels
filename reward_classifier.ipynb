{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox as mdpt, numpy as np\n",
    "import mdptoolbox.example\n",
    "import MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, given a transition function and discount rate, we generate a random reward function over all transitions. We then sparsify the reward function by setting some proportion (e.g. 10%) of the transition values to 0. We then generate the optimal policy for said reward function (using, for instance, policy iteration). We now attempt to build a model that can predict the sparsity used to generate the optimal policy given the transition function, discount rate, and policy itself, but *not* the reward function, as otherwise the problem would be trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a bunch of MDPs with different parameters, sparsity\n",
    "\n",
    "NUM_MDPs = 100\n",
    "NUM_STATES = 10\n",
    "NUM_ACTIONS = 4\n",
    "\n",
    "def get_transition_matrix(num_states, num_actions, generator = np.random.dirichlet):\n",
    "    P = np.zeros((num_actions, num_states, num_states)) # (A, S, S) shape\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            P[a, s, :] = generator(np.ones(num_states))\n",
    "    return P\n",
    "\n",
    "def get_reward_matrix(num_states, num_actions, sparsity = 0.0, generator = np.random.normal):\n",
    "    R = np.zeros((num_states, num_actions))\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            if np.random.rand() < sparsity:\n",
    "                R[s, a] = 0\n",
    "            else:\n",
    "                R[s, a] = generator()\n",
    "    return R\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "EPSILON = 0.01\n",
    "MAX_ITER = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparsity levels generated by generate_tests are divided using arange from 0 to 1 and then scrambled randomly, meaning that in effect each sparsity level in the training and test sets is sampled uniformly from [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests(num_mdps = NUM_MDPs, sparsity_levels = None, mdp_generator = mdpt.mdp.PolicyIteration):\n",
    "    \"\"\"\n",
    "    Generate a bunch of MDPs with different sparsity levels, and return the sparsity levels and the MDPs\n",
    "\n",
    "    Args:\n",
    "        sparsity_levels: a list of sparsity levels to generate MDPs with\n",
    "    Returns:\n",
    "        sparsity_levels: the sparsity levels used to generate the MDPs, in the same order as the MDPs\n",
    "        MDPS: an array of MDPs\n",
    "    \"\"\"\n",
    "    sparsity_levels = sparsity_levels if sparsity_levels is not None else np.arange(num_mdps) / num_mdps\n",
    "    sparsity_copy = sparsity_levels.copy() # defensive copy\n",
    "    np.random.shuffle(sparsity_copy)\n",
    "    MDPS = np.array([mdp_generator(\n",
    "        get_transition_matrix(NUM_STATES, NUM_ACTIONS), \n",
    "        get_reward_matrix(NUM_STATES, NUM_ACTIONS, sparsity_copy[i]), \n",
    "        DISCOUNT, max_iter = MAX_ITER) \n",
    "        for i in range(num_mdps)\n",
    "    ])\n",
    "    return sparsity_copy, MDPS\n",
    "\n",
    "sparsity_levels, MDPS = generate_tests()\n",
    "for mdp in MDPS:\n",
    "    mdp.run()\n",
    "    # print(mdp.policy) # debug\n",
    "# print(MDPS[0].policy) # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a classifier to predict sparsity level from a policy\n",
    "### Idea 1: hack-y heuristics\n",
    "\n",
    "def heuristic_classifier(MDP, policy):\n",
    "    \"\"\"\n",
    "    A heuristic classifier that predicts the sparsity level of an MDP's reward function given its \n",
    "    optimal policy\n",
    "    1. \n",
    "    \"\"\"\n",
    "    # TODO: implement this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25/25 [==============================] - 2s 26ms/step - loss: 0.1057 - mae: 0.2537 - val_loss: 0.0488 - val_mae: 0.1860\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0529 - mae: 0.1829 - val_loss: 0.0447 - val_mae: 0.1795\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0446 - mae: 0.1682 - val_loss: 0.0438 - val_mae: 0.1780\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0422 - mae: 0.1644 - val_loss: 0.0446 - val_mae: 0.1783\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0341 - mae: 0.1480 - val_loss: 0.0419 - val_mae: 0.1710\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0307 - mae: 0.1397 - val_loss: 0.0415 - val_mae: 0.1742\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.0311 - mae: 0.1449 - val_loss: 0.0422 - val_mae: 0.1729\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0282 - mae: 0.1351 - val_loss: 0.0440 - val_mae: 0.1812\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0265 - mae: 0.1305 - val_loss: 0.0419 - val_mae: 0.1701\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.0245 - mae: 0.1261 - val_loss: 0.0436 - val_mae: 0.1780\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0249 - mae: 0.1271 - val_loss: 0.0433 - val_mae: 0.1760\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0221 - mae: 0.1188 - val_loss: 0.0414 - val_mae: 0.1664\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0223 - mae: 0.1201 - val_loss: 0.0411 - val_mae: 0.1687\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0221 - mae: 0.1200 - val_loss: 0.0419 - val_mae: 0.1701\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0198 - mae: 0.1128 - val_loss: 0.0405 - val_mae: 0.1675\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0174 - mae: 0.1069 - val_loss: 0.0435 - val_mae: 0.1742\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0163 - mae: 0.1035 - val_loss: 0.0426 - val_mae: 0.1723\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0179 - mae: 0.1065 - val_loss: 0.0418 - val_mae: 0.1712\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0159 - mae: 0.1009 - val_loss: 0.0420 - val_mae: 0.1701\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0159 - mae: 0.1009 - val_loss: 0.0431 - val_mae: 0.1751\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0138 - mae: 0.0933 - val_loss: 0.0430 - val_mae: 0.1725\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0130 - mae: 0.0901 - val_loss: 0.0416 - val_mae: 0.1685\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0135 - mae: 0.0932 - val_loss: 0.0438 - val_mae: 0.1713\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0129 - mae: 0.0905 - val_loss: 0.0434 - val_mae: 0.1710\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0123 - mae: 0.0885 - val_loss: 0.0431 - val_mae: 0.1741\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0118 - mae: 0.0856 - val_loss: 0.0427 - val_mae: 0.1724\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0114 - mae: 0.0843 - val_loss: 0.0430 - val_mae: 0.1712\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0112 - mae: 0.0836 - val_loss: 0.0424 - val_mae: 0.1693\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0110 - mae: 0.0821 - val_loss: 0.0427 - val_mae: 0.1710\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0098 - mae: 0.0785 - val_loss: 0.0446 - val_mae: 0.1762\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0095 - mae: 0.0779 - val_loss: 0.0432 - val_mae: 0.1739\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0101 - mae: 0.0773 - val_loss: 0.0440 - val_mae: 0.1704\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0094 - mae: 0.0763 - val_loss: 0.0437 - val_mae: 0.1730\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0094 - mae: 0.0771 - val_loss: 0.0446 - val_mae: 0.1760\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0097 - mae: 0.0782 - val_loss: 0.0444 - val_mae: 0.1732\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.0086 - mae: 0.0731 - val_loss: 0.0434 - val_mae: 0.1686\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0089 - mae: 0.0745 - val_loss: 0.0434 - val_mae: 0.1692\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.0093 - mae: 0.0750 - val_loss: 0.0447 - val_mae: 0.1739\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0084 - mae: 0.0709 - val_loss: 0.0438 - val_mae: 0.1731\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0086 - mae: 0.0720 - val_loss: 0.0428 - val_mae: 0.1714\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0094 - mae: 0.0762 - val_loss: 0.0440 - val_mae: 0.1728\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0089 - mae: 0.0738 - val_loss: 0.0449 - val_mae: 0.1753\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.0077 - mae: 0.0681 - val_loss: 0.0437 - val_mae: 0.1722\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0076 - mae: 0.0677 - val_loss: 0.0437 - val_mae: 0.1732\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0078 - mae: 0.0680 - val_loss: 0.0466 - val_mae: 0.1782\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0077 - mae: 0.0673 - val_loss: 0.0434 - val_mae: 0.1704\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.0072 - mae: 0.0655 - val_loss: 0.0458 - val_mae: 0.1764\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.0067 - mae: 0.0634 - val_loss: 0.0467 - val_mae: 0.1799\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.0066 - mae: 0.0624 - val_loss: 0.0427 - val_mae: 0.1711\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0067 - mae: 0.0631 - val_loss: 0.0445 - val_mae: 0.1747\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.0074 - mae: 0.0660 - val_loss: 0.0466 - val_mae: 0.1794\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 0.0074 - mae: 0.0659 - val_loss: 0.0437 - val_mae: 0.1723\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0067 - mae: 0.0630 - val_loss: 0.0425 - val_mae: 0.1700\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0065 - mae: 0.0617 - val_loss: 0.0441 - val_mae: 0.1717\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0066 - mae: 0.0622 - val_loss: 0.0439 - val_mae: 0.1738\n",
      "Epoch 56/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0069 - mae: 0.0638 - val_loss: 0.0435 - val_mae: 0.1694\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 0.0059 - mae: 0.0596 - val_loss: 0.0456 - val_mae: 0.1728\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.0062 - mae: 0.0610 - val_loss: 0.0442 - val_mae: 0.1729\n",
      "Epoch 59/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0056 - mae: 0.0571 - val_loss: 0.0431 - val_mae: 0.1692\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0060 - mae: 0.0603 - val_loss: 0.0435 - val_mae: 0.1742\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.0052 - mae: 0.0575 - val_loss: 0.0444 - val_mae: 0.1723\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 0.0060 - mae: 0.0587 - val_loss: 0.0465 - val_mae: 0.1778\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0055 - mae: 0.0564 - val_loss: 0.0447 - val_mae: 0.1719\n",
      "Epoch 64/100\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0055 - mae: 0.0570 - val_loss: 0.0447 - val_mae: 0.1730\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0061 - mae: 0.0601 - val_loss: 0.0456 - val_mae: 0.1738\n",
      "Epoch 66/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0059 - mae: 0.0599 - val_loss: 0.0450 - val_mae: 0.1752\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 0.0058 - mae: 0.0592 - val_loss: 0.0459 - val_mae: 0.1740\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0056 - mae: 0.0573 - val_loss: 0.0456 - val_mae: 0.1762\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0057 - mae: 0.0579 - val_loss: 0.0454 - val_mae: 0.1751\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.0053 - mae: 0.0549 - val_loss: 0.0455 - val_mae: 0.1728\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0048 - mae: 0.0532 - val_loss: 0.0449 - val_mae: 0.1723\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.0053 - mae: 0.0554 - val_loss: 0.0446 - val_mae: 0.1723\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0053 - mae: 0.0559 - val_loss: 0.0446 - val_mae: 0.1732\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0057 - mae: 0.0589 - val_loss: 0.0465 - val_mae: 0.1752\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0049 - mae: 0.0533 - val_loss: 0.0451 - val_mae: 0.1731\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0053 - mae: 0.0549 - val_loss: 0.0455 - val_mae: 0.1719\n",
      "Epoch 77/100\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0049 - mae: 0.0535 - val_loss: 0.0460 - val_mae: 0.1759\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0046 - mae: 0.0513 - val_loss: 0.0471 - val_mae: 0.1776\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0049 - mae: 0.0530 - val_loss: 0.0472 - val_mae: 0.1765\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0045 - mae: 0.0507 - val_loss: 0.0441 - val_mae: 0.1696\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0045 - mae: 0.0521 - val_loss: 0.0459 - val_mae: 0.1739\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0048 - mae: 0.0534 - val_loss: 0.0460 - val_mae: 0.1768\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.0042 - mae: 0.0498 - val_loss: 0.0460 - val_mae: 0.1764\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0041 - mae: 0.0494 - val_loss: 0.0463 - val_mae: 0.1755\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0043 - mae: 0.0512 - val_loss: 0.0478 - val_mae: 0.1785\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0047 - mae: 0.0533 - val_loss: 0.0445 - val_mae: 0.1737\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0041 - mae: 0.0499 - val_loss: 0.0461 - val_mae: 0.1732\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0046 - mae: 0.0523 - val_loss: 0.0459 - val_mae: 0.1775\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0046 - mae: 0.0513 - val_loss: 0.0461 - val_mae: 0.1756\n",
      "Epoch 90/100\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0041 - mae: 0.0498 - val_loss: 0.0450 - val_mae: 0.1719\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0045 - mae: 0.0508 - val_loss: 0.0464 - val_mae: 0.1754\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0044 - mae: 0.0515 - val_loss: 0.0467 - val_mae: 0.1745\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0041 - mae: 0.0491 - val_loss: 0.0463 - val_mae: 0.1735\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 0s 15ms/step - loss: 0.0040 - mae: 0.0482 - val_loss: 0.0455 - val_mae: 0.1730\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0040 - mae: 0.0481 - val_loss: 0.0454 - val_mae: 0.1738\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0041 - mae: 0.0491 - val_loss: 0.0468 - val_mae: 0.1759\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0043 - mae: 0.0497 - val_loss: 0.0468 - val_mae: 0.1757\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0041 - mae: 0.0492 - val_loss: 0.0443 - val_mae: 0.1709\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0041 - mae: 0.0495 - val_loss: 0.0447 - val_mae: 0.1753\n",
      "Epoch 100/100\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.0041 - mae: 0.0488 - val_loss: 0.0453 - val_mae: 0.1755\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "Predicted sparsity level for MDP 0: [[0.13937923]], actual sparsity level: 0.496, Squared error: 0.1271783709526062\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Predicted sparsity level for MDP 1: [[0.7279967]], actual sparsity level: 0.724, Squared error: 1.5973850167938508e-05\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "Predicted sparsity level for MDP 2: [[0.3317951]], actual sparsity level: 0.096, Squared error: 0.05559932813048363\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Predicted sparsity level for MDP 3: [[0.14749476]], actual sparsity level: 0.215, Squared error: 0.004556957632303238\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Predicted sparsity level for MDP 4: [[0.09213389]], actual sparsity level: 0.203, Squared error: 0.01229129172861576\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "Predicted sparsity level for MDP 5: [[0.64118516]], actual sparsity level: 0.686, Squared error: 0.00200836849398911\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "Predicted sparsity level for MDP 6: [[0.32184237]], actual sparsity level: 0.078, Squared error: 0.05945909768342972\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "Predicted sparsity level for MDP 7: [[0.5862855]], actual sparsity level: 0.618, Squared error: 0.001005809404887259\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Predicted sparsity level for MDP 8: [[0.30122864]], actual sparsity level: 0.399, Squared error: 0.009559236466884613\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "Predicted sparsity level for MDP 9: [[0.2083655]], actual sparsity level: 0.206, Squared error: 5.59558884560829e-06\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "Predicted sparsity level for MDP 10: [[0.3969925]], actual sparsity level: 0.625, Squared error: 0.0519874170422554\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Predicted sparsity level for MDP 11: [[0.29766697]], actual sparsity level: 0.209, Squared error: 0.007861830294132233\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Predicted sparsity level for MDP 12: [[0.44836074]], actual sparsity level: 0.429, Squared error: 0.00037483868072740734\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Predicted sparsity level for MDP 13: [[0.10346846]], actual sparsity level: 0.269, Squared error: 0.027400687336921692\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Predicted sparsity level for MDP 14: [[0.4472404]], actual sparsity level: 0.691, Squared error: 0.059418730437755585\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "Predicted sparsity level for MDP 15: [[0.36001122]], actual sparsity level: 0.237, Squared error: 0.015131759457290173\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "Predicted sparsity level for MDP 16: [[0.3524674]], actual sparsity level: 0.499, Squared error: 0.021471809595823288\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Predicted sparsity level for MDP 17: [[0.7823236]], actual sparsity level: 0.942, Squared error: 0.02549654431641102\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "Predicted sparsity level for MDP 18: [[0.7309605]], actual sparsity level: 0.703, Squared error: 0.0007817883742973208\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Predicted sparsity level for MDP 19: [[0.23439787]], actual sparsity level: 0.279, Squared error: 0.001989350887015462\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "Predicted sparsity level for MDP 20: [[0.85697556]], actual sparsity level: 0.947, Squared error: 0.008104405365884304\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "Predicted sparsity level for MDP 21: [[0.5227119]], actual sparsity level: 0.104, Squared error: 0.1753196269273758\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Predicted sparsity level for MDP 22: [[0.5130819]], actual sparsity level: 0.595, Squared error: 0.0067105782218277454\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "Predicted sparsity level for MDP 23: [[0.20831093]], actual sparsity level: 0.306, Squared error: 0.009543152526021004\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Predicted sparsity level for MDP 24: [[0.7803432]], actual sparsity level: 0.877, Squared error: 0.009342537261545658\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "Predicted sparsity level for MDP 25: [[0.2530685]], actual sparsity level: 0.264, Squared error: 0.0001194975120597519\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "Predicted sparsity level for MDP 26: [[0.44799387]], actual sparsity level: 0.532, Squared error: 0.0070570302195847034\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Predicted sparsity level for MDP 27: [[0.3447906]], actual sparsity level: 0.351, Squared error: 3.855668910546228e-05\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "Predicted sparsity level for MDP 28: [[0.5664944]], actual sparsity level: 0.908, Squared error: 0.1166260689496994\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "Predicted sparsity level for MDP 29: [[0.14944452]], actual sparsity level: 0.005, Squared error: 0.020864220336079597\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Predicted sparsity level for MDP 30: [[0.17957911]], actual sparsity level: 0.022, Squared error: 0.024831175804138184\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "Predicted sparsity level for MDP 31: [[0.26954246]], actual sparsity level: 0.06, Squared error: 0.04390804097056389\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Predicted sparsity level for MDP 32: [[0.55644]], actual sparsity level: 0.584, Squared error: 0.0007595533388666809\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "Predicted sparsity level for MDP 33: [[0.36125994]], actual sparsity level: 0.231, Squared error: 0.01696765050292015\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Predicted sparsity level for MDP 34: [[0.6202365]], actual sparsity level: 0.677, Squared error: 0.0032220915891230106\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "Predicted sparsity level for MDP 35: [[0.2841987]], actual sparsity level: 0.238, Squared error: 0.002134319394826889\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Predicted sparsity level for MDP 36: [[0.4845603]], actual sparsity level: 0.24, Squared error: 0.05980974808335304\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "Predicted sparsity level for MDP 37: [[0.27998543]], actual sparsity level: 0.088, Squared error: 0.03685840591788292\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Predicted sparsity level for MDP 38: [[0.6408795]], actual sparsity level: 0.467, Squared error: 0.0302340816706419\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Predicted sparsity level for MDP 39: [[0.5473887]], actual sparsity level: 0.834, Squared error: 0.0821460485458374\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "Predicted sparsity level for MDP 40: [[0.8451983]], actual sparsity level: 0.87, Squared error: 0.0006151258712634444\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Predicted sparsity level for MDP 41: [[0.22439864]], actual sparsity level: 0.285, Squared error: 0.0036725241225212812\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "Predicted sparsity level for MDP 42: [[0.10299595]], actual sparsity level: 0.039, Squared error: 0.004095480777323246\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Predicted sparsity level for MDP 43: [[0.83306754]], actual sparsity level: 0.964, Squared error: 0.017143307253718376\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Predicted sparsity level for MDP 44: [[0.74424183]], actual sparsity level: 0.466, Squared error: 0.07741852104663849\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "Predicted sparsity level for MDP 45: [[0.72388744]], actual sparsity level: 0.932, Squared error: 0.043310828506946564\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "Predicted sparsity level for MDP 46: [[0.3075456]], actual sparsity level: 0.358, Squared error: 0.0025456473231315613\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "Predicted sparsity level for MDP 47: [[0.28452292]], actual sparsity level: 0.32, Squared error: 0.0012586226221174002\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Predicted sparsity level for MDP 48: [[0.91916764]], actual sparsity level: 0.971, Squared error: 0.0026865953113883734\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "Predicted sparsity level for MDP 49: [[0.22467874]], actual sparsity level: 0.25, Squared error: 0.0006411662325263023\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Predicted sparsity level for MDP 50: [[0.3302385]], actual sparsity level: 0.409, Squared error: 0.006203376688063145\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Predicted sparsity level for MDP 51: [[0.38410962]], actual sparsity level: 0.345, Squared error: 0.0015295621706172824\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "Predicted sparsity level for MDP 52: [[0.48849612]], actual sparsity level: 0.573, Squared error: 0.00714090745896101\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Predicted sparsity level for MDP 53: [[0.30043694]], actual sparsity level: 0.304, Squared error: 1.269530002900865e-05\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "Predicted sparsity level for MDP 54: [[0.1339583]], actual sparsity level: 0.156, Squared error: 0.0004858369065914303\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "Predicted sparsity level for MDP 55: [[0.5725621]], actual sparsity level: 0.616, Squared error: 0.001886851037852466\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "Predicted sparsity level for MDP 56: [[0.25498652]], actual sparsity level: 0.044, Squared error: 0.04451531544327736\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "Predicted sparsity level for MDP 57: [[0.82788885]], actual sparsity level: 0.82, Squared error: 6.223400851013139e-05\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Predicted sparsity level for MDP 58: [[0.57520586]], actual sparsity level: 0.032, Squared error: 0.29507261514663696\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "Predicted sparsity level for MDP 59: [[1.0747346]], actual sparsity level: 0.925, Squared error: 0.02242043800652027\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "Predicted sparsity level for MDP 60: [[0.9365287]], actual sparsity level: 0.997, Squared error: 0.00365677778609097\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Predicted sparsity level for MDP 61: [[0.73144543]], actual sparsity level: 0.759, Squared error: 0.0007592544425278902\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Predicted sparsity level for MDP 62: [[0.7329423]], actual sparsity level: 0.711, Squared error: 0.0004814626881852746\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Predicted sparsity level for MDP 63: [[0.12355505]], actual sparsity level: 0.183, Squared error: 0.003533701878041029\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Predicted sparsity level for MDP 64: [[0.42410678]], actual sparsity level: 0.868, Squared error: 0.19704116880893707\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Predicted sparsity level for MDP 65: [[0.37399077]], actual sparsity level: 0.73, Squared error: 0.12674258649349213\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "Predicted sparsity level for MDP 66: [[0.28870818]], actual sparsity level: 0.314, Squared error: 0.0006396766984835267\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Predicted sparsity level for MDP 67: [[0.4567098]], actual sparsity level: 0.403, Squared error: 0.002884743269532919\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Predicted sparsity level for MDP 68: [[0.68307287]], actual sparsity level: 0.875, Squared error: 0.03683602437376976\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "Predicted sparsity level for MDP 69: [[0.5111348]], actual sparsity level: 0.153, Squared error: 0.128260537981987\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "Predicted sparsity level for MDP 70: [[0.47347987]], actual sparsity level: 0.251, Squared error: 0.049497295171022415\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Predicted sparsity level for MDP 71: [[0.8222184]], actual sparsity level: 0.968, Squared error: 0.02125226892530918\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "Predicted sparsity level for MDP 72: [[0.619017]], actual sparsity level: 0.367, Squared error: 0.06351256370544434\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "Predicted sparsity level for MDP 73: [[0.31746143]], actual sparsity level: 0.602, Squared error: 0.08096219599246979\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Predicted sparsity level for MDP 74: [[0.35459632]], actual sparsity level: 0.29, Squared error: 0.004172685090452433\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "Predicted sparsity level for MDP 75: [[0.51078653]], actual sparsity level: 0.436, Squared error: 0.005593026988208294\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "Predicted sparsity level for MDP 76: [[0.19264267]], actual sparsity level: 0.08, Squared error: 0.012688372284173965\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "Predicted sparsity level for MDP 77: [[0.8909725]], actual sparsity level: 0.993, Squared error: 0.01040960568934679\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "Predicted sparsity level for MDP 78: [[0.54029846]], actual sparsity level: 0.302, Squared error: 0.05678616464138031\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "Predicted sparsity level for MDP 79: [[0.97783136]], actual sparsity level: 0.912, Squared error: 0.004333768505603075\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "Predicted sparsity level for MDP 80: [[0.32399172]], actual sparsity level: 0.563, Squared error: 0.057124972343444824\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Predicted sparsity level for MDP 81: [[0.32478616]], actual sparsity level: 0.09, Squared error: 0.05512453615665436\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "Predicted sparsity level for MDP 82: [[0.32184795]], actual sparsity level: 0.213, Squared error: 0.011847875081002712\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "Predicted sparsity level for MDP 83: [[0.18662754]], actual sparsity level: 0.103, Squared error: 0.006993564777076244\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "Predicted sparsity level for MDP 84: [[0.34750044]], actual sparsity level: 0.11, Squared error: 0.05640646070241928\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Predicted sparsity level for MDP 85: [[0.39007175]], actual sparsity level: 0.368, Squared error: 0.00048716209130361676\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "Predicted sparsity level for MDP 86: [[0.9286032]], actual sparsity level: 0.845, Squared error: 0.0069894855841994286\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "Predicted sparsity level for MDP 87: [[0.20814404]], actual sparsity level: 0.257, Squared error: 0.002386904787272215\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Predicted sparsity level for MDP 88: [[0.5337509]], actual sparsity level: 0.783, Squared error: 0.06212511286139488\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "Predicted sparsity level for MDP 89: [[0.51725864]], actual sparsity level: 0.492, Squared error: 0.0006379983969964087\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "Predicted sparsity level for MDP 90: [[0.79125845]], actual sparsity level: 0.754, Squared error: 0.0013881918275728822\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "Predicted sparsity level for MDP 91: [[0.5801847]], actual sparsity level: 0.485, Squared error: 0.009060123935341835\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "Predicted sparsity level for MDP 92: [[0.54508936]], actual sparsity level: 0.757, Squared error: 0.0449061281979084\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "Predicted sparsity level for MDP 93: [[0.3448256]], actual sparsity level: 0.706, Squared error: 0.1304469257593155\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "Predicted sparsity level for MDP 94: [[0.21562088]], actual sparsity level: 0.0, Squared error: 0.04649236053228378\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "Predicted sparsity level for MDP 95: [[0.8880173]], actual sparsity level: 0.963, Squared error: 0.005622405558824539\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "Predicted sparsity level for MDP 96: [[0.1454681]], actual sparsity level: 0.253, Squared error: 0.011563107371330261\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "Predicted sparsity level for MDP 97: [[0.35582507]], actual sparsity level: 0.079, Squared error: 0.07663211971521378\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "Predicted sparsity level for MDP 98: [[0.78822696]], actual sparsity level: 0.712, Squared error: 0.005810548085719347\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "Predicted sparsity level for MDP 99: [[0.17856815]], actual sparsity level: 0.22, Squared error: 0.0017165977042168379\n",
      "Mean squared error: 0.030747156903007635\n",
      "Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\n"
     ]
    }
   ],
   "source": [
    "### Idea 2: neural network\n",
    "# Thanks again ChatGPT for outlining the code structure\n",
    "\n",
    "sparsity, MDPs = generate_tests(1000)\n",
    "# print(np.array(MDPs[0].P).shape)\n",
    "training_data = [(np.array(mdp.P), mdp.discount, mdp.policy, sparsity[i]) for i, mdp in enumerate(MDPs)]\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Step 1: Feature extraction function\n",
    "def extract_features(transition_function, discount_rate, optimal_policy):\n",
    "    \"\"\"\n",
    "    Extract features from the MDP's transition function, discount rate, and optimal policy\n",
    "    \"\"\"\n",
    "    # opt_policy = optimal_policy.reshape(-1, 1)  # Reshape for sklearn which expects 2D input\n",
    "\n",
    "    # # Initialize the OneHotEncoder\n",
    "    # encoder = OneHotEncoder(sparse=False)  # Use sparse=False to get a dense array\n",
    "\n",
    "    # # Fit and transform\n",
    "    # opt_policy_one_hot = encoder.fit_transform(opt_policy)\n",
    "    features = np.concatenate((transition_function.flatten(), [discount_rate], optimal_policy.flatten()))\n",
    "    # print(features.shape)\n",
    "    # length 10*10*4 + 1 + 10 = 411\n",
    "\n",
    "    # Placeholder features\n",
    "    # features = np.random.rand(411)\n",
    "    return features\n",
    "\n",
    "# Step 2: Data preparation (assuming you have your data in an appropriate format)\n",
    "# This is a placeholder function - you would replace it with actual data loading and processing\n",
    "def prepare_data(training_data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for transition_function, discount_rate, optimal_policy, sparsity_level in training_data:\n",
    "        features.append(extract_features(transition_function, discount_rate, optimal_policy))\n",
    "        labels.append(sparsity_level)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Step 3: Model selection\n",
    "\n",
    "def build_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='linear')  # Linear activation for regression output\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='mean_squared_error',  # Suitable for regression\n",
    "                  metrics=['mae'])  # Mean Absolute Error as an additional metric\n",
    "    return model\n",
    "\n",
    "# Assuming you have already defined the feature extraction and data preparation functions\n",
    "# and have your data ready in 'features' and 'labels':\n",
    "features, labels = prepare_data(training_data)\n",
    "# Example: features shape is (num_samples, num_features), adjust 'input_dim' accordingly\n",
    "input_dim = features.shape[1]  # Assuming 'features' is already defined and preprocessed\n",
    "\n",
    "model = build_model(input_dim)\n",
    "\n",
    "# Training the model\n",
    "model.fit(features, labels, epochs=100, validation_split=0.2, verbose = 1)\n",
    "\n",
    "# Don't forget to preprocess your new data before making predictions\n",
    "# predicted_sparsity = model.predict(new_features)\n",
    "\n",
    "# Step 4: Training the model (placeholder for training data)\n",
    "# training_data = load_your_data_somehow()\n",
    "# features, labels = prepare_data(training_data)\n",
    "# model.fit(features, labels)\n",
    "\n",
    "# Step 5: Prediction function\n",
    "def predict_sparsity(transition_function, discount_rate, optimal_policy):\n",
    "    features = extract_features(transition_function, discount_rate, optimal_policy).reshape(1, -1)\n",
    "    predicted_sparsity = model.predict(features)\n",
    "    return predicted_sparsity\n",
    "\n",
    "# Note: The actual training step and data preparation would depend on your specific dataset and environment setup.\n",
    "test_sparsity, test_MDPs = generate_tests()\n",
    "test_data = [(np.array(mdp.P), mdp.discount, mdp.policy) for mdp in (test_MDPs)]\n",
    "NUM_TESTS = 100\n",
    "mse = np.zeros(NUM_TESTS)\n",
    "\n",
    "for i in range(min(NUM_TESTS, len(test_data))):\n",
    "    transition_function, discount_rate, optimal_policy = test_data[i]\n",
    "    prediction = predict_sparsity(transition_function, discount_rate, optimal_policy)\n",
    "    mse[i] = (prediction - test_sparsity[i])**2\n",
    "    print(f\"Predicted sparsity level for MDP {i}: {prediction}, actual sparsity level: {test_sparsity[i]}, Squared error: {mse[i]}\")\n",
    "\n",
    "print(f\"Mean squared error: {np.mean(mse)}\")\n",
    "print(\"Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a control, when the input layer (with same dimension as transition_function + discount rate + optimal policy) is randomized, MSE = ~0.115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.798 0.946 0.765 0.258 0.247 0.781 0.04  0.57  0.189 0.243 0.036 0.864\n",
      " 0.472 0.473 0.67  0.735 0.738 0.805 0.481 0.948 0.421 0.875 0.719 0.16\n",
      " 0.453 0.938 0.222 0.409 0.098 0.142 0.814 0.272 0.849 0.619 0.433 0.868\n",
      " 0.032 0.289 0.336 0.718 0.234 0.763 0.834 0.231 0.532 0.431 0.518 0.496\n",
      " 0.655 0.726 0.871 0.1   0.475 0.498 0.757 0.252 0.857 0.955 0.595 0.82\n",
      " 0.439 0.756 0.463 0.178 0.872 0.187 0.701 0.845 0.048 0.277 0.9   0.107\n",
      " 0.332 0.622 0.576 0.188 0.11  0.401 0.139 0.852 0.286 0.235 0.593 0.898\n",
      " 0.143 0.68  0.87  0.491 0.687 0.043 0.55  0.464 0.487 0.13  0.311 0.601\n",
      " 0.919 0.017 0.216 0.093 0.362 0.998 0.79  0.633 0.772 0.294 0.097 0.545\n",
      " 0.678 0.026 0.227 0.501 0.479 0.268 0.324 0.536 0.4   0.117 0.449 0.016\n",
      " 0.577 0.209 0.915 0.351 0.182 0.493 0.626 0.882 0.944 0.204 0.748 0.483\n",
      " 0.349 0.922 0.2   0.918 0.482 0.777 0.205 0.373 0.506 0.429 0.345 0.901\n",
      " 0.398 0.008 0.81  0.208 0.847 0.335 0.812 0.8   0.681 0.48  0.054 0.842\n",
      " 0.788 0.457 0.966 0.122 0.835 0.217 0.978 0.407 0.168 0.212 0.877 0.698\n",
      " 0.124 0.741 0.322 0.154 0.743 0.797 0.405 0.064 0.237 0.887 0.954 0.704\n",
      " 0.376 0.298 0.796 0.366 0.953 0.724 0.933 0.148 0.135 0.684 0.986 0.455\n",
      " 0.542 0.075 0.649 0.38  0.446 0.876 0.158 0.883 0.892 0.176 0.062 0.673\n",
      " 0.821 0.609 0.299 0.74  0.013 0.141 0.612 0.651 0.156 0.502 0.987 0.226\n",
      " 0.126 0.443 0.152 0.395 0.369 0.232 0.634 0.185 0.801 0.254 0.094 0.878\n",
      " 0.116 0.642 0.29  0.667 0.844 0.3   0.019 0.023 0.543 0.279 0.049 0.995\n",
      " 0.983 0.173 0.167 0.925 0.495 0.08  0.99  0.76  0.993 0.712 0.413 0.49\n",
      " 0.51  0.361 0.641 0.066 0.629 0.758 0.578 0.714 0.858 0.974 0.558 0.61\n",
      " 0.469 0.259 0.454 0.339 0.091 0.544 0.513 0.321 0.202 0.822 0.745 0.659\n",
      " 0.668 0.125 0.224 0.731 0.177 0.058 0.355 0.375 0.281 0.327 0.884 0.909\n",
      " 0.474 0.551 0.606 0.203 0.318 0.674 0.317 0.603 0.087 0.458 0.566 0.489\n",
      " 0.769 0.047 0.683 0.346 0.604 0.358 0.179 0.526 0.424 0.828 0.17  0.534\n",
      " 0.537 0.689 0.009 0.786 0.284 0.414 0.253 0.31  0.197 0.587 0.888 0.826\n",
      " 0.722 0.824 0.989 0.546 0.913 0.547 0.671 0.85  0.07  0.529 0.149 0.196\n",
      " 0.658 0.709 0.027 0.195 0.082 0.183 0.509 0.874 0.906 0.514 0.809 0.356\n",
      " 0.427 0.343 0.248 0.548 0.236 0.159 0.753 0.223 0.759 0.663 0.274 0.239\n",
      " 0.004 0.752 0.71  0.099 0.569 0.549 0.555 0.533 0.951 0.725 0.541 0.261\n",
      " 0.263 0.088 0.418 0.706 0.562 0.044 0.947 0.614 0.935 0.115 0.319 0.823\n",
      " 0.831 0.572 0.827 0.716 0.736 0.692 0.896 0.363 0.78  0.305 0.256 0.445\n",
      " 0.18  0.931 0.865 0.323 0.816 0.486 0.484 0.057 0.686 0.382 0.97  0.357\n",
      " 0.963 0.285 0.393 0.221 0.44  0.607 0.839 0.508 0.697 0.832 0.984 0.863\n",
      " 0.207 0.727 0.138 0.672 0.121 0.444 0.033 0.635 0.118 0.466 0.194 0.646\n",
      " 0.657 0.962 0.728 0.941 0.729 0.083 0.599 0.293 0.145 0.419 0.785 0.384\n",
      " 0.841 0.119 0.32  0.09  0.53  0.749 0.191 0.768 0.521 0.314 0.385 0.206\n",
      " 0.669 0.302 0.734 0.654 0.677 0.784 0.949 0.575 0.73  0.344 0.854 0.241\n",
      " 0.21  0.802 0.28  0.594 0.136 0.096 0.12  0.666 0.164 0.089 0.804 0.958\n",
      " 0.055 0.109 0.58  0.306 0.96  0.696 0.43  0.012 0.895 0.817 0.03  0.374\n",
      " 0.199 0.102 0.836 0.707 0.198 0.997 0.639 0.478 0.999 0.574 0.146 0.86\n",
      " 0.069 0.065 0.652 0.354 0.037 0.067 0.417 0.23  0.611 0.567 0.528 0.621\n",
      " 0.952 0.151 0.052 0.111 0.246 0.452 0.035 0.904 0.37  0.605 0.45  0.392\n",
      " 0.644 0.573 0.211 0.643 0.565 0.448 0.564 0.553 0.625 0.52  0.779 0.011\n",
      " 0.789 0.977 0.163 0.787 0.942 0.64  0.588 0.359 0.406 0.461 0.957 0.218\n",
      " 0.965 0.773 0.    0.059 0.137 0.365 0.423 0.155 0.078 0.519 0.721 0.766\n",
      " 0.853 0.608 0.968 0.128 0.242 0.076 0.214 0.045 0.969 0.982 0.192 0.675\n",
      " 0.051 0.394 0.36  0.554 0.003 0.937 0.22  0.042 0.511 0.676 0.086 0.793\n",
      " 0.347 0.388 0.266 0.046 0.264 0.894 0.72  0.396 0.26  0.465 0.353 0.416\n",
      " 0.557 0.833 0.33  0.964 0.792 0.767 0.988 0.348 0.613 0.278 0.34  0.342\n",
      " 0.774 0.597 0.061 0.623 0.803 0.085 0.309 0.552 0.257 0.46  0.907 0.885\n",
      " 0.561 0.664 0.228 0.522 0.846 0.815 0.88  0.65  0.229 0.628 0.507 0.516\n",
      " 0.754 0.181 0.166 0.69  0.56  0.881 0.838 0.806 0.338 0.699 0.201 0.147\n",
      " 0.014 0.956 0.459 0.15  0.41  0.819 0.074 0.705 0.297 0.333 0.307 0.123\n",
      " 0.468 0.244 0.291 0.861 0.476 0.172 0.367 0.162 0.571 0.632 0.029 0.637\n",
      " 0.807 0.702 0.329 0.848 0.276 0.975 0.859 0.91  0.985 0.063 0.039 0.992\n",
      " 0.328 0.379 0.267 0.131 0.936 0.924 0.732 0.761 0.923 0.27  0.851 0.255\n",
      " 0.05  0.95  0.296 0.94  0.331 0.391 0.14  0.034 0.98  0.022 0.494 0.991\n",
      " 0.103 0.031 0.539 0.912 0.905 0.39  0.917 0.315 0.313 0.886 0.24  0.273\n",
      " 0.326 0.971 0.432 0.891 0.079 0.7   0.694 0.994 0.615 0.437 0.186 0.624\n",
      " 0.92  0.742 0.638 0.723 0.755 0.219 0.563 0.245 0.308 0.467 0.25  0.837\n",
      " 0.648 0.967 0.556 0.897 0.071 0.739 0.979 0.316 0.568 0.024 0.62  0.531\n",
      " 0.038 0.238 0.015 0.471 0.843 0.66  0.104 0.488 0.517 0.341 0.959 0.866\n",
      " 0.582 0.352 0.025 0.523 0.825 0.778 0.399 0.63  0.292 0.337 0.403 0.855\n",
      " 0.381 0.617 0.631 0.914 0.485 0.249 0.527 0.713 0.581 0.441 0.428 0.592\n",
      " 0.171 0.213 0.799 0.304 0.775 0.442 0.262 0.661 0.591 0.447 0.737 0.492\n",
      " 0.682 0.703 0.106 0.175 0.133 0.512 0.325 0.75  0.662 0.397 0.477 0.771\n",
      " 0.903 0.386 0.524 0.271 0.583 0.497 0.435 0.387 0.5   0.813 0.579 0.54\n",
      " 0.283 0.596 0.653 0.462 0.174 0.01  0.056 0.538 0.301 0.811 0.287 0.412\n",
      " 0.829 0.295 0.889 0.438 0.364 0.636 0.422 0.275 0.973 0.425 0.972 0.688\n",
      " 0.93  0.584 0.916 0.157 0.005 0.169 0.436 0.535 0.893 0.072 0.35  0.144\n",
      " 0.921 0.899 0.679 0.627 0.499 0.869 0.808 0.762 0.434 0.161 0.717 0.515\n",
      " 0.265 0.068 0.378 0.585 0.184 0.795 0.095 0.134 0.602 0.408 0.402 0.764\n",
      " 0.783 0.007 0.006 0.981 0.269 0.929 0.665 0.021 0.794 0.426 0.47  0.127\n",
      " 0.647 0.867 0.028 0.616 0.715 0.303 0.932 0.751 0.873 0.112 0.908 0.456\n",
      " 0.744 0.791 0.879 0.073 0.334 0.911 0.377 0.084 0.598 0.215 0.193 0.404\n",
      " 0.371 0.961 0.818 0.053 0.233 0.83  0.389 0.695 0.733 0.018 0.945 0.902\n",
      " 0.708 0.505 0.288 0.383 0.101 0.42  0.559 0.589 0.165 0.282 0.108 0.776\n",
      " 0.504 0.746 0.656 0.092 0.251 0.747 0.685 0.782 0.77  0.934 0.105 0.451\n",
      " 0.618 0.586 0.411 0.89  0.927 0.693 0.926 0.153 0.002 0.939 0.711 0.415\n",
      " 0.077 0.943 0.225 0.02  0.041 0.19  0.368 0.6   0.312 0.001 0.645 0.114\n",
      " 0.928 0.132 0.996 0.525 0.691 0.856 0.862 0.84  0.06  0.113 0.372 0.59\n",
      " 0.081 0.503 0.129 0.976]\n",
      "Mean squared error: 0.06680235977981243\n",
      "Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\n"
     ]
    }
   ],
   "source": [
    "### Idea 3: Multiple linear regression \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "sparsity, MDPs = generate_tests(1000)\n",
    "# print(np.array(MDPs[0].P).shape)\n",
    "training_data = [(np.array(mdp.P), mdp.discount, mdp.policy, sparsity[i]) for i, mdp in enumerate(MDPs)]\n",
    "print(sparsity)\n",
    "features, labels = prepare_data(training_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean squared error: {mse}\")\n",
    "print(\"Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
