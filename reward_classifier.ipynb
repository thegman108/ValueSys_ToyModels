{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox as mdpt, numpy as np\n",
    "import mdptoolbox.example\n",
    "import MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, given a transition function and discount rate, we generate a random reward function over all transitions. We then sparsify the reward function by setting some proportion (e.g. 10%) of the transition values to 0. We then generate the optimal policy for said reward function (using, for instance, policy iteration). We now attempt to build a model that can predict the sparsity used to generate the optimal policy given the transition function, discount rate, and policy itself, but *not* the reward function, as otherwise the problem would be trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a bunch of MDPs with different parameters, sparsity\n",
    "\n",
    "NUM_MDPs = 100\n",
    "NUM_STATES = 10\n",
    "NUM_ACTIONS = 4\n",
    "\n",
    "def get_transition_matrix(num_states, num_actions, generator = np.random.dirichlet):\n",
    "    P = np.zeros((num_actions, num_states, num_states)) # (A, S, S) shape\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            P[a, s, :] = generator(np.ones(num_states))\n",
    "    return P\n",
    "\n",
    "def get_reward_matrix(num_states, num_actions, sparsity = 0.0, generator = np.random.normal):\n",
    "    R = np.zeros((num_states, num_actions))\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            if np.random.rand() < sparsity:\n",
    "                R[s, a] = 0\n",
    "            else:\n",
    "                R[s, a] = generator()\n",
    "    return R\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "EPSILON = 0.01\n",
    "MAX_ITER = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparsity levels generated by generate_tests are divided using arange from 0 to 1 and then scrambled randomly, meaning that in effect each sparsity level in the training and test sets is sampled uniformly from [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests(num_mdps = NUM_MDPs, sparsity_levels = np.arange(NUM_MDPs) / NUM_MDPs, mdp_generator = mdpt.mdp.PolicyIteration):\n",
    "    \"\"\"\n",
    "    Generate a bunch of MDPs with different sparsity levels, and return the sparsity levels and the MDPs\n",
    "\n",
    "    Args:\n",
    "        sparsity_levels: a list of sparsity levels to generate MDPs with\n",
    "    Returns:\n",
    "        sparsity_levels: the sparsity levels used to generate the MDPs, in the same order as the MDPs\n",
    "        MDPS: an array of MDPs\n",
    "    \"\"\"\n",
    "    sparsity_copy = sparsity_levels.copy() # defensive copy\n",
    "    np.random.shuffle(sparsity_copy)\n",
    "    MDPS = np.array([mdp_generator(\n",
    "        get_transition_matrix(NUM_STATES, NUM_ACTIONS), \n",
    "        get_reward_matrix(NUM_STATES, NUM_ACTIONS, sparsity_copy[i]), \n",
    "        DISCOUNT, max_iter = MAX_ITER) \n",
    "        for i in range(num_mdps)\n",
    "    ])\n",
    "    return sparsity_copy, MDPS\n",
    "\n",
    "sparsity_levels, MDPS = generate_tests()\n",
    "for mdp in MDPS:\n",
    "    mdp.run()\n",
    "    # print(mdp.policy) # debug\n",
    "# print(MDPS[0].policy) # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a classifier to predict sparsity level from a policy\n",
    "### Idea 1: hack-y heuristics\n",
    "\n",
    "def heuristic_classifier(MDP, policy):\n",
    "    \"\"\"\n",
    "    A heuristic classifier that predicts the sparsity level of an MDP's reward function given its \n",
    "    optimal policy\n",
    "    1. \n",
    "    \"\"\"\n",
    "    # TODO: implement this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 2s 178ms/step - loss: 0.2886 - mae: 0.4491 - val_loss: 0.2103 - val_mae: 0.4115\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.2119 - mae: 0.3814 - val_loss: 0.1499 - val_mae: 0.3210\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.1328 - mae: 0.3095 - val_loss: 0.1607 - val_mae: 0.3308\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.1092 - mae: 0.2767 - val_loss: 0.1121 - val_mae: 0.2833\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.0743 - mae: 0.2234 - val_loss: 0.0593 - val_mae: 0.2177\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0579 - mae: 0.1881 - val_loss: 0.0580 - val_mae: 0.1984\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.0481 - mae: 0.1656 - val_loss: 0.0470 - val_mae: 0.1884\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.0401 - mae: 0.1670 - val_loss: 0.0532 - val_mae: 0.1926\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.0438 - mae: 0.1730 - val_loss: 0.0516 - val_mae: 0.1844\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.0454 - mae: 0.1651 - val_loss: 0.0454 - val_mae: 0.1782\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.0442 - mae: 0.1693 - val_loss: 0.0511 - val_mae: 0.1862\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0349 - mae: 0.1493 - val_loss: 0.0502 - val_mae: 0.1876\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.0342 - mae: 0.1500 - val_loss: 0.0522 - val_mae: 0.1910\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0232 - mae: 0.1287 - val_loss: 0.0539 - val_mae: 0.1915\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0263 - mae: 0.1337 - val_loss: 0.0445 - val_mae: 0.1736\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0214 - mae: 0.1230 - val_loss: 0.0400 - val_mae: 0.1624\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0319 - mae: 0.1399 - val_loss: 0.0409 - val_mae: 0.1694\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0179 - mae: 0.1045 - val_loss: 0.0400 - val_mae: 0.1689\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.0196 - mae: 0.1119 - val_loss: 0.0384 - val_mae: 0.1645\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.0237 - mae: 0.1217 - val_loss: 0.0378 - val_mae: 0.1606\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0229 - mae: 0.1152 - val_loss: 0.0390 - val_mae: 0.1636\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.0215 - mae: 0.1206 - val_loss: 0.0413 - val_mae: 0.1719\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0269 - mae: 0.1272 - val_loss: 0.0432 - val_mae: 0.1767\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.0154 - mae: 0.1042 - val_loss: 0.0441 - val_mae: 0.1784\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0160 - mae: 0.1001 - val_loss: 0.0425 - val_mae: 0.1729\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0245 - mae: 0.1287 - val_loss: 0.0410 - val_mae: 0.1690\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0164 - mae: 0.0980 - val_loss: 0.0408 - val_mae: 0.1671\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0196 - mae: 0.1121 - val_loss: 0.0415 - val_mae: 0.1668\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.0156 - mae: 0.0967 - val_loss: 0.0403 - val_mae: 0.1663\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0131 - mae: 0.0931 - val_loss: 0.0408 - val_mae: 0.1694\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.0201 - mae: 0.1178 - val_loss: 0.0404 - val_mae: 0.1708\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0205 - mae: 0.1166 - val_loss: 0.0422 - val_mae: 0.1744\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0167 - mae: 0.0996 - val_loss: 0.0411 - val_mae: 0.1721\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.0168 - mae: 0.1049 - val_loss: 0.0396 - val_mae: 0.1706\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.0202 - mae: 0.1119 - val_loss: 0.0402 - val_mae: 0.1736\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0148 - mae: 0.0960 - val_loss: 0.0408 - val_mae: 0.1747\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0211 - mae: 0.1113 - val_loss: 0.0397 - val_mae: 0.1681\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0150 - mae: 0.0983 - val_loss: 0.0353 - val_mae: 0.1563\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.0147 - mae: 0.0899 - val_loss: 0.0343 - val_mae: 0.1539\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0165 - mae: 0.1011 - val_loss: 0.0372 - val_mae: 0.1613\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.0153 - mae: 0.0988 - val_loss: 0.0393 - val_mae: 0.1687\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0165 - mae: 0.1006 - val_loss: 0.0373 - val_mae: 0.1641\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.0173 - mae: 0.1038 - val_loss: 0.0350 - val_mae: 0.1582\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.0111 - mae: 0.0816 - val_loss: 0.0350 - val_mae: 0.1593\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.0116 - mae: 0.0879 - val_loss: 0.0364 - val_mae: 0.1642\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0123 - mae: 0.0864 - val_loss: 0.0357 - val_mae: 0.1628\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.0157 - mae: 0.0981 - val_loss: 0.0338 - val_mae: 0.1577\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.0148 - mae: 0.0974 - val_loss: 0.0337 - val_mae: 0.1537\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0112 - mae: 0.0853 - val_loss: 0.0339 - val_mae: 0.1523\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0143 - mae: 0.0908 - val_loss: 0.0345 - val_mae: 0.1498\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 47ms/step - loss: 0.0116 - mae: 0.0848 - val_loss: 0.0335 - val_mae: 0.1469\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0086 - mae: 0.0740 - val_loss: 0.0322 - val_mae: 0.1439\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.0118 - mae: 0.0803 - val_loss: 0.0318 - val_mae: 0.1431\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.0184 - mae: 0.1058 - val_loss: 0.0318 - val_mae: 0.1443\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.0144 - mae: 0.0963 - val_loss: 0.0352 - val_mae: 0.1499\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0140 - mae: 0.0935 - val_loss: 0.0364 - val_mae: 0.1521\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.0094 - mae: 0.0778 - val_loss: 0.0325 - val_mae: 0.1464\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.0133 - mae: 0.0914 - val_loss: 0.0319 - val_mae: 0.1439\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 46ms/step - loss: 0.0140 - mae: 0.0914 - val_loss: 0.0317 - val_mae: 0.1460\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.0101 - mae: 0.0790 - val_loss: 0.0354 - val_mae: 0.1537\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.0083 - mae: 0.0711 - val_loss: 0.0421 - val_mae: 0.1703\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.0144 - mae: 0.0986 - val_loss: 0.0410 - val_mae: 0.1682\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0128 - mae: 0.0880 - val_loss: 0.0361 - val_mae: 0.1580\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0121 - mae: 0.0822 - val_loss: 0.0349 - val_mae: 0.1555\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0144 - mae: 0.0972 - val_loss: 0.0354 - val_mae: 0.1571\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0153 - mae: 0.0950 - val_loss: 0.0388 - val_mae: 0.1647\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.0094 - mae: 0.0731 - val_loss: 0.0411 - val_mae: 0.1721\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.0086 - mae: 0.0748 - val_loss: 0.0382 - val_mae: 0.1647\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.0116 - mae: 0.0858 - val_loss: 0.0363 - val_mae: 0.1596\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0124 - mae: 0.0890 - val_loss: 0.0359 - val_mae: 0.1585\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0112 - mae: 0.0820 - val_loss: 0.0368 - val_mae: 0.1602\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0110 - mae: 0.0804 - val_loss: 0.0367 - val_mae: 0.1592\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.0112 - mae: 0.0864 - val_loss: 0.0351 - val_mae: 0.1545\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.0113 - mae: 0.0804 - val_loss: 0.0356 - val_mae: 0.1560\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.0120 - mae: 0.0829 - val_loss: 0.0375 - val_mae: 0.1612\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.0113 - mae: 0.0767 - val_loss: 0.0384 - val_mae: 0.1630\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.0111 - mae: 0.0842 - val_loss: 0.0372 - val_mae: 0.1605\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0125 - mae: 0.0892 - val_loss: 0.0387 - val_mae: 0.1631\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.0114 - mae: 0.0760 - val_loss: 0.0376 - val_mae: 0.1614\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0111 - mae: 0.0825 - val_loss: 0.0364 - val_mae: 0.1579\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.0104 - mae: 0.0834 - val_loss: 0.0380 - val_mae: 0.1618\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.0117 - mae: 0.0837 - val_loss: 0.0372 - val_mae: 0.1589\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.0095 - mae: 0.0749 - val_loss: 0.0343 - val_mae: 0.1495\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.0100 - mae: 0.0764 - val_loss: 0.0343 - val_mae: 0.1468\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.0082 - mae: 0.0679 - val_loss: 0.0380 - val_mae: 0.1586\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0119 - mae: 0.0822 - val_loss: 0.0388 - val_mae: 0.1617\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 43ms/step - loss: 0.0078 - mae: 0.0721 - val_loss: 0.0368 - val_mae: 0.1567\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 40ms/step - loss: 0.0100 - mae: 0.0754 - val_loss: 0.0355 - val_mae: 0.1542\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.0091 - mae: 0.0762 - val_loss: 0.0366 - val_mae: 0.1585\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0077 - mae: 0.0711 - val_loss: 0.0375 - val_mae: 0.1620\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0099 - mae: 0.0764 - val_loss: 0.0362 - val_mae: 0.1584\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.0092 - mae: 0.0714 - val_loss: 0.0350 - val_mae: 0.1545\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.0086 - mae: 0.0756 - val_loss: 0.0358 - val_mae: 0.1531\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.0073 - mae: 0.0642 - val_loss: 0.0352 - val_mae: 0.1491\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0097 - mae: 0.0806 - val_loss: 0.0348 - val_mae: 0.1476\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.0110 - mae: 0.0774 - val_loss: 0.0342 - val_mae: 0.1484\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.0110 - mae: 0.0816 - val_loss: 0.0365 - val_mae: 0.1565\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.0099 - mae: 0.0765 - val_loss: 0.0370 - val_mae: 0.1601\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0091 - mae: 0.0758 - val_loss: 0.0373 - val_mae: 0.1622\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.0092 - mae: 0.0746 - val_loss: 0.0363 - val_mae: 0.1604\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "Predicted sparsity level for MDP 0: [[0.6577945]], actual sparsity level: 0.72, Squared error: 0.0038695307448506355\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "Predicted sparsity level for MDP 1: [[0.49202517]], actual sparsity level: 0.05, Squared error: 0.19538623094558716\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Predicted sparsity level for MDP 2: [[0.37456548]], actual sparsity level: 0.37, Squared error: 2.0843583115492947e-05\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "Predicted sparsity level for MDP 3: [[0.16752496]], actual sparsity level: 0.21, Squared error: 0.0018041281728073955\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "Predicted sparsity level for MDP 4: [[0.55395967]], actual sparsity level: 0.46, Squared error: 0.008828417398035526\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Predicted sparsity level for MDP 5: [[0.21666586]], actual sparsity level: 0.02, Squared error: 0.03867746517062187\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "Predicted sparsity level for MDP 6: [[0.2874832]], actual sparsity level: 0.1, Squared error: 0.035149946808815\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "Predicted sparsity level for MDP 7: [[0.5337472]], actual sparsity level: 0.88, Squared error: 0.11989100277423859\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "Predicted sparsity level for MDP 8: [[0.6351561]], actual sparsity level: 0.34, Squared error: 0.08711712062358856\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "Predicted sparsity level for MDP 9: [[0.4197957]], actual sparsity level: 0.07, Squared error: 0.12235703319311142\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "Predicted sparsity level for MDP 10: [[0.36399686]], actual sparsity level: 0.08, Squared error: 0.08065422624349594\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Predicted sparsity level for MDP 11: [[0.46283513]], actual sparsity level: 0.25, Squared error: 0.04529879242181778\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Predicted sparsity level for MDP 12: [[0.34616157]], actual sparsity level: 0.73, Squared error: 0.14733195304870605\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Predicted sparsity level for MDP 13: [[0.31089923]], actual sparsity level: 0.11, Squared error: 0.040360499173402786\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Predicted sparsity level for MDP 14: [[0.20852771]], actual sparsity level: 0.44, Squared error: 0.0535794198513031\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Predicted sparsity level for MDP 15: [[0.39031106]], actual sparsity level: 0.36, Squared error: 0.0009187596151605248\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Predicted sparsity level for MDP 16: [[0.76581484]], actual sparsity level: 0.97, Squared error: 0.04169159010052681\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Predicted sparsity level for MDP 17: [[0.22808534]], actual sparsity level: 0.38, Squared error: 0.023078063502907753\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Predicted sparsity level for MDP 18: [[0.32745948]], actual sparsity level: 0.4, Squared error: 0.00526212714612484\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Predicted sparsity level for MDP 19: [[0.9148797]], actual sparsity level: 0.96, Squared error: 0.0020358413457870483\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Predicted sparsity level for MDP 20: [[0.3495216]], actual sparsity level: 0.57, Squared error: 0.04861071705818176\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "Predicted sparsity level for MDP 21: [[0.5050661]], actual sparsity level: 0.8, Squared error: 0.0869860127568245\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Predicted sparsity level for MDP 22: [[0.4371204]], actual sparsity level: 0.15, Squared error: 0.08243812620639801\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted sparsity level for MDP 23: [[0.7222604]], actual sparsity level: 0.77, Squared error: 0.002279066015034914\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted sparsity level for MDP 24: [[0.4092915]], actual sparsity level: 0.12, Squared error: 0.08368957042694092\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Predicted sparsity level for MDP 25: [[0.6044059]], actual sparsity level: 0.59, Squared error: 0.00020753013086505234\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted sparsity level for MDP 26: [[0.15951976]], actual sparsity level: 0.43, Squared error: 0.07315956056118011\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Predicted sparsity level for MDP 27: [[0.9197523]], actual sparsity level: 0.98, Squared error: 0.003629787592217326\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted sparsity level for MDP 28: [[0.61963147]], actual sparsity level: 0.99, Squared error: 0.1371728628873825\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Predicted sparsity level for MDP 29: [[0.2894994]], actual sparsity level: 0.13, Squared error: 0.02544006146490574\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted sparsity level for MDP 30: [[0.6817609]], actual sparsity level: 0.78, Squared error: 0.009650913998484612\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted sparsity level for MDP 31: [[0.3836156]], actual sparsity level: 0.33, Squared error: 0.0028746325988322496\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted sparsity level for MDP 32: [[0.28127322]], actual sparsity level: 0.7, Squared error: 0.17533211410045624\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Predicted sparsity level for MDP 33: [[0.4065396]], actual sparsity level: 0.39, Squared error: 0.00027355848578736186\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted sparsity level for MDP 34: [[0.8251519]], actual sparsity level: 0.76, Squared error: 0.004244774114340544\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted sparsity level for MDP 35: [[0.9546623]], actual sparsity level: 0.94, Squared error: 0.00021498378191608936\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted sparsity level for MDP 36: [[0.65311694]], actual sparsity level: 0.71, Squared error: 0.003235679818317294\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Predicted sparsity level for MDP 37: [[0.9162616]], actual sparsity level: 0.82, Squared error: 0.009266299195587635\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "Predicted sparsity level for MDP 38: [[0.6638064]], actual sparsity level: 0.92, Squared error: 0.06563518196344376\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Predicted sparsity level for MDP 39: [[0.28571072]], actual sparsity level: 0.27, Squared error: 0.0002468264428898692\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted sparsity level for MDP 40: [[0.3310611]], actual sparsity level: 0.16, Squared error: 0.02926190011203289\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted sparsity level for MDP 41: [[0.7551516]], actual sparsity level: 0.85, Squared error: 0.008996217511594296\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted sparsity level for MDP 42: [[0.6771136]], actual sparsity level: 0.81, Squared error: 0.017658798024058342\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted sparsity level for MDP 43: [[0.75285965]], actual sparsity level: 0.68, Squared error: 0.0053085279650986195\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted sparsity level for MDP 44: [[0.70096606]], actual sparsity level: 0.48, Squared error: 0.048826005309820175\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted sparsity level for MDP 45: [[0.86657417]], actual sparsity level: 0.84, Squared error: 0.0007061877986416221\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted sparsity level for MDP 46: [[0.16472268]], actual sparsity level: 0.09, Squared error: 0.005583478603512049\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted sparsity level for MDP 47: [[0.46909508]], actual sparsity level: 0.5, Squared error: 0.0009551140246912837\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted sparsity level for MDP 48: [[0.32860303]], actual sparsity level: 0.42, Squared error: 0.008353403769433498\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted sparsity level for MDP 49: [[0.280523]], actual sparsity level: 0.28, Squared error: 2.73529991545729e-07\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted sparsity level for MDP 50: [[0.5432164]], actual sparsity level: 0.61, Squared error: 0.004460050258785486\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Predicted sparsity level for MDP 51: [[0.7079895]], actual sparsity level: 0.9, Squared error: 0.03686801716685295\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Predicted sparsity level for MDP 52: [[0.29399112]], actual sparsity level: 0.18, Squared error: 0.012993973679840565\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Predicted sparsity level for MDP 53: [[0.40436837]], actual sparsity level: 0.24, Squared error: 0.027016963809728622\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Predicted sparsity level for MDP 54: [[0.2644713]], actual sparsity level: 0.04, Squared error: 0.050387363880872726\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted sparsity level for MDP 55: [[0.293563]], actual sparsity level: 0.17, Squared error: 0.015267816372215748\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Predicted sparsity level for MDP 56: [[0.2713083]], actual sparsity level: 0.0, Squared error: 0.07360819727182388\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted sparsity level for MDP 57: [[0.46831125]], actual sparsity level: 0.67, Squared error: 0.04067835956811905\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Predicted sparsity level for MDP 58: [[0.35794622]], actual sparsity level: 0.63, Squared error: 0.07401325553655624\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted sparsity level for MDP 59: [[0.66284734]], actual sparsity level: 0.74, Squared error: 0.005952534265816212\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted sparsity level for MDP 60: [[0.314111]], actual sparsity level: 0.03, Squared error: 0.08071905374526978\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Predicted sparsity level for MDP 61: [[0.857721]], actual sparsity level: 0.93, Squared error: 0.005224259104579687\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Predicted sparsity level for MDP 62: [[0.64348376]], actual sparsity level: 0.56, Squared error: 0.00696953758597374\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Predicted sparsity level for MDP 63: [[0.46063793]], actual sparsity level: 0.01, Squared error: 0.20307454466819763\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted sparsity level for MDP 64: [[0.27637842]], actual sparsity level: 0.14, Squared error: 0.018599074333906174\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Predicted sparsity level for MDP 65: [[0.19169569]], actual sparsity level: 0.32, Squared error: 0.016461994498968124\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Predicted sparsity level for MDP 66: [[0.32588047]], actual sparsity level: 0.53, Squared error: 0.04166477173566818\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Predicted sparsity level for MDP 67: [[0.30016598]], actual sparsity level: 0.45, Squared error: 0.02245022915303707\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "Predicted sparsity level for MDP 68: [[0.6918845]], actual sparsity level: 0.95, Squared error: 0.06662359833717346\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Predicted sparsity level for MDP 69: [[0.42668974]], actual sparsity level: 0.86, Squared error: 0.18775779008865356\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "Predicted sparsity level for MDP 70: [[0.4332025]], actual sparsity level: 0.31, Squared error: 0.015178856439888477\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Predicted sparsity level for MDP 71: [[0.595914]], actual sparsity level: 0.22, Squared error: 0.14131134748458862\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Predicted sparsity level for MDP 72: [[0.76278937]], actual sparsity level: 0.55, Squared error: 0.045279309153556824\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Predicted sparsity level for MDP 73: [[0.5128947]], actual sparsity level: 0.51, Squared error: 8.379285645787604e-06\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Predicted sparsity level for MDP 74: [[0.20908976]], actual sparsity level: 0.06, Squared error: 0.022227754816412926\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Predicted sparsity level for MDP 75: [[0.36828202]], actual sparsity level: 0.29, Squared error: 0.006128075998276472\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Predicted sparsity level for MDP 76: [[0.2758464]], actual sparsity level: 0.52, Squared error: 0.05961097404360771\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Predicted sparsity level for MDP 77: [[0.48510775]], actual sparsity level: 0.65, Squared error: 0.02718944661319256\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Predicted sparsity level for MDP 78: [[0.6120215]], actual sparsity level: 0.83, Squared error: 0.04751461744308472\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "Predicted sparsity level for MDP 79: [[0.78432065]], actual sparsity level: 0.91, Squared error: 0.015795305371284485\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Predicted sparsity level for MDP 80: [[0.6522465]], actual sparsity level: 0.87, Squared error: 0.047416601330041885\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Predicted sparsity level for MDP 81: [[0.2746641]], actual sparsity level: 0.47, Squared error: 0.03815611079335213\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "Predicted sparsity level for MDP 82: [[0.6928781]], actual sparsity level: 0.79, Squared error: 0.009432662278413773\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "Predicted sparsity level for MDP 83: [[0.2450132]], actual sparsity level: 0.19, Squared error: 0.00302645331248641\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Predicted sparsity level for MDP 84: [[0.47403756]], actual sparsity level: 0.26, Squared error: 0.045812081545591354\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Predicted sparsity level for MDP 85: [[0.44523415]], actual sparsity level: 0.62, Squared error: 0.03054310381412506\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Predicted sparsity level for MDP 86: [[0.41906613]], actual sparsity level: 0.41, Squared error: 8.219479786930606e-05\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "Predicted sparsity level for MDP 87: [[0.17421931]], actual sparsity level: 0.2, Squared error: 0.0006646441179327667\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "Predicted sparsity level for MDP 88: [[0.42518505]], actual sparsity level: 0.3, Squared error: 0.015671294182538986\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Predicted sparsity level for MDP 89: [[0.6105978]], actual sparsity level: 0.89, Squared error: 0.07806558907032013\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Predicted sparsity level for MDP 90: [[0.51453656]], actual sparsity level: 0.69, Squared error: 0.03078741766512394\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Predicted sparsity level for MDP 91: [[0.26275927]], actual sparsity level: 0.23, Squared error: 0.0010731694055721164\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Predicted sparsity level for MDP 92: [[0.41975734]], actual sparsity level: 0.35, Squared error: 0.004866086877882481\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Predicted sparsity level for MDP 93: [[0.27703568]], actual sparsity level: 0.54, Squared error: 0.06915024667978287\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "Predicted sparsity level for MDP 94: [[0.46106136]], actual sparsity level: 0.6, Squared error: 0.019303953275084496\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Predicted sparsity level for MDP 95: [[0.32750505]], actual sparsity level: 0.58, Squared error: 0.06375368684530258\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "Predicted sparsity level for MDP 96: [[0.6781639]], actual sparsity level: 0.49, Squared error: 0.03540564328432083\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Predicted sparsity level for MDP 97: [[0.49224597]], actual sparsity level: 0.75, Squared error: 0.06643714010715485\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "Predicted sparsity level for MDP 98: [[0.5751592]], actual sparsity level: 0.64, Squared error: 0.004204328637570143\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Predicted sparsity level for MDP 99: [[0.42200676]], actual sparsity level: 0.66, Squared error: 0.05664079636335373\n",
      "Mean squared error: 0.03985079814208262\n",
      "Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\n"
     ]
    }
   ],
   "source": [
    "### Idea 2: neural network\n",
    "# Thanks again ChatGPT for outlining the code structure\n",
    "\n",
    "NUM_MDPs = 1000\n",
    "sparsity, MDPs = generate_tests()\n",
    "# print(np.array(MDPs[0].P).shape)\n",
    "training_data = [(np.array(mdp.P), mdp.discount, mdp.policy, sparsity[i]) for i, mdp in enumerate(MDPs)]\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Step 1: Feature extraction function\n",
    "def extract_features(transition_function, discount_rate, optimal_policy):\n",
    "    \"\"\"\n",
    "    Extract features from the MDP's transition function, discount rate, and optimal policy\n",
    "    \"\"\"\n",
    "    # opt_policy = optimal_policy.reshape(-1, 1)  # Reshape for sklearn which expects 2D input\n",
    "\n",
    "    # # Initialize the OneHotEncoder\n",
    "    # encoder = OneHotEncoder(sparse=False)  # Use sparse=False to get a dense array\n",
    "\n",
    "    # # Fit and transform\n",
    "    # opt_policy_one_hot = encoder.fit_transform(opt_policy)\n",
    "    features = np.concatenate((transition_function.flatten(), [discount_rate], optimal_policy.flatten()))\n",
    "    # print(features.shape)\n",
    "    # length 10*10*4 + 1 + 10 = 411\n",
    "\n",
    "    # Placeholder features\n",
    "    # features = np.random.rand(411)\n",
    "    return features\n",
    "\n",
    "# Step 2: Data preparation (assuming you have your data in an appropriate format)\n",
    "# This is a placeholder function - you would replace it with actual data loading and processing\n",
    "def prepare_data(training_data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for transition_function, discount_rate, optimal_policy, sparsity_level in training_data:\n",
    "        features.append(extract_features(transition_function, discount_rate, optimal_policy))\n",
    "        labels.append(sparsity_level)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Step 3: Model selection\n",
    "\n",
    "def build_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='linear')  # Linear activation for regression output\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='mean_squared_error',  # Suitable for regression\n",
    "                  metrics=['mae'])  # Mean Absolute Error as an additional metric\n",
    "    return model\n",
    "\n",
    "# Assuming you have already defined the feature extraction and data preparation functions\n",
    "# and have your data ready in 'features' and 'labels':\n",
    "features, labels = prepare_data(training_data)\n",
    "# Example: features shape is (num_samples, num_features), adjust 'input_dim' accordingly\n",
    "input_dim = features.shape[1]  # Assuming 'features' is already defined and preprocessed\n",
    "\n",
    "model = build_model(input_dim)\n",
    "\n",
    "# Training the model\n",
    "model.fit(features, labels, epochs=100, validation_split=0.2, verbose = 1)\n",
    "\n",
    "# Don't forget to preprocess your new data before making predictions\n",
    "# predicted_sparsity = model.predict(new_features)\n",
    "\n",
    "# Step 4: Training the model (placeholder for training data)\n",
    "# training_data = load_your_data_somehow()\n",
    "# features, labels = prepare_data(training_data)\n",
    "# model.fit(features, labels)\n",
    "\n",
    "# Step 5: Prediction function\n",
    "def predict_sparsity(transition_function, discount_rate, optimal_policy):\n",
    "    features = extract_features(transition_function, discount_rate, optimal_policy).reshape(1, -1)\n",
    "    predicted_sparsity = model.predict(features)\n",
    "    return predicted_sparsity\n",
    "\n",
    "# Note: The actual training step and data preparation would depend on your specific dataset and environment setup.\n",
    "test_sparsity, test_MDPs = generate_tests()\n",
    "test_data = [(np.array(mdp.P), mdp.discount, mdp.policy) for mdp in (test_MDPs)]\n",
    "NUM_TESTS = 100\n",
    "mse = np.zeros(NUM_TESTS)\n",
    "\n",
    "for i in range(min(NUM_TESTS, len(test_data))):\n",
    "    transition_function, discount_rate, optimal_policy = test_data[i]\n",
    "    prediction = predict_sparsity(transition_function, discount_rate, optimal_policy)\n",
    "    mse[i] = (prediction - test_sparsity[i])**2\n",
    "    print(f\"Predicted sparsity level for MDP {i}: {prediction}, actual sparsity level: {test_sparsity[i]}, Squared error: {mse[i]}\")\n",
    "\n",
    "print(f\"Mean squared error: {np.mean(mse)}\")\n",
    "print(\"Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a control, when the input layer (with same dimension as transition_function + discount rate + optimal policy) is randomized, MSE = ~0.115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.1231347215487631\n",
      "Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\n"
     ]
    }
   ],
   "source": [
    "### Idea 3: Multiple linear regression \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "sparsity, MDPs = generate_tests()\n",
    "# print(np.array(MDPs[0].P).shape)\n",
    "training_data = [(np.array(mdp.P), mdp.discount, mdp.policy, sparsity[i]) for i, mdp in enumerate(MDPs)]\n",
    "features, labels = prepare_data(training_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean squared error: {mse}\")\n",
    "print(\"Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
