{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox as mdpt, numpy as np\n",
    "import mdptoolbox.example\n",
    "import MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a bunch of MDPs with different parameters, sparsity\n",
    "\n",
    "NUM_MDPs = 100\n",
    "NUM_STATES = 10\n",
    "NUM_ACTIONS = 4\n",
    "\n",
    "def get_transition_matrix(num_states, num_actions, generator = np.random.dirichlet):\n",
    "    P = np.zeros((num_actions, num_states, num_states)) # (A, S, S) shape\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            P[a, s, :] = generator(np.ones(num_states))\n",
    "    return P\n",
    "\n",
    "def get_reward_matrix(num_states, num_actions, sparsity = 0.0, generator = np.random.normal):\n",
    "    R = np.zeros((num_states, num_actions))\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            if np.random.rand() < sparsity:\n",
    "                R[s, a] = 0\n",
    "            else:\n",
    "                R[s, a] = generator()\n",
    "    return R\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "EPSILON = 0.01\n",
    "MAX_ITER = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests(num_mdps = NUM_MDPs, sparsity_levels = np.arange(NUM_MDPs) / NUM_MDPs, mdp_generator = mdpt.mdp.PolicyIteration):\n",
    "    \"\"\"\n",
    "    Generate a bunch of MDPs with different sparsity levels, and return the sparsity levels and the MDPs\n",
    "\n",
    "    Args:\n",
    "        sparsity_levels: a list of sparsity levels to generate MDPs with\n",
    "    Returns:\n",
    "        sparsity_levels: the sparsity levels used to generate the MDPs, in the same order as the MDPs\n",
    "        MDPS: an array of MDPs\n",
    "    \"\"\"\n",
    "    sparsity_copy = sparsity_levels.copy() # defensive copy\n",
    "    np.random.shuffle(sparsity_copy)\n",
    "    MDPS = np.array([mdp_generator(\n",
    "        get_transition_matrix(NUM_STATES, NUM_ACTIONS), \n",
    "        get_reward_matrix(NUM_STATES, NUM_ACTIONS, sparsity_copy[i]), \n",
    "        DISCOUNT, max_iter = MAX_ITER) \n",
    "        for i in range(num_mdps)\n",
    "    ])\n",
    "    return sparsity_copy, MDPS\n",
    "\n",
    "sparsity_levels, MDPS = generate_tests()\n",
    "for mdp in MDPS:\n",
    "    mdp.run()\n",
    "    # print(mdp.policy) # debug\n",
    "# print(MDPS[0].policy) # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a classifier to predict sparsity level from a policy\n",
    "### Idea 1: hack-y heuristics\n",
    "\n",
    "def heuristic_classifier(MDP, policy):\n",
    "    \"\"\"\n",
    "    A heuristic classifier that predicts the sparsity level of an MDP's reward function given its \n",
    "    optimal policy\n",
    "    1. \n",
    "    \"\"\"\n",
    "    # TODO: implement this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 113ms/step - loss: 0.6775 - mae: 0.6841 - val_loss: 0.4080 - val_mae: 0.5651\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.4630 - mae: 0.5594 - val_loss: 0.3514 - val_mae: 0.4985\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4387 - mae: 0.4931 - val_loss: 0.3519 - val_mae: 0.5022\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4114 - mae: 0.5313 - val_loss: 0.3162 - val_mae: 0.4716\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3300 - mae: 0.4757 - val_loss: 0.2789 - val_mae: 0.4262\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2776 - mae: 0.4187 - val_loss: 0.2594 - val_mae: 0.3989\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2593 - mae: 0.4191 - val_loss: 0.2508 - val_mae: 0.4014\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2877 - mae: 0.4540 - val_loss: 0.2438 - val_mae: 0.4016\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2231 - mae: 0.3817 - val_loss: 0.2372 - val_mae: 0.4002\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1655 - mae: 0.3158 - val_loss: 0.2295 - val_mae: 0.3966\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2241 - mae: 0.3597 - val_loss: 0.2225 - val_mae: 0.3923\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1903 - mae: 0.3488 - val_loss: 0.2163 - val_mae: 0.3850\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1976 - mae: 0.3696 - val_loss: 0.2123 - val_mae: 0.3772\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 41ms/step - loss: 0.1777 - mae: 0.3454 - val_loss: 0.2108 - val_mae: 0.3703\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.1851 - mae: 0.3406 - val_loss: 0.2085 - val_mae: 0.3674\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1523 - mae: 0.3061 - val_loss: 0.2056 - val_mae: 0.3661\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1698 - mae: 0.3309 - val_loss: 0.2025 - val_mae: 0.3648\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.1420 - mae: 0.3003 - val_loss: 0.1994 - val_mae: 0.3655\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1376 - mae: 0.3051 - val_loss: 0.1970 - val_mae: 0.3629\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.1481 - mae: 0.3138 - val_loss: 0.1937 - val_mae: 0.3599\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1472 - mae: 0.3034 - val_loss: 0.1895 - val_mae: 0.3592\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1393 - mae: 0.3014 - val_loss: 0.1852 - val_mae: 0.3584\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1172 - mae: 0.2822 - val_loss: 0.1812 - val_mae: 0.3587\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1206 - mae: 0.2840 - val_loss: 0.1790 - val_mae: 0.3591\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1309 - mae: 0.2902 - val_loss: 0.1764 - val_mae: 0.3562\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1090 - mae: 0.2593 - val_loss: 0.1733 - val_mae: 0.3528\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1076 - mae: 0.2645 - val_loss: 0.1691 - val_mae: 0.3467\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1167 - mae: 0.2698 - val_loss: 0.1654 - val_mae: 0.3398\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0968 - mae: 0.2441 - val_loss: 0.1641 - val_mae: 0.3370\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0992 - mae: 0.2602 - val_loss: 0.1619 - val_mae: 0.3349\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1072 - mae: 0.2549 - val_loss: 0.1593 - val_mae: 0.3329\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0986 - mae: 0.2422 - val_loss: 0.1568 - val_mae: 0.3305\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1011 - mae: 0.2425 - val_loss: 0.1552 - val_mae: 0.3276\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0896 - mae: 0.2397 - val_loss: 0.1535 - val_mae: 0.3262\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0742 - mae: 0.2224 - val_loss: 0.1530 - val_mae: 0.3259\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1149 - mae: 0.2677 - val_loss: 0.1519 - val_mae: 0.3259\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0911 - mae: 0.2367 - val_loss: 0.1503 - val_mae: 0.3255\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0847 - mae: 0.2384 - val_loss: 0.1482 - val_mae: 0.3234\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0986 - mae: 0.2510 - val_loss: 0.1459 - val_mae: 0.3206\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0806 - mae: 0.2407 - val_loss: 0.1432 - val_mae: 0.3178\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.1041 - mae: 0.2557 - val_loss: 0.1399 - val_mae: 0.3145\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0980 - mae: 0.2618 - val_loss: 0.1373 - val_mae: 0.3120\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0848 - mae: 0.2252 - val_loss: 0.1344 - val_mae: 0.3098\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0807 - mae: 0.2175 - val_loss: 0.1312 - val_mae: 0.3075\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0848 - mae: 0.2392 - val_loss: 0.1288 - val_mae: 0.3060\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0738 - mae: 0.2101 - val_loss: 0.1273 - val_mae: 0.3048\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0785 - mae: 0.2206 - val_loss: 0.1271 - val_mae: 0.3042\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0853 - mae: 0.2329 - val_loss: 0.1272 - val_mae: 0.3040\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0693 - mae: 0.2068 - val_loss: 0.1271 - val_mae: 0.3040\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0585 - mae: 0.1999 - val_loss: 0.1264 - val_mae: 0.3022\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0722 - mae: 0.2153 - val_loss: 0.1260 - val_mae: 0.3008\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0706 - mae: 0.2050 - val_loss: 0.1267 - val_mae: 0.3009\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0688 - mae: 0.2029 - val_loss: 0.1266 - val_mae: 0.3006\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0567 - mae: 0.1901 - val_loss: 0.1265 - val_mae: 0.3002\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0752 - mae: 0.2102 - val_loss: 0.1255 - val_mae: 0.3001\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0661 - mae: 0.2070 - val_loss: 0.1248 - val_mae: 0.3007\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0512 - mae: 0.1804 - val_loss: 0.1241 - val_mae: 0.3008\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0555 - mae: 0.1967 - val_loss: 0.1228 - val_mae: 0.2997\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0602 - mae: 0.2030 - val_loss: 0.1217 - val_mae: 0.2987\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0537 - mae: 0.1846 - val_loss: 0.1206 - val_mae: 0.2974\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0481 - mae: 0.1799 - val_loss: 0.1191 - val_mae: 0.2954\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0605 - mae: 0.2002 - val_loss: 0.1176 - val_mae: 0.2930\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0621 - mae: 0.2017 - val_loss: 0.1167 - val_mae: 0.2911\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0605 - mae: 0.1936 - val_loss: 0.1161 - val_mae: 0.2896\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0720 - mae: 0.2128 - val_loss: 0.1157 - val_mae: 0.2889\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0649 - mae: 0.2042 - val_loss: 0.1156 - val_mae: 0.2890\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0698 - mae: 0.2053 - val_loss: 0.1149 - val_mae: 0.2877\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0742 - mae: 0.2182 - val_loss: 0.1147 - val_mae: 0.2862\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0537 - mae: 0.1925 - val_loss: 0.1143 - val_mae: 0.2852\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0513 - mae: 0.1797 - val_loss: 0.1138 - val_mae: 0.2851\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0501 - mae: 0.1769 - val_loss: 0.1132 - val_mae: 0.2858\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0592 - mae: 0.2041 - val_loss: 0.1132 - val_mae: 0.2865\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0424 - mae: 0.1654 - val_loss: 0.1127 - val_mae: 0.2861\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0486 - mae: 0.1753 - val_loss: 0.1118 - val_mae: 0.2858\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0497 - mae: 0.1712 - val_loss: 0.1106 - val_mae: 0.2844\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0591 - mae: 0.1943 - val_loss: 0.1092 - val_mae: 0.2831\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0531 - mae: 0.1859 - val_loss: 0.1082 - val_mae: 0.2830\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0643 - mae: 0.2021 - val_loss: 0.1071 - val_mae: 0.2824\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0585 - mae: 0.1884 - val_loss: 0.1054 - val_mae: 0.2811\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0434 - mae: 0.1679 - val_loss: 0.1040 - val_mae: 0.2798\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0497 - mae: 0.1764 - val_loss: 0.1025 - val_mae: 0.2782\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0595 - mae: 0.1941 - val_loss: 0.1021 - val_mae: 0.2780\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0483 - mae: 0.1748 - val_loss: 0.1020 - val_mae: 0.2780\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0389 - mae: 0.1620 - val_loss: 0.1022 - val_mae: 0.2782\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0471 - mae: 0.1779 - val_loss: 0.1025 - val_mae: 0.2782\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0441 - mae: 0.1727 - val_loss: 0.1015 - val_mae: 0.2766\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0564 - mae: 0.1917 - val_loss: 0.1005 - val_mae: 0.2744\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0395 - mae: 0.1550 - val_loss: 0.1002 - val_mae: 0.2732\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0493 - mae: 0.1777 - val_loss: 0.1006 - val_mae: 0.2720\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0434 - mae: 0.1702 - val_loss: 0.1014 - val_mae: 0.2715\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0519 - mae: 0.1716 - val_loss: 0.1017 - val_mae: 0.2707\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0425 - mae: 0.1601 - val_loss: 0.1007 - val_mae: 0.2703\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.0373 - mae: 0.1574 - val_loss: 0.1000 - val_mae: 0.2703\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0492 - mae: 0.1771 - val_loss: 0.0992 - val_mae: 0.2703\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0380 - mae: 0.1578 - val_loss: 0.0988 - val_mae: 0.2708\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0362 - mae: 0.1538 - val_loss: 0.0984 - val_mae: 0.2709\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0412 - mae: 0.1539 - val_loss: 0.0979 - val_mae: 0.2702\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0433 - mae: 0.1595 - val_loss: 0.0975 - val_mae: 0.2689\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0396 - mae: 0.1627 - val_loss: 0.0967 - val_mae: 0.2671\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0402 - mae: 0.1537 - val_loss: 0.0963 - val_mae: 0.2655\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "Predicted sparsity level for MDP 0: [[0.680024]], actual sparsity level: 0.42, Squared error: 0.06761249899864197\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 1: [[0.6188769]], actual sparsity level: 0.43, Squared error: 0.03567447140812874\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 2: [[0.19024873]], actual sparsity level: 0.39, Squared error: 0.03990056365728378\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 3: [[0.5511433]], actual sparsity level: 0.65, Squared error: 0.009772644378244877\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 4: [[0.25496835]], actual sparsity level: 0.31, Squared error: 0.0030284833628684282\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 5: [[0.4957712]], actual sparsity level: 0.55, Squared error: 0.002940764185041189\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 6: [[0.30140752]], actual sparsity level: 0.32, Squared error: 0.0003456801932770759\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 7: [[0.7146187]], actual sparsity level: 0.88, Squared error: 0.027350978925824165\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 8: [[0.4100015]], actual sparsity level: 0.66, Squared error: 0.0624992698431015\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 9: [[0.61760354]], actual sparsity level: 0.74, Squared error: 0.014980895444750786\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 10: [[0.5957407]], actual sparsity level: 0.0, Squared error: 0.35490694642066956\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 11: [[0.39988184]], actual sparsity level: 0.64, Squared error: 0.057656724005937576\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 12: [[0.3663995]], actual sparsity level: 0.8, Squared error: 0.18800941109657288\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 13: [[0.59536195]], actual sparsity level: 0.92, Squared error: 0.10538987815380096\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 14: [[0.2958424]], actual sparsity level: 0.36, Squared error: 0.004116198513656855\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted sparsity level for MDP 15: [[0.41021165]], actual sparsity level: 0.72, Squared error: 0.0959688350558281\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 16: [[0.19672647]], actual sparsity level: 0.6, Squared error: 0.16262955963611603\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 17: [[0.5068855]], actual sparsity level: 0.83, Squared error: 0.10440295189619064\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Predicted sparsity level for MDP 18: [[0.16387835]], actual sparsity level: 0.45, Squared error: 0.081865593791008\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted sparsity level for MDP 19: [[0.6573457]], actual sparsity level: 0.35, Squared error: 0.09446138888597488\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 20: [[0.6458126]], actual sparsity level: 0.63, Squared error: 0.000250037555815652\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 21: [[0.7325394]], actual sparsity level: 0.69, Squared error: 0.001809602021239698\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted sparsity level for MDP 22: [[0.14637592]], actual sparsity level: 0.18, Squared error: 0.0011305789230391383\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted sparsity level for MDP 23: [[0.43735912]], actual sparsity level: 0.24, Squared error: 0.03895062580704689\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted sparsity level for MDP 24: [[0.45558518]], actual sparsity level: 0.5, Squared error: 0.001972676021978259\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted sparsity level for MDP 25: [[0.26853293]], actual sparsity level: 0.48, Squared error: 0.044718317687511444\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted sparsity level for MDP 26: [[0.7146187]], actual sparsity level: 0.97, Squared error: 0.06521963328123093\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 27: [[0.7573046]], actual sparsity level: 0.9, Squared error: 0.020361967384815216\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 28: [[0.22944064]], actual sparsity level: 0.38, Squared error: 0.02266811765730381\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 29: [[0.36292887]], actual sparsity level: 0.46, Squared error: 0.009422806091606617\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 30: [[0.25640848]], actual sparsity level: 0.3, Squared error: 0.0019002214539796114\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 31: [[0.26023588]], actual sparsity level: 0.34, Squared error: 0.006362316198647022\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 32: [[0.3755076]], actual sparsity level: 0.12, Squared error: 0.0652841255068779\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 33: [[0.24655949]], actual sparsity level: 0.58, Squared error: 0.1111825555562973\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 34: [[0.5346811]], actual sparsity level: 0.47, Squared error: 0.004183642566204071\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 35: [[0.45850027]], actual sparsity level: 0.56, Squared error: 0.010302196256816387\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 36: [[0.23098052]], actual sparsity level: 0.06, Squared error: 0.02923433668911457\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 37: [[0.3714332]], actual sparsity level: 0.05, Squared error: 0.10331929475069046\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted sparsity level for MDP 38: [[0.6192025]], actual sparsity level: 0.91, Squared error: 0.08456320315599442\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 39: [[0.3500609]], actual sparsity level: 0.17, Squared error: 0.03242193162441254\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted sparsity level for MDP 40: [[0.4989493]], actual sparsity level: 0.94, Squared error: 0.19452573359012604\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted sparsity level for MDP 41: [[0.35547867]], actual sparsity level: 0.02, Squared error: 0.1125459372997284\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted sparsity level for MDP 42: [[0.435656]], actual sparsity level: 0.4, Squared error: 0.0012713506584987044\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 43: [[0.8016166]], actual sparsity level: 0.86, Squared error: 0.003408621996641159\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 44: [[0.7922102]], actual sparsity level: 0.96, Squared error: 0.028153402730822563\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 45: [[0.3319947]], actual sparsity level: 0.21, Squared error: 0.014882711693644524\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 46: [[0.49199763]], actual sparsity level: 0.79, Squared error: 0.0888054221868515\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 47: [[1.1071498]], actual sparsity level: 0.75, Squared error: 0.12755601108074188\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 48: [[0.31151477]], actual sparsity level: 0.25, Squared error: 0.0037840662989765406\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted sparsity level for MDP 49: [[1.00523]], actual sparsity level: 0.85, Squared error: 0.024096330627799034\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 50: [[0.6823542]], actual sparsity level: 0.61, Squared error: 0.005235129967331886\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 51: [[0.5129503]], actual sparsity level: 0.87, Squared error: 0.1274844855070114\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 52: [[0.3942676]], actual sparsity level: 0.08, Squared error: 0.09876410663127899\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 53: [[0.2857306]], actual sparsity level: 0.15, Squared error: 0.018422793596982956\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted sparsity level for MDP 54: [[0.3227923]], actual sparsity level: 0.23, Squared error: 0.008610408753156662\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 55: [[0.5329044]], actual sparsity level: 0.22, Squared error: 0.09790915250778198\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 56: [[0.42109936]], actual sparsity level: 0.19, Squared error: 0.05340691655874252\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted sparsity level for MDP 57: [[0.44922912]], actual sparsity level: 0.01, Squared error: 0.19292223453521729\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Predicted sparsity level for MDP 58: [[0.7190677]], actual sparsity level: 0.52, Squared error: 0.03962795436382294\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted sparsity level for MDP 59: [[0.45157635]], actual sparsity level: 0.29, Squared error: 0.02610692009329796\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Predicted sparsity level for MDP 60: [[0.26693326]], actual sparsity level: 0.51, Squared error: 0.059081435203552246\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 61: [[0.63141084]], actual sparsity level: 0.71, Squared error: 0.006176253315061331\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 62: [[0.11104664]], actual sparsity level: 0.37, Squared error: 0.06705684214830399\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Predicted sparsity level for MDP 63: [[0.21045846]], actual sparsity level: 0.27, Squared error: 0.003545196494087577\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted sparsity level for MDP 64: [[0.56833476]], actual sparsity level: 0.7, Squared error: 0.017335733398795128\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted sparsity level for MDP 65: [[0.45403582]], actual sparsity level: 0.1, Squared error: 0.12534137070178986\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 66: [[0.33279097]], actual sparsity level: 0.16, Squared error: 0.029856720939278603\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 67: [[0.20198753]], actual sparsity level: 0.49, Squared error: 0.08295118808746338\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 68: [[0.13781005]], actual sparsity level: 0.44, Squared error: 0.0913187637925148\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 69: [[0.40911058]], actual sparsity level: 0.04, Squared error: 0.13624262809753418\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 70: [[0.45222405]], actual sparsity level: 0.93, Squared error: 0.22826987504959106\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 71: [[0.8483871]], actual sparsity level: 0.89, Squared error: 0.00173163041472435\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 72: [[0.05744696]], actual sparsity level: 0.11, Squared error: 0.0027618214953690767\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 73: [[0.43541276]], actual sparsity level: 0.33, Squared error: 0.011111848056316376\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 74: [[0.49784726]], actual sparsity level: 0.41, Squared error: 0.007717141415923834\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted sparsity level for MDP 75: [[0.57488024]], actual sparsity level: 0.59, Squared error: 0.00022860628087073565\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted sparsity level for MDP 76: [[0.596293]], actual sparsity level: 0.67, Squared error: 0.005432728212326765\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted sparsity level for MDP 77: [[0.6796007]], actual sparsity level: 0.57, Squared error: 0.012012318708002567\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 78: [[0.40556622]], actual sparsity level: 0.03, Squared error: 0.14104998111724854\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 79: [[0.4251373]], actual sparsity level: 0.53, Squared error: 0.010996177792549133\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted sparsity level for MDP 80: [[0.34818196]], actual sparsity level: 0.09, Squared error: 0.06665792316198349\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 81: [[0.37568423]], actual sparsity level: 0.28, Squared error: 0.009155471809208393\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 82: [[0.20196223]], actual sparsity level: 0.14, Squared error: 0.003839318174868822\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 83: [[0.22922613]], actual sparsity level: 0.2, Squared error: 0.0008541663410142064\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 84: [[0.4038557]], actual sparsity level: 0.07, Squared error: 0.1114596426486969\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted sparsity level for MDP 85: [[0.44700238]], actual sparsity level: 0.77, Squared error: 0.10432744771242142\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 86: [[0.49424988]], actual sparsity level: 0.82, Squared error: 0.10611313581466675\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 87: [[0.5513872]], actual sparsity level: 0.62, Squared error: 0.004707718268036842\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted sparsity level for MDP 88: [[0.6110163]], actual sparsity level: 0.26, Squared error: 0.12321243435144424\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 89: [[0.4195748]], actual sparsity level: 0.78, Squared error: 0.1299063116312027\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 90: [[0.35369283]], actual sparsity level: 0.76, Squared error: 0.16508550941944122\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted sparsity level for MDP 91: [[0.01850478]], actual sparsity level: 0.54, Squared error: 0.27195727825164795\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 92: [[0.6779849]], actual sparsity level: 0.84, Squared error: 0.026248887181282043\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted sparsity level for MDP 93: [[0.0978955]], actual sparsity level: 0.13, Squared error: 0.0010306988842785358\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 94: [[0.7573046]], actual sparsity level: 0.95, Squared error: 0.03713151067495346\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted sparsity level for MDP 95: [[0.7573046]], actual sparsity level: 0.99, Squared error: 0.0541471503674984\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 96: [[0.6768902]], actual sparsity level: 0.68, Squared error: 9.670935469330288e-06\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 97: [[0.56641686]], actual sparsity level: 0.73, Squared error: 0.026759449392557144\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 98: [[0.7573046]], actual sparsity level: 0.98, Squared error: 0.0495932474732399\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 99: [[0.83981013]], actual sparsity level: 0.81, Squared error: 0.0008886439027264714\n",
      "Mean squared error: 0.057695954918399364\n",
      "Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/6\n"
     ]
    }
   ],
   "source": [
    "### Idea 2: neural network\n",
    "sparsity, MDPs = generate_tests()\n",
    "# print(np.array(MDPs[0].P).shape)\n",
    "training_data = [(np.array(mdp.P), mdp.discount, mdp.policy, sparsity[i]) for i, mdp in enumerate(MDPs)]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Step 1: Feature extraction function\n",
    "def extract_features(transition_function, discount_rate, optimal_policy):\n",
    "    \"\"\"\n",
    "    Extract features from the MDP's transition function, discount rate, and optimal policy\n",
    "    \"\"\"\n",
    "    # num_states = transition_function.shape[0]\n",
    "    # num_actions = transition_function.shape[1]\n",
    "    # avg_transition_prob = np.mean(transition_function)\n",
    "    # var_reachable_states = np.var(np.sum(transition_function > 0, axis=2))\n",
    "    # # Add more features as needed\n",
    "    # features = np.array([num_states, num_actions, avg_transition_prob, var_reachable_states])\n",
    "    \n",
    "    features = np.array(optimal_policy)\n",
    "    return features\n",
    "\n",
    "# Step 2: Data preparation (assuming you have your data in an appropriate format)\n",
    "# This is a placeholder function - you would replace it with actual data loading and processing\n",
    "def prepare_data(training_data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for transition_function, discount_rate, optimal_policy, sparsity_level in training_data:\n",
    "        features.append(extract_features(transition_function, discount_rate, optimal_policy))\n",
    "        labels.append(sparsity_level)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Step 3: Model selection\n",
    "\n",
    "def build_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='linear')  # Linear activation for regression output\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='mean_squared_error',  # Suitable for regression\n",
    "                  metrics=['mae'])  # Mean Absolute Error as an additional metric\n",
    "    return model\n",
    "\n",
    "# Assuming you have already defined the feature extraction and data preparation functions\n",
    "# and have your data ready in 'features' and 'labels':\n",
    "features, labels = prepare_data(training_data)\n",
    "# Example: features shape is (num_samples, num_features), adjust 'input_dim' accordingly\n",
    "input_dim = features.shape[1]  # Assuming 'features' is already defined and preprocessed\n",
    "\n",
    "model = build_model(input_dim)\n",
    "\n",
    "# Training the model\n",
    "model.fit(features, labels, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Don't forget to preprocess your new data before making predictions\n",
    "# predicted_sparsity = model.predict(new_features)\n",
    "\n",
    "# Step 4: Training the model (placeholder for training data)\n",
    "# training_data = load_your_data_somehow()\n",
    "# features, labels = prepare_data(training_data)\n",
    "# model.fit(features, labels)\n",
    "\n",
    "# Step 5: Prediction function\n",
    "def predict_sparsity(transition_function, discount_rate, optimal_policy):\n",
    "    features = extract_features(transition_function, discount_rate, optimal_policy).reshape(1, -1)\n",
    "    predicted_sparsity = model.predict(features)\n",
    "    return predicted_sparsity\n",
    "\n",
    "# Note: The actual training step and data preparation would depend on your specific dataset and environment setup.\n",
    "test_sparsity, test_MDPs = generate_tests()\n",
    "test_data = [(np.array(mdp.P), mdp.discount, mdp.policy) for mdp in (test_MDPs)]\n",
    "NUM_TESTS = 100\n",
    "mse = np.zeros(NUM_TESTS)\n",
    "\n",
    "for i in range(min(NUM_TESTS, len(test_data))):\n",
    "    transition_function, discount_rate, optimal_policy = test_data[i]\n",
    "    prediction = predict_sparsity(transition_function, discount_rate, optimal_policy)\n",
    "    mse[i] = (prediction - test_sparsity[i])**2\n",
    "    print(f\"Predicted sparsity level for MDP {i}: {prediction}, actual sparsity level: {test_sparsity[i]}, Squared error: {mse[i]}\")\n",
    "\n",
    "print(f\"Mean squared error: {np.mean(mse)}\")\n",
    "print(\"Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/6\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
