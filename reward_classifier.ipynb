{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox as mdpt, numpy as np\n",
    "import mdptoolbox.example\n",
    "import MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a bunch of MDPs with different parameters, sparsity\n",
    "\n",
    "NUM_MDPs = 100\n",
    "NUM_STATES = 10\n",
    "NUM_ACTIONS = 4\n",
    "\n",
    "def get_transition_matrix(num_states, num_actions, generator = np.random.dirichlet):\n",
    "    P = np.zeros((num_actions, num_states, num_states)) # (A, S, S) shape\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            P[a, s, :] = generator(np.ones(num_states))\n",
    "    return P\n",
    "\n",
    "def get_reward_matrix(num_states, num_actions, sparsity = 0.0, generator = np.random.normal):\n",
    "    R = np.zeros((num_states, num_actions))\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            if np.random.rand() < sparsity:\n",
    "                R[s, a] = 0\n",
    "            else:\n",
    "                R[s, a] = generator()\n",
    "    return R\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "EPSILON = 0.01\n",
    "MAX_ITER = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests(num_mdps = NUM_MDPs, sparsity_levels = np.arange(NUM_MDPs) / NUM_MDPs, mdp_generator = mdpt.mdp.PolicyIteration):\n",
    "    \"\"\"\n",
    "    Generate a bunch of MDPs with different sparsity levels, and return the sparsity levels and the MDPs\n",
    "\n",
    "    Args:\n",
    "        sparsity_levels: a list of sparsity levels to generate MDPs with\n",
    "    Returns:\n",
    "        sparsity_levels: the sparsity levels used to generate the MDPs, in the same order as the MDPs\n",
    "        MDPS: an array of MDPs\n",
    "    \"\"\"\n",
    "    sparsity_copy = sparsity_levels.copy() # defensive copy\n",
    "    np.random.shuffle(sparsity_copy)\n",
    "    MDPS = np.array([mdp_generator(\n",
    "        get_transition_matrix(NUM_STATES, NUM_ACTIONS), \n",
    "        get_reward_matrix(NUM_STATES, NUM_ACTIONS, sparsity_copy[i]), \n",
    "        DISCOUNT, max_iter = MAX_ITER) \n",
    "        for i in range(num_mdps)\n",
    "    ])\n",
    "    return sparsity_copy, MDPS\n",
    "\n",
    "sparsity_levels, MDPS = generate_tests()\n",
    "for mdp in MDPS:\n",
    "    mdp.run()\n",
    "    # print(mdp.policy) # debug\n",
    "# print(MDPS[0].policy) # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a classifier to predict sparsity level from a policy\n",
    "### Idea 1: hack-y heuristics\n",
    "\n",
    "def heuristic_classifier(MDP, policy):\n",
    "    \"\"\"\n",
    "    A heuristic classifier that predicts the sparsity level of an MDP's reward function given its \n",
    "    optimal policy\n",
    "    1. \n",
    "    \"\"\"\n",
    "    # TODO: implement this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 115ms/step - loss: 0.2580 - mae: 0.4333 - val_loss: 0.1199 - val_mae: 0.2893\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1351 - mae: 0.3011 - val_loss: 0.0848 - val_mae: 0.2496\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.1171 - mae: 0.2700 - val_loss: 0.0643 - val_mae: 0.2190\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1027 - mae: 0.2697 - val_loss: 0.0576 - val_mae: 0.2129\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0736 - mae: 0.2219 - val_loss: 0.0576 - val_mae: 0.2140\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0688 - mae: 0.2195 - val_loss: 0.0422 - val_mae: 0.1843\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0626 - mae: 0.2124 - val_loss: 0.0265 - val_mae: 0.1356\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0581 - mae: 0.1930 - val_loss: 0.0267 - val_mae: 0.1296\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0583 - mae: 0.1932 - val_loss: 0.0278 - val_mae: 0.1355\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0444 - mae: 0.1683 - val_loss: 0.0380 - val_mae: 0.1688\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0536 - mae: 0.1930 - val_loss: 0.0324 - val_mae: 0.1525\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0360 - mae: 0.1510 - val_loss: 0.0304 - val_mae: 0.1307\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0407 - mae: 0.1733 - val_loss: 0.0314 - val_mae: 0.1339\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0434 - mae: 0.1677 - val_loss: 0.0323 - val_mae: 0.1462\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0450 - mae: 0.1743 - val_loss: 0.0441 - val_mae: 0.1861\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.0389 - mae: 0.1592 - val_loss: 0.0450 - val_mae: 0.1879\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0428 - mae: 0.1669 - val_loss: 0.0363 - val_mae: 0.1590\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0387 - mae: 0.1616 - val_loss: 0.0338 - val_mae: 0.1439\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 35ms/step - loss: 0.0395 - mae: 0.1515 - val_loss: 0.0393 - val_mae: 0.1724\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.0244 - mae: 0.1251 - val_loss: 0.0456 - val_mae: 0.1892\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0333 - mae: 0.1374 - val_loss: 0.0418 - val_mae: 0.1794\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0347 - mae: 0.1444 - val_loss: 0.0345 - val_mae: 0.1514\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0312 - mae: 0.1460 - val_loss: 0.0339 - val_mae: 0.1478\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0284 - mae: 0.1404 - val_loss: 0.0355 - val_mae: 0.1566\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0196 - mae: 0.1114 - val_loss: 0.0394 - val_mae: 0.1710\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.0200 - mae: 0.1140 - val_loss: 0.0435 - val_mae: 0.1823\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0242 - mae: 0.1159 - val_loss: 0.0408 - val_mae: 0.1744\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0248 - mae: 0.1320 - val_loss: 0.0361 - val_mae: 0.1532\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0172 - mae: 0.1055 - val_loss: 0.0340 - val_mae: 0.1385\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.0189 - mae: 0.1095 - val_loss: 0.0322 - val_mae: 0.1384\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0178 - mae: 0.1032 - val_loss: 0.0338 - val_mae: 0.1540\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.0238 - mae: 0.1217 - val_loss: 0.0388 - val_mae: 0.1701\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0240 - mae: 0.1241 - val_loss: 0.0353 - val_mae: 0.1590\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0190 - mae: 0.1084 - val_loss: 0.0320 - val_mae: 0.1454\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0295 - mae: 0.1333 - val_loss: 0.0296 - val_mae: 0.1270\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0290 - mae: 0.1200 - val_loss: 0.0332 - val_mae: 0.1528\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0228 - mae: 0.1160 - val_loss: 0.0460 - val_mae: 0.1941\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0182 - mae: 0.1080 - val_loss: 0.0527 - val_mae: 0.2085\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0267 - mae: 0.1278 - val_loss: 0.0465 - val_mae: 0.1940\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0148 - mae: 0.0982 - val_loss: 0.0407 - val_mae: 0.1693\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0193 - mae: 0.1052 - val_loss: 0.0420 - val_mae: 0.1737\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0198 - mae: 0.1137 - val_loss: 0.0449 - val_mae: 0.1816\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0243 - mae: 0.1178 - val_loss: 0.0468 - val_mae: 0.1878\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0179 - mae: 0.1054 - val_loss: 0.0492 - val_mae: 0.1939\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0200 - mae: 0.1136 - val_loss: 0.0453 - val_mae: 0.1832\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0182 - mae: 0.1037 - val_loss: 0.0385 - val_mae: 0.1593\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0190 - mae: 0.1036 - val_loss: 0.0348 - val_mae: 0.1452\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0163 - mae: 0.0982 - val_loss: 0.0367 - val_mae: 0.1602\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0170 - mae: 0.1007 - val_loss: 0.0371 - val_mae: 0.1625\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0169 - mae: 0.0993 - val_loss: 0.0339 - val_mae: 0.1476\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0154 - mae: 0.0920 - val_loss: 0.0325 - val_mae: 0.1351\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0189 - mae: 0.0998 - val_loss: 0.0336 - val_mae: 0.1424\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0147 - mae: 0.0948 - val_loss: 0.0405 - val_mae: 0.1689\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0159 - mae: 0.0897 - val_loss: 0.0383 - val_mae: 0.1604\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0150 - mae: 0.0928 - val_loss: 0.0355 - val_mae: 0.1508\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0180 - mae: 0.0944 - val_loss: 0.0342 - val_mae: 0.1491\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0111 - mae: 0.0859 - val_loss: 0.0340 - val_mae: 0.1512\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0133 - mae: 0.0959 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0157 - mae: 0.0899 - val_loss: 0.0321 - val_mae: 0.1483\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0125 - mae: 0.0880 - val_loss: 0.0338 - val_mae: 0.1563\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0148 - mae: 0.0952 - val_loss: 0.0365 - val_mae: 0.1648\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0148 - mae: 0.0903 - val_loss: 0.0377 - val_mae: 0.1681\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0108 - mae: 0.0798 - val_loss: 0.0382 - val_mae: 0.1686\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0192 - mae: 0.1016 - val_loss: 0.0369 - val_mae: 0.1615\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0111 - mae: 0.0825 - val_loss: 0.0377 - val_mae: 0.1618\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0131 - mae: 0.0828 - val_loss: 0.0394 - val_mae: 0.1672\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0096 - mae: 0.0744 - val_loss: 0.0399 - val_mae: 0.1687\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0118 - mae: 0.0849 - val_loss: 0.0412 - val_mae: 0.1756\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0115 - mae: 0.0819 - val_loss: 0.0410 - val_mae: 0.1754\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0095 - mae: 0.0748 - val_loss: 0.0382 - val_mae: 0.1657\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0129 - mae: 0.0895 - val_loss: 0.0344 - val_mae: 0.1477\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0146 - mae: 0.0871 - val_loss: 0.0352 - val_mae: 0.1532\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0156 - mae: 0.0967 - val_loss: 0.0395 - val_mae: 0.1698\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0106 - mae: 0.0727 - val_loss: 0.0415 - val_mae: 0.1750\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0113 - mae: 0.0815 - val_loss: 0.0398 - val_mae: 0.1682\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0115 - mae: 0.0850 - val_loss: 0.0382 - val_mae: 0.1612\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0121 - mae: 0.0815 - val_loss: 0.0393 - val_mae: 0.1641\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0118 - mae: 0.0875 - val_loss: 0.0420 - val_mae: 0.1737\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0145 - mae: 0.0877 - val_loss: 0.0394 - val_mae: 0.1671\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0096 - mae: 0.0768 - val_loss: 0.0348 - val_mae: 0.1512\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.0100 - mae: 0.0737 - val_loss: 0.0314 - val_mae: 0.1370\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0104 - mae: 0.0750 - val_loss: 0.0314 - val_mae: 0.1438\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0086 - mae: 0.0727 - val_loss: 0.0331 - val_mae: 0.1532\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0150 - mae: 0.0902 - val_loss: 0.0324 - val_mae: 0.1484\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0119 - mae: 0.0736 - val_loss: 0.0331 - val_mae: 0.1483\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0096 - mae: 0.0704 - val_loss: 0.0361 - val_mae: 0.1590\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0082 - mae: 0.0745 - val_loss: 0.0391 - val_mae: 0.1676\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0107 - mae: 0.0808 - val_loss: 0.0368 - val_mae: 0.1588\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0121 - mae: 0.0811 - val_loss: 0.0334 - val_mae: 0.1347\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0089 - mae: 0.0755 - val_loss: 0.0328 - val_mae: 0.1274\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0169 - mae: 0.0948 - val_loss: 0.0346 - val_mae: 0.1484\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0109 - mae: 0.0816 - val_loss: 0.0395 - val_mae: 0.1674\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0064 - mae: 0.0615 - val_loss: 0.0335 - val_mae: 0.1485\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0136 - mae: 0.0834 - val_loss: 0.0303 - val_mae: 0.1332\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0100 - mae: 0.0739 - val_loss: 0.0311 - val_mae: 0.1374\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0098 - mae: 0.0727 - val_loss: 0.0331 - val_mae: 0.1476\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0106 - mae: 0.0730 - val_loss: 0.0352 - val_mae: 0.1561\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0109 - mae: 0.0786 - val_loss: 0.0341 - val_mae: 0.1521\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0094 - mae: 0.0737 - val_loss: 0.0319 - val_mae: 0.1409\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0074 - mae: 0.0663 - val_loss: 0.0319 - val_mae: 0.1394\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "Predicted sparsity level for MDP 0: [[0.37831897]], actual sparsity level: 0.13, Squared error: 0.06166231259703636\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 1: [[0.8234666]], actual sparsity level: 0.86, Squared error: 0.0013346904888749123\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted sparsity level for MDP 2: [[0.23307386]], actual sparsity level: 0.29, Squared error: 0.003240584395825863\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted sparsity level for MDP 3: [[0.58505076]], actual sparsity level: 0.92, Squared error: 0.11219100654125214\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 4: [[0.3277418]], actual sparsity level: 0.38, Squared error: 0.0027309188153594732\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted sparsity level for MDP 5: [[0.80678093]], actual sparsity level: 0.89, Squared error: 0.006925410591065884\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 6: [[0.47116944]], actual sparsity level: 0.41, Squared error: 0.0037417011335492134\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted sparsity level for MDP 7: [[0.3356602]], actual sparsity level: 0.36, Squared error: 0.0005924270953983068\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted sparsity level for MDP 8: [[0.41798148]], actual sparsity level: 0.31, Squared error: 0.011659998446702957\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 9: [[0.35572156]], actual sparsity level: 0.01, Squared error: 0.11952340602874756\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 10: [[0.35314626]], actual sparsity level: 0.06, Squared error: 0.08593472838401794\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted sparsity level for MDP 11: [[0.12250808]], actual sparsity level: 0.0, Squared error: 0.01500822976231575\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 12: [[0.41407204]], actual sparsity level: 0.68, Squared error: 0.07071768492460251\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 13: [[0.2821452]], actual sparsity level: 0.33, Squared error: 0.002290082862600684\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted sparsity level for MDP 14: [[0.15204161]], actual sparsity level: 0.5, Squared error: 0.12107504159212112\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 15: [[0.36616075]], actual sparsity level: 0.04, Squared error: 0.10638084262609482\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 16: [[0.8682133]], actual sparsity level: 0.97, Squared error: 0.010360538959503174\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 17: [[0.4186725]], actual sparsity level: 0.03, Squared error: 0.15106631815433502\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 18: [[0.34993917]], actual sparsity level: 0.74, Squared error: 0.15214745700359344\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 19: [[0.60828173]], actual sparsity level: 0.67, Squared error: 0.0038091468159109354\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 20: [[0.4652932]], actual sparsity level: 0.78, Squared error: 0.09904035180807114\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 21: [[0.58064485]], actual sparsity level: 0.79, Squared error: 0.04382959008216858\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 22: [[0.71778953]], actual sparsity level: 0.96, Squared error: 0.05866590142250061\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 23: [[0.57053477]], actual sparsity level: 0.76, Squared error: 0.03589707240462303\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 24: [[0.26979902]], actual sparsity level: 0.37, Squared error: 0.01004023663699627\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 25: [[0.45225418]], actual sparsity level: 0.71, Squared error: 0.0664329007267952\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 26: [[0.2599937]], actual sparsity level: 0.42, Squared error: 0.025602011010050774\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 27: [[0.39227915]], actual sparsity level: 0.39, Squared error: 5.194581262912834e-06\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Predicted sparsity level for MDP 28: [[0.22637013]], actual sparsity level: 0.08, Squared error: 0.02142421342432499\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted sparsity level for MDP 29: [[0.5513193]], actual sparsity level: 0.45, Squared error: 0.01026560366153717\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 30: [[0.40161633]], actual sparsity level: 0.54, Squared error: 0.01915004476904869\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 31: [[0.31025895]], actual sparsity level: 0.14, Squared error: 0.02898811176419258\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 32: [[0.5424925]], actual sparsity level: 0.73, Squared error: 0.03515906631946564\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 33: [[0.83755326]], actual sparsity level: 0.99, Squared error: 0.02324001118540764\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted sparsity level for MDP 34: [[0.6002565]], actual sparsity level: 0.75, Squared error: 0.022423114627599716\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 35: [[0.38547572]], actual sparsity level: 0.17, Squared error: 0.04642978683114052\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted sparsity level for MDP 36: [[0.27389917]], actual sparsity level: 0.65, Squared error: 0.14145182073116302\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted sparsity level for MDP 37: [[0.7038823]], actual sparsity level: 0.53, Squared error: 0.030235055834054947\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 38: [[0.2803361]], actual sparsity level: 0.26, Squared error: 0.0004135578346904367\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Predicted sparsity level for MDP 39: [[0.6848274]], actual sparsity level: 0.85, Squared error: 0.027281999588012695\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 40: [[0.28326038]], actual sparsity level: 0.4, Squared error: 0.013628141023218632\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted sparsity level for MDP 41: [[0.592019]], actual sparsity level: 0.7, Squared error: 0.011659889481961727\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 42: [[0.80358326]], actual sparsity level: 0.8, Squared error: 1.2839697774325032e-05\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 43: [[0.5154131]], actual sparsity level: 0.21, Squared error: 0.09327717870473862\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted sparsity level for MDP 44: [[0.42848733]], actual sparsity level: 0.22, Squared error: 0.043466966599226\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 45: [[0.6989537]], actual sparsity level: 0.87, Squared error: 0.02925684303045273\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 46: [[0.7589321]], actual sparsity level: 0.83, Squared error: 0.00505064195021987\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 47: [[0.2653461]], actual sparsity level: 0.05, Squared error: 0.04637394845485687\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Predicted sparsity level for MDP 48: [[0.39101776]], actual sparsity level: 0.59, Squared error: 0.03959392011165619\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Predicted sparsity level for MDP 49: [[0.4959241]], actual sparsity level: 0.3, Squared error: 0.03838624432682991\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted sparsity level for MDP 50: [[0.4761637]], actual sparsity level: 0.43, Squared error: 0.002131085144355893\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 51: [[0.43580848]], actual sparsity level: 0.88, Squared error: 0.1973060965538025\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 52: [[0.5113771]], actual sparsity level: 0.12, Squared error: 0.15317602455615997\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted sparsity level for MDP 53: [[0.48153064]], actual sparsity level: 0.44, Squared error: 0.0017247939249500632\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 54: [[0.33270827]], actual sparsity level: 0.47, Squared error: 0.018849018961191177\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 55: [[0.32742622]], actual sparsity level: 0.63, Squared error: 0.09155088663101196\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted sparsity level for MDP 56: [[0.5559112]], actual sparsity level: 0.69, Squared error: 0.017979810014367104\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 57: [[0.5116772]], actual sparsity level: 0.49, Squared error: 0.0004699008131865412\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 58: [[0.40054047]], actual sparsity level: 0.51, Squared error: 0.011981386691331863\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 59: [[0.7992471]], actual sparsity level: 0.84, Squared error: 0.001660797861404717\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 60: [[0.8385842]], actual sparsity level: 0.98, Squared error: 0.01999843865633011\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 61: [[0.46219027]], actual sparsity level: 0.64, Squared error: 0.031616296619176865\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 62: [[0.33285668]], actual sparsity level: 0.52, Squared error: 0.035022612661123276\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 63: [[0.36511263]], actual sparsity level: 0.48, Squared error: 0.01319910492748022\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 64: [[0.27056882]], actual sparsity level: 0.34, Squared error: 0.004820689558982849\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 65: [[0.5197494]], actual sparsity level: 0.61, Squared error: 0.008145173080265522\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted sparsity level for MDP 66: [[0.36871317]], actual sparsity level: 0.24, Squared error: 0.016567081212997437\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted sparsity level for MDP 67: [[0.28762463]], actual sparsity level: 0.18, Squared error: 0.011583059094846249\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted sparsity level for MDP 68: [[0.4395743]], actual sparsity level: 0.09, Squared error: 0.12220218777656555\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Predicted sparsity level for MDP 69: [[0.47891533]], actual sparsity level: 0.32, Squared error: 0.025254085659980774\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted sparsity level for MDP 70: [[0.46025103]], actual sparsity level: 0.82, Squared error: 0.12941931188106537\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted sparsity level for MDP 71: [[0.29794204]], actual sparsity level: 0.1, Squared error: 0.03918105363845825\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 72: [[0.6714535]], actual sparsity level: 0.94, Squared error: 0.07211723178625107\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted sparsity level for MDP 73: [[0.38806164]], actual sparsity level: 0.57, Squared error: 0.03310156241059303\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Predicted sparsity level for MDP 74: [[0.53671795]], actual sparsity level: 0.46, Squared error: 0.00588564295321703\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted sparsity level for MDP 75: [[0.28960848]], actual sparsity level: 0.16, Squared error: 0.016798358410596848\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted sparsity level for MDP 76: [[0.36657536]], actual sparsity level: 0.56, Squared error: 0.037413090467453\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Predicted sparsity level for MDP 77: [[0.32700956]], actual sparsity level: 0.62, Squared error: 0.08584339916706085\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted sparsity level for MDP 78: [[0.3668355]], actual sparsity level: 0.02, Squared error: 0.12029486149549484\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Predicted sparsity level for MDP 79: [[0.41550824]], actual sparsity level: 0.25, Squared error: 0.027392977848649025\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Predicted sparsity level for MDP 80: [[0.11467397]], actual sparsity level: 0.19, Squared error: 0.005674010142683983\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted sparsity level for MDP 81: [[0.7511168]], actual sparsity level: 0.95, Squared error: 0.03955451771616936\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Predicted sparsity level for MDP 82: [[0.46675736]], actual sparsity level: 0.58, Squared error: 0.012823892757296562\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 83: [[0.28538892]], actual sparsity level: 0.11, Squared error: 0.030761271715164185\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted sparsity level for MDP 84: [[0.23101845]], actual sparsity level: 0.15, Squared error: 0.006563988979905844\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted sparsity level for MDP 85: [[0.601637]], actual sparsity level: 0.81, Squared error: 0.0434151366353035\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Predicted sparsity level for MDP 86: [[0.5050267]], actual sparsity level: 0.66, Squared error: 0.0240167323499918\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted sparsity level for MDP 87: [[0.4373248]], actual sparsity level: 0.23, Squared error: 0.04298356920480728\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Predicted sparsity level for MDP 88: [[0.2907764]], actual sparsity level: 0.27, Squared error: 0.000431658438174054\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Predicted sparsity level for MDP 89: [[0.45267]], actual sparsity level: 0.35, Squared error: 0.010541131719946861\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted sparsity level for MDP 90: [[0.5942244]], actual sparsity level: 0.9, Squared error: 0.09349870681762695\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 91: [[0.455347]], actual sparsity level: 0.6, Squared error: 0.02092449739575386\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 92: [[0.6523802]], actual sparsity level: 0.77, Squared error: 0.01383440662175417\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted sparsity level for MDP 93: [[0.65141046]], actual sparsity level: 0.72, Squared error: 0.004704528953880072\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted sparsity level for MDP 94: [[0.11717677]], actual sparsity level: 0.07, Squared error: 0.0022256476804614067\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 95: [[0.41998717]], actual sparsity level: 0.91, Squared error: 0.24011260271072388\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 96: [[0.41197786]], actual sparsity level: 0.55, Squared error: 0.0190501157194376\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 97: [[0.36195767]], actual sparsity level: 0.28, Squared error: 0.0067170592956244946\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted sparsity level for MDP 98: [[0.28915247]], actual sparsity level: 0.2, Squared error: 0.007948162965476513\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted sparsity level for MDP 99: [[0.7356441]], actual sparsity level: 0.93, Squared error: 0.0377742163836956\n",
      "Mean squared error: 0.04125316660505177\n",
      "Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\n"
     ]
    }
   ],
   "source": [
    "### Idea 2: neural network\n",
    "# Thanks again ChatGPT for outlining the code structure\n",
    "\n",
    "sparsity, MDPs = generate_tests()\n",
    "# print(np.array(MDPs[0].P).shape)\n",
    "training_data = [(np.array(mdp.P), mdp.discount, mdp.policy, sparsity[i]) for i, mdp in enumerate(MDPs)]\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Step 1: Feature extraction function\n",
    "def extract_features(transition_function, discount_rate, optimal_policy):\n",
    "    \"\"\"\n",
    "    Extract features from the MDP's transition function, discount rate, and optimal policy\n",
    "    \"\"\"\n",
    "    # num_states = transition_function.shape[0]\n",
    "    # num_actions = transition_function.shape[1]\n",
    "    # avg_transition_prob = np.mean(transition_function)\n",
    "    # var_reachable_states = np.var(np.sum(transition_function > 0, axis=2))\n",
    "    # # Add more features as needed\n",
    "    # features = np.array([num_states, num_actions, avg_transition_prob, var_reachable_states])\n",
    "    \n",
    "    \n",
    "    # opt_policy = optimal_policy.reshape(-1, 1)  # Reshape for sklearn which expects 2D input\n",
    "\n",
    "    # # Initialize the OneHotEncoder\n",
    "    # encoder = OneHotEncoder(sparse=False)  # Use sparse=False to get a dense array\n",
    "\n",
    "    # # Fit and transform\n",
    "    # opt_policy_one_hot = encoder.fit_transform(opt_policy)\n",
    "    features = np.concatenate((transition_function.flatten(), [discount_rate], optimal_policy.flatten()))\n",
    "    # print(features.shape)\n",
    "    # length 10*10*4 + 1 + 10 = 411\n",
    "    return features\n",
    "\n",
    "# Step 2: Data preparation (assuming you have your data in an appropriate format)\n",
    "# This is a placeholder function - you would replace it with actual data loading and processing\n",
    "def prepare_data(training_data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for transition_function, discount_rate, optimal_policy, sparsity_level in training_data:\n",
    "        features.append(extract_features(transition_function, discount_rate, optimal_policy))\n",
    "        labels.append(sparsity_level)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Step 3: Model selection\n",
    "\n",
    "def build_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.2),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='linear')  # Linear activation for regression output\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='mean_squared_error',  # Suitable for regression\n",
    "                  metrics=['mae'])  # Mean Absolute Error as an additional metric\n",
    "    return model\n",
    "\n",
    "# Assuming you have already defined the feature extraction and data preparation functions\n",
    "# and have your data ready in 'features' and 'labels':\n",
    "features, labels = prepare_data(training_data)\n",
    "# Example: features shape is (num_samples, num_features), adjust 'input_dim' accordingly\n",
    "input_dim = features.shape[1]  # Assuming 'features' is already defined and preprocessed\n",
    "\n",
    "model = build_model(input_dim)\n",
    "\n",
    "# Training the model\n",
    "model.fit(features, labels, epochs=100, batch_size=32, validation_split=0.2, verbose = 1)\n",
    "\n",
    "# Don't forget to preprocess your new data before making predictions\n",
    "# predicted_sparsity = model.predict(new_features)\n",
    "\n",
    "# Step 4: Training the model (placeholder for training data)\n",
    "# training_data = load_your_data_somehow()\n",
    "# features, labels = prepare_data(training_data)\n",
    "# model.fit(features, labels)\n",
    "\n",
    "# Step 5: Prediction function\n",
    "def predict_sparsity(transition_function, discount_rate, optimal_policy):\n",
    "    features = extract_features(transition_function, discount_rate, optimal_policy).reshape(1, -1)\n",
    "    predicted_sparsity = model.predict(features)\n",
    "    return predicted_sparsity\n",
    "\n",
    "# Note: The actual training step and data preparation would depend on your specific dataset and environment setup.\n",
    "test_sparsity, test_MDPs = generate_tests()\n",
    "test_data = [(np.array(mdp.P), mdp.discount, mdp.policy) for mdp in (test_MDPs)]\n",
    "NUM_TESTS = 100\n",
    "mse = np.zeros(NUM_TESTS)\n",
    "\n",
    "for i in range(min(NUM_TESTS, len(test_data))):\n",
    "    transition_function, discount_rate, optimal_policy = test_data[i]\n",
    "    prediction = predict_sparsity(transition_function, discount_rate, optimal_policy)\n",
    "    mse[i] = (prediction - test_sparsity[i])**2\n",
    "    print(f\"Predicted sparsity level for MDP {i}: {prediction}, actual sparsity level: {test_sparsity[i]}, Squared error: {mse[i]}\")\n",
    "\n",
    "print(f\"Mean squared error: {np.mean(mse)}\")\n",
    "print(\"Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
