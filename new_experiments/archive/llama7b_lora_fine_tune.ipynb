{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "from peft import get_peft_model\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW,    TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#import the bits and bites optimizer again\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "#import adamw\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment, these must be kept outside the main or else, the environment variables are not set\n",
    "os.environ['HF_HOME'] = '/workspace/.cache/huggingface'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/.cache/huggingface/models'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/.cache/huggingface/datasets'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:64'  \n",
    "\n",
    "# Log in to wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#including a function to then rip out the correct answers\n",
    "def extract_steps_and_final_answer(answer):\n",
    "    # Extract calculations and the final answer from the structured answer string\n",
    "    steps = re.findall(r\"<<(.*?)>>\", answer)\n",
    "    final_answer = answer.split('####')[-1].strip()\n",
    "    return steps, final_answer\n",
    "\n",
    "# Function to generate an answer for a single question\n",
    "def generate_answer(model, tokenizer, question, device):\n",
    "    # Tokenize the question\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=300)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Generate an answer using the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=300)\n",
    "    \n",
    "    # Decode the generated tokens to a string\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Question: {question} END Question\")\n",
    "    print(f\"Answer: {answer} END Answer\")\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(tokenizer, examples):\n",
    "    # Tokenize the question to create the model input\n",
    "    sentence_to_append = \"Please place all of your calculations within the <<Calculations here>>, for example<<48/2=24>>. Inlcude the finsl answer after ####, such as ####NumberAnswer\"\n",
    "    \n",
    "    #for each row within the examples['question] dataset to each row append sentence to append\n",
    "    examples['question'] = [x + sentence_to_append for x in examples['question']]\n",
    "\n",
    "    model_inputs = tokenizer(examples['question'], truncation=True, padding='max_length', max_length=64)\n",
    "    \n",
    "    # Tokenize the answer to create the labels\n",
    "    # The labels should be the input_ids from the tokenized answer\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['answer'], truncation=True, padding='max_length', max_length=64)\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(epochs,token, log_interval=10):\n",
    "    \n",
    "    #i guess I should just force this to the cached local\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=token, )\n",
    "    print('about to get model')\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=token, cache_dir='/workspace/.cache/huggingface/models/')\n",
    "    print(\"model gotten\")\n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    #bring in content for wandb\n",
    "    #if rank == 0:  # Initialize only once\n",
    "    wandb.init(project=\"coherence\", entity=\"jprivera44\", config={\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": 1,\n",
    "        \"learning_rate\": 5e-5,\n",
    "    })\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    ##############TRAIN###############\n",
    "    # Correct dataset configuration and preprocessing\n",
    "    data = load_dataset(\"gsm8k\", \"main\", split='train[:500]')\n",
    "    data = data.map(lambda e: preprocess_data(tokenizer, e), batched=True)\n",
    "    data.set_format(type='torch', columns=['input_ids', 'attention_mask','labels'])\n",
    "    ##############TRAIN###############\n",
    "    \n",
    "    ##############VALIDATION###############\n",
    "    data_v_string = load_dataset(\"gsm8k\", \"main\", split='test[:500]')\n",
    "    data_v = data_v_string.map(lambda e: preprocess_data(tokenizer, e), batched=True)\n",
    "    data_v.set_format(type='torch', columns=['input_ids', 'attention_mask','labels'])\n",
    "    ##############VALIDATION###############\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Training Params\n",
    "    train_params = TrainingArguments(\n",
    "        output_dir=\"./results_modified\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=9,\n",
    "        gradient_accumulation_steps=1,\n",
    "        #why this optimizer\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_steps=50,\n",
    "        logging_steps=10,\n",
    "        eval_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        learning_rate=20e-1,\n",
    "        weight_decay=0.001,\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        max_grad_norm=0.3,\n",
    "        max_steps=-1,\n",
    "        warmup_ratio=0.03,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"constant\",\n",
    "        report_to=\"wandb\"\n",
    "    )\n",
    "    \n",
    "    # LoRA Config\n",
    "    peft_parameters = LoraConfig(\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, peft_parameters)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    #set up the optimizer\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    def log_step_metrics(current_loss, epoch_num, step, total_steps):\n",
    "        print(f\"Epoch {epoch_num}, Step {step}: Current Average Loss: {current_loss}\")\n",
    "        wandb.log({\n",
    "            \"step_average_loss\": current_loss,\n",
    "            \"total_steps\": total_steps\n",
    "        })\n",
    "        \n",
    "    \n",
    "    \n",
    "    def dense_loss(tokenizer, model_output, labels):\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()  # Standard classification loss\n",
    "\n",
    "        #answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(\"model output, loss\",model_output[0])\n",
    "        print(\" \")\n",
    "        print(\"model output, logits\",model_output.logits.argmax(dim=-1)[0])\n",
    "        print(\" \")\n",
    "        print(\"labels\",labels[0])\n",
    "        print(\" \")\n",
    "        print(\" \")\n",
    "\n",
    "        # Decode the outputs and labels\n",
    "        decoded_outputs = tokenizer.decode(model_output.logits.argmax(dim=-1)[0], skip_special_tokens=True)\n",
    "        \n",
    "        #printing out the decoded outputs\n",
    "        print(\"decode\", decoded_outputs)\n",
    "        \n",
    "        #don't like how this might work as selecting the first answer.\n",
    "        decoded_labels = tokenizer.decode(labels[0], skip_special_tokens=True)\n",
    "        \n",
    "        \n",
    "        true_steps, true_final = extract_steps_and_final_answer(decoded_labels)\n",
    "        \n",
    "        #print eh true steps and the true final\n",
    "        print(\"True Steps\",true_steps)\n",
    "        print(\"True Final\",true_final)\n",
    "\n",
    "        # Extract steps and final answers\n",
    "        model_steps, model_final = extract_steps_and_final_answer(decoded_outputs)\n",
    "        \n",
    "        print(\"Model steps\", model_steps)\n",
    "        print(\"model final\", model_final)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print the model steps and the model final\n",
    "\n",
    "        # Calculate rewards for steps\n",
    "        step_rewards = sum(1 for model_step, true_step in zip(model_steps, true_steps) if model_step == true_step)\n",
    "        # Bonus for correct final answer\n",
    "        final_reward = 1 if model_final == true_final else 0\n",
    "\n",
    "        # Normalize the rewards (example normalization, adjust as needed)\n",
    "        total_rewards = step_rewards + final_reward\n",
    "        max_possible_rewards = len(true_steps) + 1  # Each step + final answer\n",
    "        normalized_reward = total_rewards / max_possible_rewards\n",
    "\n",
    "        # Convert reward to a loss (simple inversion for demonstration)\n",
    "        loss = 1 - normalized_reward  # Assuming the reward is scaled between 0 and 1\n",
    "\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Ok new version.\n",
    "    def custom_loss(model_output, labels):\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()  # Assuming a classification task\n",
    "        loss = loss_fct(model_output.logits.view(-1, model_output.logits.size(-1)), labels.view(-1))\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def evaluate(model, eval_loader, device):\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_loss = 0\n",
    "        total_steps = 0\n",
    "        \n",
    "        with torch.no_grad():  # Turn off gradients for evaluation\n",
    "            for batch in eval_loader:\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                masks = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = model(input_ids=inputs, attention_mask=masks, labels=labels)\n",
    "                    loss = custom_loss(outputs, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_steps += 1\n",
    "        \n",
    "        avg_loss = total_loss / total_steps\n",
    "        return avg_loss\n",
    "\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    def train_epoch(model, data_loader, optimizer, device, epoch_num, log_interval=10):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for batch_index, batch in enumerate(data_loader):\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            masks = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)  # Ensure labels are part of the batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():  # Mixed precision\n",
    "                outputs = model(input_ids=inputs, attention_mask=masks, labels=labels)\n",
    "                #loss = custom_loss(outputs, labels)\n",
    "                loss = dense_loss(tokenizer, outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()  # Accumulate the total loss for the epoch\n",
    "            steps +=1\n",
    "            \n",
    "            # Log the loss at the specified interval\n",
    "            if (batch_index + 1) % log_interval == 0:\n",
    "                current_loss = total_loss / steps\n",
    "                log_step_metrics(current_loss, epoch_num, batch_index + 1, epoch_num * len(data_loader) + batch_index)\n",
    "                \n",
    "                \n",
    "                validation_loss = evaluate(model, eval_loader, device)\n",
    "                print(f\"Validation Loss after Epoch {epoch_num}: {validation_loss}\")\n",
    "                wandb.log({\n",
    "                    \"validation_loss\": validation_loss,\n",
    "                    \"epoch\": epoch_num\n",
    "                })\n",
    "\n",
    "        \n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f\"Average Training Loss for Epoch {epoch_num}: {avg_loss}\")\n",
    "        wandb.log({\"average_train_loss\": avg_loss, \"epoch\": epoch_num})\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(data, batch_size=train_params.per_device_train_batch_size, shuffle=True)\n",
    "    eval_loader = DataLoader(data_v, batch_size=train_params.per_device_train_batch_size)\n",
    "    \n",
    "\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        \n",
    "        #call the training loop\n",
    "        train_epoch(model, train_loader,optimizer, device, epoch_num, log_interval=log_interval)\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    world_size = torch.cuda.device_count()\n",
    "    epoch_count = 3\n",
    "    token = \"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\"\n",
    "    log_interval = 10\n",
    "    \n",
    "    train(epochs=epoch_count,token=token)\n",
    "    \n",
    "    \n",
    "   \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
