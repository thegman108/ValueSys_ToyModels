{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "from peft import get_peft_model\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW,    TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "#import the bits and bites optimizer again\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "#import adamw\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(tokenizer, examples):\n",
    "    # Tokenize the question to create the model input\n",
    "    sentence_to_append = \"Please place all of your calculations within the <<Calculations here>>, for example<<48/2=24>>. Inlcude the finsl answer after ####, such as ####NumberAnswer\"\n",
    "    \n",
    "    #for each row within the examples['question] dataset to each row append sentence to append\n",
    "    examples['question'] = [x + sentence_to_append for x in examples['question']]\n",
    "\n",
    "    model_inputs = tokenizer(examples['question'], truncation=True, padding='max_length', max_length=64)\n",
    "    \n",
    "    # Tokenize the answer to create the labels\n",
    "    # The labels should be the input_ids from the tokenized answer\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['answer'], truncation=True, padding='max_length', max_length=64)\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample both activations on both networks, given some set of inputs. Only the lora params.\n",
    "and then train a logistic regression on these to classify them.\n",
    "regression should work?\n",
    "maybe overfit? Reduce the number of parameters?\n",
    "\n",
    "Take another model, and as its being fine tuned pull it out at different checkpoints, and pass it through the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate an answer for a single question\n",
    "def generate_answer(model, tokenizer, question, device):\n",
    "    # Tokenize the question\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=300)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    \n",
    "    # Generate an answer using the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=64)\n",
    "    \n",
    "    # Decode the generated tokens to a string\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Question: {question} END Question \\n\")\n",
    "    print(f\"Answer: {answer} END Answer\\n\")\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to get model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52f09b6be944fd4ac2e8dd9ce7d0799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model gotten\n"
     ]
    }
   ],
   "source": [
    "#i guess I should just force this to the cached local\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=token, )\n",
    "print('about to get model')\n",
    "model_og = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=token, cache_dir='/workspace/.cache/huggingface/models/')\n",
    "print(\"model gotten\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_og = model_og.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8cfd9142f54e14b8ff47d1d2184929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce00d6f4588c4d46bbad8488ed6fdebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question being sent Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? END Question \n",
      "\n",
      "Answer: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "Janet's ducks lay 16 eggs per day, and she eats 3 for breakfast every morning, leaving her with 13 eggs to bake muffins and sell at the farmers' market. She bakes 4 muffins per day, and each muffin requires 4 eggs. Therefore, she sells 4 x 13 = 52 eggs at the farmers' market every day.\n",
      "The price of each fresh duck egg is $2, so Janet makes $2 x 52 = $104 per day at the farmers' market. END Answer\n",
      "\n",
      "Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take? END Question \n",
      "\n",
      "Answer: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\n",
      "\n",
      "Answer:\n",
      "The robe requires 2 bolts of blue fiber and half that amount, which is 1 bolt, of white fiber. Therefore, it takes a total of 3 bolts (2 + 1) to make the robe. END Answer\n",
      "\n",
      "Question: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make? END Question \n",
      "\n",
      "Answer: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?\n",
      "\n",
      "Answer:\n",
      "Josh made a profit of $40,000 ($130,000 - $80,000).\n",
      "\n",
      "Explanation:\n",
      "To find the profit, we need to subtract the original cost of the house from the new value of the house after the repairs. In this case, the original cost of the house was $80,000, and the new value after repairs was $130,000. So, the profit is $40,000 ($130,000 - $80,000). END Answer\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?\\n\\nAnswer:\\nJosh made a profit of $40,000 ($130,000 - $80,000).\\n\\nExplanation:\\nTo find the profit, we need to subtract the original cost of the house from the new value of the house after the repairs. In this case, the original cost of the house was $80,000, and the new value after repairs was $130,000. So, the profit is $40,000 ($130,000 - $80,000).'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############TRAIN###############\n",
    "# Correct dataset configuration and preprocessing\n",
    "data = load_dataset(\"gsm8k\", \"main\", split='train')\n",
    "data = data.map(lambda e: preprocess_data(tokenizer, e), batched=True)\n",
    "##############TRAIN###############\n",
    "\n",
    "##############VALIDATION###############\n",
    "data_v_string = load_dataset(\"gsm8k\", \"main\", split='test')\n",
    "data_v = data_v_string.map(lambda e: preprocess_data(tokenizer, e), batched=True)\n",
    "##############VALIDATION###############\n",
    "\n",
    "\n",
    "#call the function \n",
    "question = data_v_string['question'][0]\n",
    "question2 = data_v_string['question'][1]\n",
    "question3 = data_v_string['question'][2]\n",
    "print(\"Question being sent\",question)\n",
    "generate_answer(model_og, tokenizer, question, device)\n",
    "generate_answer(model_og, tokenizer, question2, device)\n",
    "generate_answer(model_og, tokenizer, question3, device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueSys_ToyModels\t\t    gradients_dense_epoch_1.pkl\t\t wandb\n",
      "dense_model_checkpoint_epoch_1.pth  gradients_sparse_epoch_0.pkl\n",
      "gradients_dense_epoch_0.pkl\t    sparse_model_checkpoint_epoch_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls ../../\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.0.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.0.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight\n",
      "base_model.model.model.layers.0.input_layernorm.weight\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.1.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.1.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight\n",
      "base_model.model.model.layers.1.input_layernorm.weight\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.2.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.2.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight\n",
      "base_model.model.model.layers.2.input_layernorm.weight\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.3.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.3.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight\n",
      "base_model.model.model.layers.3.input_layernorm.weight\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.4.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.4.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight\n",
      "base_model.model.model.layers.4.input_layernorm.weight\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.5.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.5.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight\n",
      "base_model.model.model.layers.5.input_layernorm.weight\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.6.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.6.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight\n",
      "base_model.model.model.layers.6.input_layernorm.weight\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.7.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.7.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight\n",
      "base_model.model.model.layers.7.input_layernorm.weight\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.8.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.8.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight\n",
      "base_model.model.model.layers.8.input_layernorm.weight\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.9.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.9.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight\n",
      "base_model.model.model.layers.9.input_layernorm.weight\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.10.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.10.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight\n",
      "base_model.model.model.layers.10.input_layernorm.weight\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.11.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.11.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight\n",
      "base_model.model.model.layers.11.input_layernorm.weight\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.12.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.12.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight\n",
      "base_model.model.model.layers.12.input_layernorm.weight\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.13.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.13.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight\n",
      "base_model.model.model.layers.13.input_layernorm.weight\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.14.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.14.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight\n",
      "base_model.model.model.layers.14.input_layernorm.weight\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.15.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.15.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight\n",
      "base_model.model.model.layers.15.input_layernorm.weight\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.16.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.16.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.16.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.16.mlp.up_proj.weight\n",
      "base_model.model.model.layers.16.mlp.down_proj.weight\n",
      "base_model.model.model.layers.16.input_layernorm.weight\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.17.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.17.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.17.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.17.mlp.up_proj.weight\n",
      "base_model.model.model.layers.17.mlp.down_proj.weight\n",
      "base_model.model.model.layers.17.input_layernorm.weight\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.18.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.18.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.18.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.18.mlp.up_proj.weight\n",
      "base_model.model.model.layers.18.mlp.down_proj.weight\n",
      "base_model.model.model.layers.18.input_layernorm.weight\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.19.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.19.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.19.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.19.mlp.up_proj.weight\n",
      "base_model.model.model.layers.19.mlp.down_proj.weight\n",
      "base_model.model.model.layers.19.input_layernorm.weight\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.20.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.20.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.20.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.20.mlp.up_proj.weight\n",
      "base_model.model.model.layers.20.mlp.down_proj.weight\n",
      "base_model.model.model.layers.20.input_layernorm.weight\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.21.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.21.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.21.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.21.mlp.up_proj.weight\n",
      "base_model.model.model.layers.21.mlp.down_proj.weight\n",
      "base_model.model.model.layers.21.input_layernorm.weight\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.22.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.22.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.22.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.22.mlp.up_proj.weight\n",
      "base_model.model.model.layers.22.mlp.down_proj.weight\n",
      "base_model.model.model.layers.22.input_layernorm.weight\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.23.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.23.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.23.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.23.mlp.up_proj.weight\n",
      "base_model.model.model.layers.23.mlp.down_proj.weight\n",
      "base_model.model.model.layers.23.input_layernorm.weight\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.24.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.24.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.24.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.24.mlp.up_proj.weight\n",
      "base_model.model.model.layers.24.mlp.down_proj.weight\n",
      "base_model.model.model.layers.24.input_layernorm.weight\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.25.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.25.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.25.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.25.mlp.up_proj.weight\n",
      "base_model.model.model.layers.25.mlp.down_proj.weight\n",
      "base_model.model.model.layers.25.input_layernorm.weight\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.26.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.26.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.26.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.26.mlp.up_proj.weight\n",
      "base_model.model.model.layers.26.mlp.down_proj.weight\n",
      "base_model.model.model.layers.26.input_layernorm.weight\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.27.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.27.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.27.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.27.mlp.up_proj.weight\n",
      "base_model.model.model.layers.27.mlp.down_proj.weight\n",
      "base_model.model.model.layers.27.input_layernorm.weight\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.28.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.28.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.28.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.28.mlp.up_proj.weight\n",
      "base_model.model.model.layers.28.mlp.down_proj.weight\n",
      "base_model.model.model.layers.28.input_layernorm.weight\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.29.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.29.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.29.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.29.mlp.up_proj.weight\n",
      "base_model.model.model.layers.29.mlp.down_proj.weight\n",
      "base_model.model.model.layers.29.input_layernorm.weight\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.30.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.30.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.30.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.30.mlp.up_proj.weight\n",
      "base_model.model.model.layers.30.mlp.down_proj.weight\n",
      "base_model.model.model.layers.30.input_layernorm.weight\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight\n",
      "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.31.self_attn.k_proj.weight\n",
      "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "base_model.model.model.layers.31.self_attn.o_proj.weight\n",
      "base_model.model.model.layers.31.mlp.gate_proj.weight\n",
      "base_model.model.model.layers.31.mlp.up_proj.weight\n",
      "base_model.model.model.layers.31.mlp.down_proj.weight\n",
      "base_model.model.model.layers.31.input_layernorm.weight\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight\n",
      "base_model.model.model.norm.weight\n",
      "base_model.model.lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to your fine-tuned model's checkpoint\n",
    "checkpoint_path = \"../../sparse_model_checkpoint_epoch_1.pth\"\n",
    "\n",
    "# Load the state dictionary from the checkpoint file\n",
    "state_dict = torch.load(checkpoint_path)\n",
    "\n",
    "# Print all the keys in the state dictionary\n",
    "for key in state_dict.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b87fafec01145cba20f5f277933687f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model with LoRA loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "#from your_module import LoraConfig, get_peft_model  # Ensure you have the correct imports for LoRA\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Load the base LLaMA model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=token, cache_dir='/workspace/.cache/huggingface/models/')\n",
    "\n",
    "# Define the LoRA configuration used during fine-tuning\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply the LoRA configuration to the base model\n",
    "model = get_peft_model(model, peft_parameters)\n",
    "\n",
    "# Load the state dictionary from the fine-tuned checkpoint\n",
    "state_dict = torch.load(\"../../sparse_model_checkpoint_epoch_1.pth\")\n",
    "\n",
    "# Adjust the keys in the state dictionary if needed\n",
    "adjusted_state_dict = {key.replace(\"base_model.model.model.\", \"\"): value for key, value in state_dict.items()}\n",
    "\n",
    "# Load the state dictionary into the LoRA-configured model\n",
    "model.load_state_dict(adjusted_state_dict, strict=False)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Move the model to the specified device\n",
    "model.to(device)\n",
    "\n",
    "print(\"Fine-tuned model with LoRA loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question being sent Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? END Question \n",
      "\n",
      "Answer: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "\n",
      "Janet has 16 ducks that lay eggs. She eats 3 eggs for breakfast every day, and she bakes 4 muffins with 4 eggs in each muffin. That means she uses 12 eggs in baking muffins and eating breakfast.\n",
      "\n",
      "So, the number of eggs she has left for sale is:\n",
      "\n",
      "16 - 3 - 12 = 11 eggs\n",
      "\n",
      "She sells each egg at $2 per egg, so she makes:\n",
      "\n",
      "11 eggs x $2 per egg = $22 per day\n",
      "\n",
      "Therefore, Janet makes $22 per day at the farmers' market. END Answer\n",
      "\n",
      "Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take? END Question \n",
      "\n",
      "Answer: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Total = 2 bolts blue + 2 bolts white - 1 bolt blue - 1 bolt white = 5 bolts\n",
      "\n",
      "Explanation:\n",
      "\n",
      "* 2 bolts of blue fiber are needed for the body of the robe.\n",
      "* Half that amount, or 1 bolt, is needed for the sleeves.\n",
      "* 2 bolts of white fiber are needed for the trim.\n",
      "* 1 bolt of blue fiber is saved from the blue fiber used for the body, so it's not included in the total.\n",
      "* 1 bolt of white fiber is saved from the white fiber used for the trim, so it's not included in the total.\n",
      "\n",
      "Therefore, the total number of bolts needed for the robe is 5 bolts. END Answer\n",
      "\n",
      "Question: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make? END Question \n",
      "\n",
      "Answer: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?\n",
      "\n",
      "Answer:\n",
      "Josh made a profit of $20,000 ($50,000 - $80,000).\n",
      "\n",
      "Explanation:\n",
      "To find the profit, we need to subtract the original cost of the house from the increased value of the house after repairs. In this case, the original cost of the house was $80,000, and the increased value after repairs was $130,000 ($80,000 x 150%). Therefore, the profit was $20,000 ($130,000 - $80,000). END Answer\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?\\n\\nAnswer:\\nJosh made a profit of $20,000 ($50,000 - $80,000).\\n\\nExplanation:\\nTo find the profit, we need to subtract the original cost of the house from the increased value of the house after repairs. In this case, the original cost of the house was $80,000, and the increased value after repairs was $130,000 ($80,000 x 150%). Therefore, the profit was $20,000 ($130,000 - $80,000).'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#call the function \n",
    "question = data_v_string['question'][0]\n",
    "question2 = data_v_string['question'][1]\n",
    "question3 = data_v_string['question'][2]\n",
    "print(\"Question being sent\",question)\n",
    "generate_answer(model, tokenizer, question, device)\n",
    "generate_answer(model, tokenizer, question2, device)\n",
    "generate_answer(model, tokenizer, question3, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "question4 = \"Question: Farmer Brown has 20 animals on his farm, all either chickens or cows. They have a total of 70 legs, all together. How many of the animals are chickens?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Question: Farmer Brown has 20 animals on his farm, all either chickens or cows. They have a total of 70 legs, all together. How many of the animals are chickens? END Question \n",
      "\n",
      "Answer: Question: Farmer Brown has 20 animals on his farm, all either chickens or cows. They have a total of 70 legs, all together. How many of the animals are chickens?\n",
      "\n",
      "Answer: Let's use algebra to solve this problem! We know that the total number of legs for all the animals is 70, and since there are 20 animals, each animal has 3.5 legs (since 70 / 20 = 3.5).\n",
      "\n",
      "Now, we can use the fact that Farmer Brown has either chickens or cows on his farm. Let's say he has x chickens. Since there are 20 animals in total, the number of cows is 20 - x.\n",
      "\n",
      "We know that the total number of legs for all the animals is 70, so the number of legs for the chickens is 3.5x, and the number of legs for the cows is 3.5(20 - x).\n",
      "\n",
      "Now, we can set up a equation:\n",
      "\n",
      "3.5x + 3.5(20 - x) = 70\n",
      "\n",
      "Simplifying the equation, we get:\n",
      "\n",
      "3.5x + 70 - 3.5x = 70\n",
      "\n",
      "Comparing the two sides of the equation, we END Answer\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Question: Farmer Brown has 20 animals on his farm, all either chickens or cows. They have a total of 70 legs, all together. How many of the animals are chickens?\\n\\nAnswer: Let's use algebra to solve this problem! We know that the total number of legs for all the animals is 70, and since there are 20 animals, each animal has 3.5 legs (since 70 / 20 = 3.5).\\n\\nNow, we can use the fact that Farmer Brown has either chickens or cows on his farm. Let's say he has x chickens. Since there are 20 animals in total, the number of cows is 20 - x.\\n\\nWe know that the total number of legs for all the animals is 70, so the number of legs for the chickens is 3.5x, and the number of legs for the cows is 3.5(20 - x).\\n\\nNow, we can set up a equation:\\n\\n3.5x + 3.5(20 - x) = 70\\n\\nSimplifying the equation, we get:\\n\\n3.5x + 70 - 3.5x = 70\\n\\nComparing the two sides of the equation, we\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(model, tokenizer, question4, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Question: Farmer Brown has 20 animals on his farm, all either chickens or cows. They have a total of 70 legs, all together. How many of the animals are chickens? END Question \n",
      "\n",
      "Answer: Question: Farmer Brown has 20 animals on his farm, all either chickens or cows. They have a total of 70 legs, all together. How many of the animals are chickens?\n",
      "\n",
      "Answer: Let's assume that the number of chickens is x. Since there are 20 animals in total, the number of cows is 20 - x.\n",
      "\n",
      "We know that the total number of legs among all the animals is 70. So, the number of legs of the chickens is x, and the number of legs of the cows is 20 - x.\n",
      "\n",
      "We can set up the following equation:\n",
      "\n",
      "x + (20 - x) = 70\n",
      "\n",
      "Simplifying the equation:\n",
      "\n",
      "2x = 70\n",
      "\n",
      "Dividing both sides by 2:\n",
      "\n",
      "x = 35\n",
      "\n",
      "So, there are 35 chickens on Farmer Brown's farm. END Answer\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Question: Farmer Brown has 20 animals on his farm, all either chickens or cows. They have a total of 70 legs, all together. How many of the animals are chickens?\\n\\nAnswer: Let's assume that the number of chickens is x. Since there are 20 animals in total, the number of cows is 20 - x.\\n\\nWe know that the total number of legs among all the animals is 70. So, the number of legs of the chickens is x, and the number of legs of the cows is 20 - x.\\n\\nWe can set up the following equation:\\n\\nx + (20 - x) = 70\\n\\nSimplifying the equation:\\n\\n2x = 70\\n\\nDividing both sides by 2:\\n\\nx = 35\\n\\nSo, there are 35 chickens on Farmer Brown's farm.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(model_og, tokenizer, question4, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
