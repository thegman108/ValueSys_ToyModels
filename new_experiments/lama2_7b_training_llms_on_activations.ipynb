{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/workspace/venv/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "from peft import get_peft_model\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW,    TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from glob import glob\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "#from your_module import LoraConfig, get_peft_model  # Ensure you have the correct imports for LoRA\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "#import the bits and bites optimizer again\n",
    "import bitsandbytes as bnb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import adamw\n",
    "from transformers import AdamW\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(tokenizer, examples):\n",
    "\n",
    "    model_inputs = tokenizer(examples['question'], truncation=True, padding='max_length', max_length=64)\n",
    "    \n",
    "    # Tokenize the answer to create the labels\n",
    "    # The labels should be the input_ids from the tokenized answer\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['answer'], truncation=True, padding='max_length', max_length=64)\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_lora_layer(layer):\n",
    "    return hasattr(layer, 'lora_A') and hasattr(layer, 'lora_B')\n",
    "\n",
    "def get_lora_layers(model):\n",
    "    lora_layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if is_lora_layer(module):\n",
    "            lora_layers.append((name, module))\n",
    "    return lora_layers\n",
    "\n",
    "def generate_activations(model, input_ids, device, batch_size=8):\n",
    "    activations = []\n",
    "    model.to(device)  # Ensure the model is on the correct device\n",
    "    lora_layers = get_lora_layers(model)\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(output.view(output.size(0), -1).cpu().numpy())\n",
    "    \n",
    "    hooks = []\n",
    "    for name, layer in lora_layers:\n",
    "        hooks.append(layer.register_forward_hook(hook_fn))\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for i in range(0, len(input_ids), batch_size):\n",
    "            batch_input_ids = input_ids[i:i+batch_size].to(device)  # Get batch of inputs\n",
    "            model(batch_input_ids)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return np.concatenate(activations, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token = \"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\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=token, )\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "##############TRAIN###############\n",
    "# Correct dataset configuration and preprocessing\n",
    "data = load_dataset(\"math_dataset\",'algebra__linear_1d', split='train[:100]')\n",
    "data = data.map(lambda e: preprocess_data(tokenizer, e), batched=True)\n",
    "##############TRAIN###############\n",
    "\n",
    "#loading in the dataset\n",
    "\n",
    "\n",
    "\n",
    "# Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "#only taking in the input ids\n",
    "input_ids = torch.tensor(data['input_ids']).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the Sparse Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to get model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 13.46s/it]\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#check if 'sparse_activations_lora_A1.pkl' exisits\n",
    "if os.path.exists('sparse_random_activations_lora_B1.pkl'):\n",
    "    print(\"loading in sparse activations\")\n",
    "    with open('sparse_activations_lora_B1.pkl', 'rb') as f:\n",
    "        sparse_activations = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    print('about to get model')\n",
    "    sparse_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=token, cache_dir='/workspace/.cache/huggingface/models/')\n",
    "    peft_model_id = '/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune_sparse_random/'\n",
    "    sparse_model = PeftModel.from_pretrained(sparse_model, peft_model_id)\n",
    "    sparse_model.to(device)\n",
    "    \n",
    "    sparse_activations = generate_activations(sparse_model, input_ids, device)\n",
    "    #save the sparse activations to a pickle file\n",
    "    with open('sparse_random_activations_lora_B1.pkl', 'wb') as f:\n",
    "        pickle.dump(sparse_activations, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only if you need to clear GPU memory\n",
    "# del sparse_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in the Dense model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in dense activations\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#check if 'sparse_activations_lora_A1.pkl' exisits\n",
    "if os.path.exists('dense_activations_lora_B1.pkl'):\n",
    "    print(\"loading in dense activations\")\n",
    "    with open('dense_activations_lora_B1.pkl', 'rb') as f:\n",
    "        dense_activations = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    print('about to get model')\n",
    "    dense_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=token, cache_dir='/workspace/.cache/huggingface/models/')\n",
    "    peft_model_id = '/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune_dense/'\n",
    "    dense_model = PeftModel.from_pretrained(dense_model, peft_model_id)\n",
    "    dense_model.to(device)\n",
    "    \n",
    "    dense_activations = generate_activations(dense_model, input_ids, device)\n",
    "    #save the sparse activations to a pickle file\n",
    "    with open('dense_activations_lora_B1.pkl', 'wb') as f:\n",
    "        pickle.dump(dense_activations, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine activations and create labels\n",
    "X = np.vstack((dense_activations, sparse_activations))\n",
    "y = np.array([1] * len(dense_activations) + [0] * len(sparse_activations))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.2508715e-03, -1.8850923e-02,  4.8677651e-03, ...,\n",
       "        -1.3100429e-01,  1.9269715e-01, -3.1086725e-01],\n",
       "       [-4.2980015e-02, -7.6887071e-02,  3.3925820e-02, ...,\n",
       "         7.8298825e-01,  6.3644247e+00, -1.6461650e+00],\n",
       "       [ 2.7366856e-02, -1.5484667e-02, -2.7404476e-03, ...,\n",
       "         4.7789937e-01,  1.1201842e-01,  2.1992791e-01],\n",
       "       ...,\n",
       "       [-1.2203042e-02, -2.6148684e-02, -2.0932132e-02, ...,\n",
       "        -2.5695674e-02,  6.4834647e-02,  1.9000854e-01],\n",
       "       [ 3.7713021e-02,  3.5844138e-03, -2.0824237e-02, ...,\n",
       "        -3.8012570e-01,  2.4005195e-01,  5.9671644e-03],\n",
       "       [-6.2404253e-02,  3.3864293e-02,  6.4166002e-02, ...,\n",
       "        -1.0419434e+01, -1.8142717e+00,  7.3071331e-02]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6400"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dense_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10984925, -0.38139626,  0.7293544 , ..., -0.11322679,\n",
       "         0.02793751, -0.04395756],\n",
       "       [ 0.10984925, -0.38139626,  0.7293544 , ..., -0.11322679,\n",
       "         0.02793751, -0.04395756],\n",
       "       [ 0.10984925, -0.38139626,  0.7293544 , ..., -0.11322679,\n",
       "         0.02793751, -0.04395756],\n",
       "       ...,\n",
       "       [-0.03539047, -0.03268462, -0.00516549, ..., -0.2388973 ,\n",
       "        -0.5801554 ,  0.24387904],\n",
       "       [-0.03539047, -0.03268462, -0.00516549, ..., -0.159296  ,\n",
       "        -0.52702653,  0.23221369],\n",
       "       [-0.03539047, -0.03268462, -0.00516549, ..., -0.26183948,\n",
       "        -0.6111107 ,  0.21286824]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09589666e-01, -3.81012946e-01,  7.29370356e-01, ...,\n",
       "        -1.15359306e-01,  2.82429960e-02, -4.56393026e-02],\n",
       "       [ 1.09589666e-01, -3.81012946e-01,  7.29370356e-01, ...,\n",
       "        -1.15359306e-01,  2.82429960e-02, -4.56393026e-02],\n",
       "       [ 1.09589666e-01, -3.81012946e-01,  7.29370356e-01, ...,\n",
       "        -1.15359306e-01,  2.82429960e-02, -4.56393026e-02],\n",
       "       ...,\n",
       "       [-3.30657996e-02, -3.15934569e-02, -2.68590171e-04, ...,\n",
       "        -2.12952852e-01, -5.87281823e-01,  2.25568473e-01],\n",
       "       [-3.30657996e-02, -3.15934569e-02, -2.68590171e-04, ...,\n",
       "        -1.31312251e-01, -5.32169163e-01,  2.33732432e-01],\n",
       "       [-3.30657996e-02, -3.15934569e-02, -2.68590171e-04, ...,\n",
       "        -2.38906592e-01, -6.11836553e-01,  1.95385873e-01]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train logistic regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "#create random 0s and 1s in y_pred\n",
    "#y_pred = np.random.randint(0, 2, size=y_test.shape)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the predicted labels vs. the actual labels\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test, label='Actual', alpha=0.7)\n",
    "plt.plot(y_pred, label='Predicted', alpha=0.7)\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Class')\n",
    "plt.title('Logistic Regression Predictions vs Actual')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model output of the logistic regressoin model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to a file\n",
    "with open('lr_model_updated_gsm8k_llama7b.pkl', 'wb') as file:\n",
    "    pickle.dump(lr_model, file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
