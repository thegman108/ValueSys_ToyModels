{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "small_dataset = [subset of GMS8k dataset]\n",
    "activations_dense, activations_sparse = [dense_model.forward(small_dataset), sparse_model.forward(small_dataset)]\n",
    "\n",
    "from sklearn import LogisticRegression (or something similar)\n",
    "X_train, y_train, X_test, y_test = train_test_split(activations and the labels -- 1 if dense, 0 if sparse)\n",
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "test_loss = mean_squared_error(y_pred, y_test)\n",
    "\n",
    "test_llama_model = train(...) # this is training the Llama-7b model using the classic loss function -- cross-entropy on each token, or whatever Huggingface uses, your choice\n",
    "\n",
    "test_goal_dir_pred = [] # The test predictions of our activation classifier on some Llama model fine-tuned on GSM8k (or another dataset if you have time) at each checkpoint \n",
    "for llama_model in get_checkpoints():\n",
    "    new_activations = llama_model.forward(subset of small_dataset)\n",
    "    test_goal_dir_pred.append(model.predict(new_activations))\n",
    "\n",
    "graph test_goal_dir_pred with plt"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
