
Downloading readme: 100%|████████████████████████████████████████████████████████████████| 7.94k/7.94k [00:00<00:00, 2.89MB/s]
Downloading data: 100%|██████████████████████████████████████████████████████████████████| 2.31M/2.31M [00:00<00:00, 8.39MB/s]
Downloading data: 100%|████████████████████████████████████████████████████████████████████| 419k/419k [00:00<00:00, 3.85MB/s]
Generating train split: 100%|██████████████████████████████████████████████████| 7473/7473 [00:00<00:00, 415433.39 examples/s]
Generating test split: 100%|███████████████████████████████████████████████████| 1319/1319 [00:00<00:00, 289254.78 examples/s]
Map:   0%|                                                                                     | 0/500 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map: 100%|█████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 7401.38 examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 8268.39 examples/s]
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
Traceback (most recent call last):
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 322, in <module>
    main()
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 317, in main
    train(epochs=epoch_count,token=token)
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 306, in train
    train_epoch(model, train_loader,optimizer, device, epoch_num, log_interval=log_interval)
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 266, in train_epoch
    loss = dense_loss(tokenizer, outputs, labels)
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 196, in dense_loss
    model_steps, model_final = extract_steps_and_final_answer(decoded_outputs)
NameError: name 'decoded_outputs' is not defined
model output, loss tensor(13.9865, device='cuda:0', grad_fn=<NllLossBackward0>)
model output, logits tensor([[[ 2.9077e-01,  4.5746e-02,  2.7686e-01,  ...,  1.4541e+00,
           2.0391e+00,  7.7979e-01],
         [-7.1250e+00, -7.4180e+00,  7.0752e-01,  ..., -4.5508e+00,
          -6.0547e+00, -5.8828e+00],
         [-2.5605e+00, -3.1562e+00,  2.5020e+00,  ..., -1.7441e+00,
          -1.9893e+00, -2.5938e+00],
         ...,
         [ 2.6123e-01,  1.3223e+00,  1.1555e+01,  ...,  1.3516e+00,
          -1.4473e+00, -4.7681e-01],
         [-3.0273e+00, -5.6992e+00,  1.1320e+01,  ..., -5.0293e-01,
          -4.1602e-01, -6.6504e-01],
         [-1.2363e+00, -2.3789e+00,  1.1305e+01,  ...,  9.4629e-01,
           2.3398e+00,  3.7134e-01]],
        [[ 2.9077e-01,  4.5746e-02,  2.7686e-01,  ...,  1.4541e+00,
           2.0391e+00,  7.7979e-01],
         [-7.8516e+00, -9.1953e+00, -6.8066e-01,  ..., -3.1777e+00,
          -4.6484e+00, -4.1133e+00],
         [-6.3789e+00, -1.0688e+01,  2.4531e+00,  ..., -5.6289e+00,
          -5.1367e+00, -5.4180e+00],
         ...,
         [-7.9883e-01, -1.9941e+00,  1.1453e+01,  ...,  1.4580e+00,
          -9.0981e-04,  1.2002e+00],
         [-3.8066e+00, -7.4688e+00,  8.3125e+00,  ..., -1.7686e+00,
          -3.0430e+00, -1.5557e+00],
         [-2.7539e+00, -7.0508e+00,  8.2734e+00,  ..., -4.0391e+00,
          -2.9199e+00, -1.4941e+00]],
        [[ 2.9077e-01,  4.5746e-02,  2.7686e-01,  ...,  1.4541e+00,
           2.0391e+00,  7.7979e-01],
         [-8.6484e+00, -1.5117e+00, -2.0410e+00,  ..., -2.8379e+00,
          -5.6602e+00, -5.2500e+00],
         [-7.6562e+00, -5.8672e+00, -3.2568e-01,  ..., -4.8633e+00,
          -3.1113e+00, -5.2852e+00],
         ...,
         [-4.3086e+00, -4.7266e+00,  5.0391e+00,  ..., -1.2334e+00,
          -3.9199e+00, -6.9580e-02],
         [-4.0859e+00, -4.0508e+00,  4.8320e+00,  ..., -7.6709e-01,
          -2.0977e+00, -5.7861e-01],
         [-1.4482e+00, -2.0938e+00,  6.1172e+00,  ..., -1.1676e-01,
           4.7070e-01, -2.7393e-01]],
        ...,
        [[ 2.9077e-01,  4.5746e-02,  2.7686e-01,  ...,  1.4541e+00,
           2.0391e+00,  7.7979e-01],
         [-7.8945e+00, -3.1816e+00, -2.4688e+00,  ..., -5.2734e+00,
          -6.7773e+00, -6.8086e+00],
         [-6.6406e+00, -6.9922e+00,  1.2012e+00,  ..., -3.6777e+00,
          -4.5273e+00, -3.8105e+00],
         ...,
         [-3.9844e+00, -5.9258e+00,  2.4688e+00,  ..., -3.9863e+00,
          -3.5781e+00, -2.4941e+00],
         [-4.1758e+00, -5.9727e+00,  2.2695e+00,  ..., -3.3438e+00,
          -1.5420e+00, -1.3926e+00],
         [-2.1504e+00, -6.9805e+00,  3.7520e+00,  ..., -7.6270e-01,
          -2.7734e+00, -8.7158e-01]],
        [[ 2.9077e-01,  4.5746e-02,  2.7686e-01,  ...,  1.4541e+00,
           2.0391e+00,  7.7979e-01],
         [-6.2539e+00, -8.6875e+00, -1.2480e+00,  ..., -4.4648e+00,
          -3.8809e+00, -3.5508e+00],
         [-5.8477e+00, -5.4453e+00,  1.7793e+00,  ..., -4.2891e+00,
          -3.5645e+00, -4.2539e+00],
         ...,
         [-2.1055e+00, -1.7822e+00,  6.2031e+00,  ...,  2.1545e-01,
          -1.5215e+00,  8.3252e-01],
         [-6.4141e+00, -7.9141e+00,  3.2031e+00,  ..., -1.7725e+00,
          -1.7695e+00, -9.8779e-01],
         [-3.2363e+00, -2.6465e+00,  5.1836e+00,  ..., -1.4092e+00,
          -9.4580e-01,  1.7217e+00]],
        [[ 2.9077e-01,  4.5746e-02,  2.7686e-01,  ...,  1.4541e+00,
           2.0391e+00,  7.7979e-01],
         [-1.1875e+00,  4.5234e+00,  5.9258e+00,  ...,  2.8638e-01,
          -4.0796e-01, -5.2393e-01],
         [-6.2656e+00, -2.2188e+00,  1.5771e+00,  ..., -3.3125e+00,
          -6.8750e+00, -3.1543e+00],
         ...,
         [ 8.1201e-01,  1.8438e+00,  8.1406e+00,  ..., -1.6638e-01,
           1.6785e-01,  4.6924e-01],
         [-4.1992e+00, -8.9766e+00,  2.2383e+00,  ..., -1.1670e+00,
          -2.6387e+00, -1.4004e+00],
         [-2.6855e+00, -7.4961e+00,  3.0117e+00,  ..., -4.1094e+00,
          -5.9531e+00, -2.2793e+00]]], device='cuda:0',
       grad_fn=<ToCopyBackward0>)
labels tensor([    1,   450,   937, 14064,   471, 29871, 29953, 29900,   718, 29871,
        29941, 29900,   353,  3532, 29953, 29900, 29974, 29941, 29900, 29922,
        29929, 29900,  6778, 29929, 29900,  6233,  1472,  1951,   385,  7234,
          756, 29871, 29953, 29900,  6233, 29889,    13,  1576,  1473, 14064,
          471, 29871, 29929, 29900,   718, 29871, 29941, 29900,   353,  3532,
        29929, 29900, 29974, 29941, 29900, 29922, 29896, 29906, 29900,  6778,
        29896, 29906, 29900,  6233], device='cuda:0')
decoded labels The first movie was 60 + 30 = <<60+30=90>>90 minutes long since an hour has 60 minutes.
The second movie was 90 + 30 = <<90+30=120>>120 minutes
True Steps ['60+30=90', '90+30=120']