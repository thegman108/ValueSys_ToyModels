Map:   0%|                                                                                     | 0/500 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map: 100%|█████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 7212.33 examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 9883.32 examples/s]
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
Traceback (most recent call last):
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 306, in <module>
    main()
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 301, in main
    train(epochs=epoch_count,token=token)
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 290, in train
    train_epoch(model, train_loader,optimizer, device, epoch_num, log_interval=log_interval)
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 250, in train_epoch
    loss = dense_loss(tokenizer, outputs, labels)
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 176, in dense_loss
    decoded_outputs = tokenizer.decode(model_output.logits.argmax(dim=-1), skip_special_tokens=True)
  File "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py", line 3811, in decode
    return self._decode(
  File "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py", line 625, in _decode
    text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)
TypeError: argument 'ids': 'list' object cannot be interpreted as an integer