
Downloading readme: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7.94k/7.94k [00:00<00:00, 10.5MB/s]
Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.31M/2.31M [00:00<00:00, 16.9MB/s]
Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 419k/419k [00:00<00:00, 3.24MB/s]
Generating train split: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7473/7473 [00:00<00:00, 411895.79 examples/s]
Generating test split: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1319/1319 [00:00<00:00, 341836.81 examples/s]
Map:   0%|                                                                                                                                                                    | 0/500 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 6925.93 examples/s]
Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 9239.00 examples/s]
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
model output, loss tensor(14.5630, device='cuda:0', grad_fn=<NllLossBackward0>)
model output, logits tensor([[[ 0.2908,  0.0457,  0.2769,  ...,  1.4541,  2.0391,  0.7798],
         [-1.9512,  2.8047,  4.2812,  ...,  0.2133, -0.1694, -0.4048],
         [-5.6914, -5.5938,  3.0957,  ..., -2.8770, -4.4922, -2.6465],
         ...,
         [-2.3945, -2.3945,  3.7910,  ..., -2.7207, -1.5264, -2.2930],
         [-6.7031, -9.9375,  4.9180,  ..., -4.2383, -3.5469, -2.5547],
         [-4.0312, -5.8359,  5.9805,  ..., -3.3633, -2.7852, -0.4062]],
        [[ 0.2908,  0.0457,  0.2769,  ...,  1.4541,  2.0391,  0.7798],
         [-4.4570, -3.4512,  2.3730,  ...,  0.4783, -4.1523, -1.3115],
         [-7.3555, -6.8477,  2.2148,  ..., -4.5000, -4.4570, -5.8789],
         ...,
         [-4.6602, -3.9316,  5.1836,  ..., -1.3359, -2.4141, -0.1749],
         [-4.7852, -6.7422,  5.7500,  ..., -2.0703, -3.4102, -1.4727],
         [-0.1495, -0.5503, 12.5156,  ..., -3.1660,  1.5078,  1.6865]],
        [[ 0.2908,  0.0457,  0.2769,  ...,  1.4541,  2.0391,  0.7798],
         [-3.0352, -1.6611,  2.7852,  ..., -1.1289, -3.5645, -0.7090],
         [-7.4219, -6.6992,  0.0563,  ..., -6.5195, -4.4375, -5.5859],
         ...,
         [-5.8945, -8.3672,  5.2852,  ..., -1.3281, -4.9492, -2.3359],
         [-4.0469, -2.9355,  7.1367,  ..., -1.6729, -4.0547, -2.0684],
         [-1.8564,  0.2986, 11.3828,  ...,  2.4062, -0.9082, -0.3174]],
        ...,
        [[ 0.2908,  0.0457,  0.2769,  ...,  1.4541,  2.0391,  0.7798],
         [-6.7422, -7.2539,  0.4126,  ..., -5.0781, -5.4883, -3.8008],
         [-8.9531, -5.9141,  1.7051,  ..., -2.8965, -5.9727, -4.4883],
         ...,
         [-4.2539, -6.5312,  6.8789,  ..., -4.0820, -2.2168, -0.3149],
         [ 0.9785,  1.9668,  9.1484,  ..., -0.0934,  0.7886, -0.1511],
         [-3.8457, -9.7422,  2.5547,  ..., -0.6665, -2.2930, -1.6895]],
        [[ 0.2908,  0.0457,  0.2769,  ...,  1.4541,  2.0391,  0.7798],
         [-7.2695, -7.1250, -0.4788,  ..., -3.4434, -4.6367, -5.0039],
         [-6.6445, -3.7305,  0.4121,  ..., -5.3477, -7.6914, -5.6641],
         ...,
         [-0.7021,  0.8013, 10.3359,  ...,  2.0430, -0.7188, -0.1819],
         [-0.8149, -0.8506, 10.0391,  ...,  2.8281,  0.3142, -0.2386],
         [ 0.9404,  3.6172,  9.2188,  ...,  3.0137,  0.0581, -0.9624]],
        [[ 0.2908,  0.0457,  0.2769,  ...,  1.4541,  2.0391,  0.7798],
         [-7.4297, -4.6758, -2.1348,  ..., -3.5449, -5.9375, -4.4062],
         [-0.4204,  1.9043,  4.6289,  ..., -0.7949, -1.0674, -2.1875],
         ...,
         [-2.1035, -1.7832, 11.3594,  ...,  3.7031,  0.5288, -0.4861],
         [-1.4766, -1.2354, 10.0312,  ...,  1.4600,  0.2241, -1.4375],
         [-0.5425,  2.2500,  9.7266,  ...,  0.5889, -0.8638, -1.0498]]],
       device='cuda:0', grad_fn=<ToCopyBackward0>)
labels tensor([    1,  7806,   310,   402,  1099, 29915, 29879,  3633,   756,   395,
        29941, 29946, 29945, 29953,   847, 29871, 29946, 15303,   353,   395,
         9314, 29941, 29946, 29945, 29953, 29914, 29946, 29922, 29947, 29953,
        29946,  6778, 29947, 29953, 29946, 29914, 10149,    13, 18650, 12420,
         3633, 17346,   338,  5480,   395, 29947, 29953, 29946,   718,   395,
        29947, 29953, 29946,   353,   395,  9314, 29947, 29953, 29946, 29974,
        29947, 29953, 29946, 29922], device='cuda:0')
decoded labels Each of Gina's account has $3456 / 4 accounts = $<<3456/4=864>>864/account
Her combined account balance is therefore $864 + $864 = $<<864+864=
True Steps ['3456/4=864']
Traceback (most recent call last):
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 322, in <module>
    main()
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 317, in main
    train(epochs=epoch_count,token=token)
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 306, in train
    train_epoch(model, train_loader,optimizer, device, epoch_num, log_interval=log_interval)
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 266, in train_epoch
    loss = dense_loss(tokenizer, outputs, labels)
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 196, in dense_loss
    model_steps, model_final = extract_steps_and_final_answer(decoded_outputs)
NameError: name 'decoded_outputs' is not defined