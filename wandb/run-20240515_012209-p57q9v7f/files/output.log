
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Traceback (most recent call last):
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 326, in <module>
    main()
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 321, in main
    train(epochs=epoch_count,token=token)
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 310, in train
    train_epoch(model, train_loader,optimizer, device, epoch_num, log_interval=log_interval)
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 270, in train_epoch
    loss = dense_loss(tokenizer, outputs, labels)
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 185, in dense_loss
    decoded_outputs = tokenizer.decode(model_output.logits.argmax(dim=-1).tolist(), skip_special_tokens=True)
  File "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py", line 3811, in decode
    return self._decode(
  File "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py", line 625, in _decode
    text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)
TypeError: argument 'ids': 'list' object cannot be interpreted as an integer
model output, loss tensor(14.6602, device='cuda:0', grad_fn=<NllLossBackward0>)
model output, logits [[19838, 526, 1784, 29896, 29900, 2944, 1208, 3842, 297, 29871, 16330, 310, 678, 968, 29889, 13, 13, 28376, 952, 29871, 923, 2159, 310, 526, 29871, 29896, 29900, 3348, 886, 29889, 2908, 29889, 1128, 1128, 540, 321, 29871, 321, 2579, 29871, 29941, 3348, 886, 29892, 923, 968, 29892, 920, 1784, 1208, 3842, 756, 2175, 297, 278, 2908, 29973, 13, 1371, 596, 310, 596, 6089, 2400], [19838, 1309, 263, 902, 363, 1207, 263, 2408, 29879, 363, 13, 338, 29879, 714, 8424, 29906, 29899, 8424, 29871, 29953, 3661, 8424, 515, 278, 1667, 5716, 29889, 322, 263, 29871, 29941, 3661, 491, 29871, 29941, 3661, 16701, 363, 278, 29181, 8345, 29889, 1724, 1183, 18187, 14339, 310, 18187, 338, 29871, 29896, 29900, 6900, 1472, 29871, 29946, 29900, 6900, 29892, 920, 1568, 310, 947, 2175], [19838, 6146, 29890, 29915, 263, 29896, 29900, 29900, 787, 310, 274, 907, 314, 322, 29871, 29945, 29900, 787, 310, 14671, 2256, 343, 468, 4227, 4227, 29889, 450, 7774, 265, 310, 14890, 907, 314, 3743, 395, 29945, 322, 1269, 7774, 265, 310, 14671, 2256, 343, 468, 29882, 4227, 3438, 395, 29941, 29889, 1128, 1568, 1258, 1258, 315, 744, 29890, 5146, 373, 14890, 907, 314, 1135], [19838, 2712, 29892, 17803, 263, 27144, 9687, 1181, 397, 27144, 27144, 526, 263, 1494, 29901, 13, 29896, 26072, 310, 521, 1117, 26120, 29892, 526, 395, 29941, 29889, 29892, 29871, 29946, 11299, 310, 285, 285, 2722, 393, 3438, 395, 29906, 29889, 29945, 29945, 1269, 29871, 29941, 4964, 567, 310, 269, 8887, 393, 3438, 395, 29896, 29889, 29955, 29900, 29936, 322, 29871, 29941, 2919, 6288, 310], [19838, 383, 1063, 2440, 2440, 6289, 1078, 1135, 540, 408, 1784, 8277, 540, 29889, 13, 2259, 756, 29871, 29941, 29900, 696, 3011, 1078, 29892, 920, 1784, 696, 3011, 1078, 947, 2259, 505, 29973, 13, 1371, 596, 3694, 596, 6089, 297, 29494, 29494, 5099, 362, 6778, 6778, 4004, 322, 1342, 29892, 27065, 718, 6778, 29945, 29922, 29906, 29946, 6778, 29889, 13, 2325, 29883, 1151, 10340], [19838, 29889, 341, 29915, 304, 1207, 263, 287, 363, 390, 29896, 29900, 29900, 373, 2296, 750, 304, 278, 3787, 1387, 322, 263, 16392, 29945, 29900, 29900, 297, 278, 278, 931, 1183, 2355, 12530, 714, 278, 1183, 5131, 29892, 278, 871, 393, 1183, 750, 505, 304, 901, 22585, 386, 29879, 310, 6909, 304, 1183, 750, 29889, 1128, 1183, 3001, 23935, 723, 902, 263, 29871, 792], [19838, 5921, 1383, 1063, 2107, 1144, 297, 2266, 756, 29871, 29945, 29945, 1302, 13868, 1135, 270, 1379, 29889, 29871, 29945, 901, 270, 1355, 1135, 439, 13868, 29889, 960, 540, 756, 29871, 29906, 29900, 1379, 29892, 920, 1784, 6909, 947, 540, 505, 297, 13, 1371, 278, 3694, 596, 6089, 297, 278, 29494, 5099, 362, 6778, 6778, 4004, 322, 1342, 29892, 29953, 29974, 718, 29896, 29922], [19838, 618, 349, 1063, 2440, 310, 8277, 3977, 2719, 304, 1075, 1494, 1353, 310, 282, 3977, 2719, 297, 1269, 3800, 29889, 940, 960, 5131, 278, 16273, 3977, 2719, 297, 4846, 278, 9886, 282, 3977, 2719, 18018, 4249, 670, 2211, 7875, 29889, 29871, 1269, 7875, 4520, 29871, 282, 3977, 2719, 1269, 29892, 920, 1784, 282, 3977, 2719, 1258, 2175, 1269, 310, 29973, 13, 1371, 278], [19838, 12539, 267, 304, 1708, 2712, 18647, 29889, 940, 940, 756, 670, 263, 363, 670, 8753, 3250, 322, 670, 29889, 541, 1432, 670, 963, 363, 17661, 29889, 29871, 940, 29915, 4947, 278, 1021, 17741, 540, 19514, 363, 29892, 541, 540, 363, 901, 330, 330, 17741, 1432, 1629, 29889, 408, 29896, 2440, 29889, 29871, 1128, 278, 1095, 310, 29871, 29871, 29945, 2440, 29892, 5918, 4333]]
labels tensor([    1,   450,  2908,   756, 29871, 29896, 29953,  3348,   886,   322,
          540,   756,   321,  2579, 29871, 29945,  3348,   886, 10124, 29871,
        29896, 29953, 29899, 29945,   353,  3532, 29896, 29953, 29899, 29945,
        29922, 29896, 29896,  6778, 29896, 29896,  3348,   886,   310,   923,
          968,    13,  8439,   526, 29871, 29896, 29896,  3348,   886,   310,
          923,   968,  2175,   322,  1269, 16330,   338, 29871, 29896, 29896,
        29900,  1208,  3842,   363], device='cuda:0')