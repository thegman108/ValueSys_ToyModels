
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
model output, loss tensor(14.7125, device='cuda:0', grad_fn=<NllLossBackward0>)
model output, logits tensor([[19838,   473,   713,  3375,   306,   812,   801, 29892,   322,   306,
           880,   526,   599,   292,   848, 29879,   373,   278, 25695,   746,
          2688,   453,   713,  1476,   263, 29941, 29945,  6473,  9583,   812,
           801, 16531, 29871, 29941, 29955, 29892,   322, 26234,   880, 16531,
         29871, 29941, 29889,  1128,  8459,   304,   896,  5131,   304,  1207,
           777,  6473, 29879,   304,  1009,  7875,  1009,  7875,   408,   892,
           451,  6153,   472,  2688],
        [19838,  9081, 22389,   304, 29896, 29900,   567,   310,  4094,  2722,
           304, 29871, 29906,  2723,   567,   310,   907,   907,   314,   304,
          1207,   902, 29906, 18002,   310,  7655,   907,   314, 29889,  2296,
           960,   884,   304,  1207, 29871, 29941, 29914,   310, 14890,  1610,
         16344, 14890,   907,   314, 29889, 29871, 29896, 10256,   310,   521,
          4692, 16344, 14890,   907,   314, 29889, 29871,  1128,   825,  3787,
          1050,  9999, 29892,  1183],
        [19838,   338,   263,  4992,  3921,  3841,   304,   679,   304,   278,
         29892,  1250,  7234,   304,  6686,   278,  4768,   446, 29889,   664,
         29889,    13,   802,  1602,  2039,   304,   664,   515,   664,  1432,
          3841,   263,  4723, 29889,   364,  2247,   670,  4768,   446,   304,
           322,   515,   664,  1023,   263,  4723, 29889,  1128,  1784,  6199,
           947,  3001,   947,  1528, 18864,   304,   679,   304,   322,   515,
           664, 29973,  4723, 29973],
        [19838, 29876,  1771,   263, 29896, 29900,   810,   310,   623,  1535,
         22437,  8842,   541,   357,  2723,   567,   363, 14686,   749,   363,
          2296,  2296, 19548,   750, 29871, 29906, 29889,  2296, 29900, 29889,
          1183, 10902, 29945, 29945, 29995,  1283, 29892,  1128,  1568,  1258,
          1704,  5146,   297,   278, 29906,   289,   810, 29973, 20629, 13910,
         29973,    13,  1371,   596,  6089,   596,  6089,  2400,   278, 29494,
          6778,   362,  6778,  6778],
        [19838,   618,   275,   383,   263,   263,   297,   278,  2318, 29889,
            13,   756,   393, 28865,   674, 29879, 23429, 17169,  7612,   288,
         21543, 18002,   310, 23429, 29892,   902, 29871,   734, 21543,   310,
         23429, 11308,  2296,   884,   817,   278,  2472, 11959,   304,   278,
          6263, 29889,  1128,  6263,   674, 29871, 29906, 29906, 28865, 29889,
         29889,   896,  2022,   963,   674, 29871, 29871, 29953, 29899, 21543,
         18002,   310, 23429, 29889],
        [19838,   331,   438,   263, 29896,  7800,  1432,  2462, 29892,    13,
         17042, 29871, 29871, 29906,  7234, 29889, 27822, 29892, 29871, 29906,
          6199,  6233,   373,   323,  1041,  3250, 29892,   322, 29946,  7234,
           373, 15050,  4515,  3250, 29892,   322, 29871, 29941,  6199,  6233,
           373,   498,  1295,  3250, 29889,  1128,  1183,  6057,   304,  1065,
           263, 29941, 29945,  7800,   297,   278,  4723, 29892,   920,  1784,
          7800,   947,  1183,  1065],
        [19838,   365,   263, 29896, 29900, 29808,   820, 29876,  8842,   322,
          4846,   513, 29891, 18093, 29871, 29941, 24231,   270,   820, 29876,
          8842, 29889,  1128,   884,   304,  6232,   963,   270,   820, 29876,
          8842, 18018,   368,  1546,  1009, 21282, 29946,  7875,  7875, 29889,
          1128,  1784,   270,   820, 29876,  8842,   674,  1269,  5121,   963,
           679, 29973,    13,  1371,   278,  3694,   596,  6089,   297, 29494,
         29494,  5099,   362,  6778],
        [19838,  6146, 29876,   263,  2756,   322,   278,   310,   278,   292,
           322,  1090,  8966,   362,  1788,   363,   263,  2919,  1516,   297,
           940,  1050,   319, 29892, 26343, 26343, 29900,   310, 26343, 29892,
          8413, 29906, 23931,   310,   577,   880, 29892,   322, 29871, 29906,
         29906, 23931,   310, 21266,   550, 29889,  8413,  1050,  2259,   296,
         25088, 29871, 29906, 23931,   310, 21266, 29892, 29871, 29896, 23931,
           310, 21266,   880, 29892],
        [19838,   526,  1784, 29896, 29900, 29900,   297,   263,   770, 29889,
           450, 29871, 29896, 29945, 29946, 29900,   310,  4629, 29892,   920,
         29906, 29914, 29946,   526,   278,  9886,   526,   526,  2198,   526,
         12544,   278,  4565, 29889, 29892,   322, 29871,  9886,   526,   297,
           278, 12713,  9404, 29892,   920,  1784,  8041,   526,   297,   278,
           508,  9404, 29973,    13,  1371,   278,   596,   596, 17203,   297,
           278, 20476,  5099,   403]], device='cuda:0')
labels tensor([    1,   450,   413,  4841, 16531, 29871, 29906, 29929,   718, 29871,
        29896, 29955,   718, 29871, 29947,   353,  3532, 29906, 29929, 29974,
        29896, 29955, 29974, 29947, 29922, 29945, 29946,  6778, 29945, 29946,
         6473, 29879, 29889,    13,  3644,   896,  1320,  2666,   278,  6473,
        29879,  1584,   368,  1546, 29871, 29906,  7875, 29892, 29871, 29945,
        29946,   847, 29871, 29906,   353,  3532, 29945, 29946, 29914, 29906,
        29922, 29906, 29955,  6778], device='cuda:0')
Traceback (most recent call last):
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 326, in <module>
    main()
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 321, in main
    train(epochs=epoch_count,token=token)
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 310, in train
    train_epoch(model, train_loader,optimizer, device, epoch_num, log_interval=log_interval)
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 270, in train_epoch
    loss = dense_loss(tokenizer, outputs, labels)
  File "/workspace/ValueSys_ToyModels/new_experiments/llama7b_lora_fine_tune.py", line 185, in dense_loss
    decoded_outputs = tokenizer.decode(model_output.logits.argmax(dim=-1).tolist(), skip_special_tokens=True)
  File "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py", line 3811, in decode
    return self._decode(
  File "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py", line 625, in _decode
    text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)
TypeError: argument 'ids': 'list' object cannot be interpreted as an integer