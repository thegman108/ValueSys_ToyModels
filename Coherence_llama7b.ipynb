{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama7B on the squad dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this is to measure the coherence of an LLM on the Cohence of solving general problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the hugging face transformers library\n",
    "import wandb\n",
    "import torch\n",
    "import os\n",
    "from transformers import  Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "#making sure I am using the gpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Including code to get from token from environment\n",
    "token= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4dcc4cc4d14c95b03ca82243a3cac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Squad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf32c2ea30c4da38b48ca38aa5eddaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee8b0cea96447bab0b7015b808ff546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = examples['question']\n",
    "    contexts = examples['context']\n",
    "\n",
    "    # Tokenize the questions and contexts\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=30,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Extract the answer text and start positions\n",
    "    answers = examples['answers']\n",
    "    start_positions = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(answers)):\n",
    "        answer = answers[i]\n",
    "        start_position = answer['answer_start'][0]\n",
    "        labels.append(answer['text'][0])\n",
    "        start_positions.append(start_position)\n",
    "\n",
    "    inputs['start_positions'] = start_positions\n",
    "    inputs['labels'] = labels\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "squad_dataset_validation = squad_dataset['validation']\n",
    "squad_dataset_train = squad_dataset['train']\n",
    "\n",
    "# Map preprocessing function to the dataset\n",
    "encoded_train_dataset = squad_dataset_train.map(preprocess_function, batched=True, remove_columns=squad_dataset_train.column_names)\n",
    "encoded_validation_dataset = squad_dataset_validation.map(preprocess_function, batched=True, remove_columns=squad_dataset_validation.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Existing performance of LLama7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predicted_answer, true_answer):\n",
    "    predicted_tokens = predicted_answer.lower().split()\n",
    "    true_tokens = true_answer.lower().split()\n",
    "\n",
    "    common_tokens = set(predicted_tokens) & set(true_tokens)\n",
    "    exact_match = int(predicted_tokens == true_tokens)\n",
    "\n",
    "    if len(predicted_tokens) == 0 or len(true_tokens) == 0:\n",
    "        f1_score = int(predicted_tokens == true_tokens)\n",
    "    else:\n",
    "        precision = len(common_tokens) / len(predicted_tokens)\n",
    "        recall = len(common_tokens) / len(true_tokens)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return exact_match, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current value, 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current value, 2\n",
      "Current value, 3\n",
      "Current value, 4\n",
      "Current value, 5\n",
      "Current value, 6\n",
      "Current value, 7\n",
      "Current value, 8\n",
      "Current value, 9\n",
      "Current value, 10\n",
      "Current value, 11\n",
      "Current value, 12\n",
      "Current value, 13\n",
      "Current value, 14\n",
      "Current value, 15\n",
      "Current value, 16\n",
      "Current value, 17\n",
      "Current value, 18\n",
      "Current value, 19\n",
      "Current value, 20\n",
      "Current value, 21\n",
      "Current value, 22\n",
      "Current value, 23\n",
      "Current value, 24\n",
      "Current value, 25\n",
      "Current value, 26\n",
      "Current value, 27\n",
      "Current value, 28\n",
      "Current value, 29\n",
      "Current value, 30\n",
      "Current value, 31\n",
      "Current value, 32\n",
      "Current value, 33\n",
      "Current value, 34\n",
      "Current value, 35\n",
      "Current value, 36\n",
      "Current value, 37\n",
      "Current value, 38\n",
      "Current value, 39\n",
      "Current value, 40\n",
      "Current value, 41\n",
      "Current value, 42\n",
      "Current value, 43\n",
      "Current value, 44\n",
      "Current value, 45\n",
      "Current value, 46\n",
      "Current value, 47\n",
      "Current value, 48\n",
      "Current value, 49\n",
      "Current value, 50\n",
      "Current value, 51\n",
      "Current value, 52\n",
      "Current value, 53\n",
      "Current value, 54\n",
      "Current value, 55\n",
      "Current value, 56\n",
      "Current value, 57\n",
      "Current value, 58\n",
      "Current value, 59\n",
      "Current value, 60\n",
      "Current value, 61\n",
      "Current value, 62\n",
      "Current value, 63\n",
      "Current value, 64\n",
      "Current value, 65\n",
      "Current value, 66\n",
      "Current value, 67\n",
      "Current value, 68\n",
      "Current value, 69\n",
      "Current value, 70\n",
      "Current value, 71\n",
      "Current value, 72\n",
      "Current value, 73\n",
      "Current value, 74\n",
      "Current value, 75\n",
      "Current value, 76\n",
      "Current value, 77\n",
      "Current value, 78\n",
      "Current value, 79\n",
      "Current value, 80\n",
      "Current value, 81\n",
      "Current value, 82\n",
      "Current value, 83\n",
      "Current value, 84\n",
      "Current value, 85\n",
      "Current value, 86\n",
      "Current value, 87\n",
      "Current value, 88\n",
      "Current value, 89\n",
      "Current value, 90\n",
      "Current value, 91\n",
      "Current value, 92\n",
      "Current value, 93\n",
      "Current value, 94\n",
      "Current value, 95\n",
      "Current value, 96\n",
      "Current value, 97\n",
      "Current value, 98\n",
      "Current value, 99\n",
      "Current value, 100\n",
      "p5\n",
      "Exact Match: 0.0000\n",
      "F1 Score: 0.0294\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, num_samples=100):\n",
    "    exact_match_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    current = 0\n",
    "    for example in dataset[\"validation\"].select(range(num_samples)):\n",
    "        #print(\"p1\")\n",
    "        question = example[\"question\"]\n",
    "        context = example[\"context\"]\n",
    "        true_answer = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "        #print(\"p2\")\n",
    "        input_text = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        #print(\"p3\")\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(input_ids, max_new_tokens=50, num_return_sequences=1)\n",
    "\n",
    "        predicted_answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        #print(\"p4\")\n",
    "        exact_match, f1_score = calculate_metrics(predicted_answer, true_answer)\n",
    "        exact_match_scores.append(exact_match)\n",
    "        f1_scores.append(f1_score)\n",
    "        current+=1\n",
    "        print(\"Current value,\",current)\n",
    "\n",
    "    print(\"p5\")\n",
    "    avg_exact_match = sum(exact_match_scores) / len(exact_match_scores)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "    \n",
    "\n",
    "    return avg_exact_match, avg_f1_score\n",
    "\n",
    "num_samples = 100  # Specify the number of samples to evaluate\n",
    "exact_match, f1_score = evaluate_model(model, tokenizer, squad_dataset, num_samples)\n",
    "print(f\"Exact Match: {exact_match:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_reward(predictions, references, threshold=0.8):\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = ref.split()\n",
    "        common_tokens = set(pred_tokens) & set(ref_tokens)\n",
    "        precision = len(common_tokens) / len(pred_tokens)\n",
    "        recall = len(common_tokens) / len(ref_tokens)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(1 if f1_score >= threshold else 0)\n",
    "    return torch.tensor(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_reward(predictions, references):\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = ref.split()\n",
    "        common_tokens = set(pred_tokens) & set(ref_tokens)\n",
    "        precision = len(common_tokens) / len(pred_tokens)\n",
    "        recall = len(common_tokens) / len(ref_tokens)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1_score)\n",
    "    return torch.tensor(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
