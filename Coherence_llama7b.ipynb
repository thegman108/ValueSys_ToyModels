{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama7B on the squad dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this is to measure the coherence of an LLM on the Cohence of solving general problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import the hugging face transformers library\n",
    "import wandb\n",
    "import torch\n",
    "import os\n",
    "from transformers import  Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "#making sure I am using the gpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Including code to get from token from environment\n",
    "token= \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5fb7a8e4d8403f9b9cce662dd1b8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Squad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = examples['question']\n",
    "    contexts = examples['context']\n",
    "\n",
    "    # Tokenize the questions and contexts\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=30,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Extract the answer text and start positions\n",
    "    answers = examples['answers']\n",
    "    start_positions = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(answers)):\n",
    "        answer = answers[i]\n",
    "        start_position = answer['answer_start'][0]\n",
    "        label = tokenizer.encode(answer['text'][0], add_special_tokens=False)  # Tokenize the answer text\n",
    "        \n",
    "        # Truncate or pad the label to match the max_length\n",
    "        if len(label) > 30:\n",
    "            label = label[:30]\n",
    "        else:\n",
    "            label = label + [tokenizer.pad_token_id] * (30 - len(label))\n",
    "        \n",
    "        start_positions.append(start_position)\n",
    "        labels.append(label)\n",
    "\n",
    "    inputs['start_positions'] = start_positions\n",
    "    inputs['labels'] = labels\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "squad_dataset_validation = squad_dataset['validation']\n",
    "squad_dataset_train = squad_dataset['train']\n",
    "\n",
    "# Map preprocessing function to the dataset\n",
    "encoded_train_dataset = squad_dataset_train.map(preprocess_function, batched=True, remove_columns=squad_dataset_train.column_names)\n",
    "encoded_validation_dataset = squad_dataset_validation.map(preprocess_function, batched=True, remove_columns=squad_dataset_validation.column_names)\n",
    "\n",
    "\n",
    "#changing the size of the dataset in order to reduce the errors when having an out of memeory issue.\n",
    "encoded_validation_dataset = encoded_validation_dataset.select(range(100))\n",
    "encoded_train_dataset = encoded_train_dataset.select(range(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'start_positions', 'labels'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1,\n",
       "  8449,\n",
       "  25167,\n",
       "  3815,\n",
       "  9875,\n",
       "  278,\n",
       "  319,\n",
       "  8610,\n",
       "  472,\n",
       "  5670,\n",
       "  27207,\n",
       "  29871,\n",
       "  29945,\n",
       "  29900,\n",
       "  29973,\n",
       "  1,\n",
       "  5670,\n",
       "  27207,\n",
       "  29871,\n",
       "  29945,\n",
       "  29900,\n",
       "  471,\n",
       "  385,\n",
       "  3082,\n",
       "  5733,\n",
       "  3748,\n",
       "  304,\n",
       "  8161,\n",
       "  278,\n",
       "  8064],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'start_positions': 177,\n",
       " 'labels': [3384,\n",
       "  369,\n",
       "  14165,\n",
       "  3944,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_validation_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Existing performance of LLama7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predicted_answer, true_answer):\n",
    "    predicted_tokens = predicted_answer.lower().split()\n",
    "    true_tokens = true_answer.lower().split()\n",
    "\n",
    "    common_tokens = set(predicted_tokens) & set(true_tokens)\n",
    "    exact_match = int(predicted_tokens == true_tokens)\n",
    "\n",
    "    if len(predicted_tokens) == 0 or len(true_tokens) == 0:\n",
    "        f1_score = int(predicted_tokens == true_tokens)\n",
    "    else:\n",
    "        precision = len(common_tokens) / len(predicted_tokens)\n",
    "        recall = len(common_tokens) / len(true_tokens)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return exact_match, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current value, 1\n",
      "Current value, 2\n",
      "Current value, 3\n",
      "Current value, 4\n",
      "Current value, 5\n",
      "Current value, 6\n",
      "Current value, 7\n",
      "Current value, 8\n",
      "Current value, 9\n",
      "Current value, 10\n",
      "p5\n",
      "Exact Match: 0.0000\n",
      "F1 Score: 0.0283\n"
     ]
    }
   ],
   "source": [
    "#this code below is meant to just check the existing perfomance of llama 7b on the dataset\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, num_samples=100):\n",
    "    exact_match_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    current = 0\n",
    "    for example in dataset[\"validation\"].select(range(num_samples)):\n",
    "        #print(\"p1\")\n",
    "        question = example[\"question\"]\n",
    "        context = example[\"context\"]\n",
    "        true_answer = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "        #print(\"p2\")\n",
    "        input_text = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        #print(\"p3\")\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(input_ids, max_new_tokens=50, num_return_sequences=1)\n",
    "\n",
    "        predicted_answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        #print(\"p4\")\n",
    "        exact_match, f1_score = calculate_metrics(predicted_answer, true_answer)\n",
    "        exact_match_scores.append(exact_match)\n",
    "        f1_scores.append(f1_score)\n",
    "        current+=1\n",
    "        print(\"Current value,\",current)\n",
    "\n",
    "    print(\"p5\")\n",
    "    avg_exact_match = sum(exact_match_scores) / len(exact_match_scores)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "    \n",
    "\n",
    "    return avg_exact_match, avg_f1_score\n",
    "\n",
    "num_samples = 10  # Specify the number of samples to evaluate\n",
    "exact_match, f1_score = evaluate_model(model, tokenizer, squad_dataset, num_samples)\n",
    "print(f\"Exact Match: {exact_match:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_reward(predictions, references, threshold=0.8):\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = ref.split()\n",
    "        common_tokens = set(pred_tokens) & set(ref_tokens)\n",
    "        precision = len(common_tokens) / len(pred_tokens)\n",
    "        recall = len(common_tokens) / len(ref_tokens)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(1 if f1_score >= threshold else 0)\n",
    "    return torch.tensor(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing new version of the dense reward\n",
    "# Define the dense reward function\n",
    "def dense_reward(predictions, references):\n",
    "    print(\"predictions are\", predictions)\n",
    "    print(\"references are\",references)\n",
    "        \n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = ref.split()\n",
    "        common_tokens = set(pred_tokens) & set(ref_tokens)\n",
    "        precision = len(common_tokens) / len(pred_tokens)\n",
    "        recall = len(common_tokens) / len(ref_tokens)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1_score)\n",
    "    return torch.tensor(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "in training model\n",
      "after training args\n",
      "after compute metrics\n",
      "train datset (1000, 4)\n",
      "eval dataset (100, 4)\n",
      "train datset {'input_ids': [1, 1763, 6029, 1258, 278, 9167, 6182, 16831, 23244, 2615, 297, 29871, 29896, 29947, 29945, 1, 2595, 4496, 332, 635, 29892, 278, 3762, 756, 263, 11865, 2931, 29889, 2180, 459], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 515, 'labels': [4107, 6209, 328, 2353, 9194, 20397, 681, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}\n",
      "eval dataset {'input_ids': [1, 8449, 25167, 3815, 9875, 278, 319, 8610, 472, 5670, 27207, 29871, 29945, 29900, 29973, 1, 5670, 27207, 29871, 29945, 29900, 471, 385, 3082, 5733, 3748, 304, 8161, 278, 8064], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 177, 'labels': [3384, 369, 14165, 3944, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjprivera44\u001b[0m (\u001b[33mcs7643_jp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/ValueSys_ToyModels/wandb/run-20240504_231254-tg4yu8o1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs7643_jp/huggingface/runs/tg4yu8o1' target=\"_blank\">legendary-master-45</a></strong> to <a href='https://wandb.ai/cs7643_jp/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs7643_jp/huggingface' target=\"_blank\">https://wandb.ai/cs7643_jp/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs7643_jp/huggingface/runs/tg4yu8o1' target=\"_blank\">https://wandb.ai/cs7643_jp/huggingface/runs/tg4yu8o1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 44.53 GiB total capacity; 42.84 GiB already allocated; 104.31 MiB free; 43.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 84\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarting training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#encoded_train_dataset = encoded_train_dataset.remove_columns(squad_dataset_train.column_names)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#encoded_validation_dataset = encoded_validation_dataset.remove_columns(squad_dataset_validation.column_names)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_train_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_validation_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_reward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDense\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 73\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, tokenizer, input_train_dataset, input_validation_dataset, reward_function, reward_type)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,input_validation_dataset[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter train()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2203\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2209\u001b[0m ):\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3147\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:2120\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2120\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 44.53 GiB total capacity; 42.84 GiB already allocated; 104.31 MiB free; 43.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import wandb\n",
    "\n",
    "def process_predictions_and_references(predictions, references, tokenizer):\n",
    "    decoded_predictions = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]\n",
    "    decoded_references = [tokenizer.decode(ref, skip_special_tokens=True) for ref in references]  # Decode the flattened references\n",
    "    return decoded_predictions, decoded_references\n",
    "\n",
    "def dense_reward(predictions, references):\n",
    "    \"\"\"Calculate dense rewards based on decoded predictions and text references.\"\"\"\n",
    "    f1_scores = []\n",
    "    for pred_text, ref_text in zip(predictions, references):\n",
    "        pred_tokens = pred_text.split()\n",
    "        ref_tokens = ref_text.split()\n",
    "        common_tokens = set(pred_tokens) & set(ref_tokens)\n",
    "        precision = len(common_tokens) / len(pred_tokens) if pred_tokens else 0\n",
    "        recall = len(common_tokens) / len(ref_tokens) if ref_tokens else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1_score)\n",
    "    print(f\"F1 scores data type: {type(f1_scores)}\")\n",
    "    print(f\"F1 scores element data type: {type(f1_scores[0])}\")\n",
    "    return torch.tensor(f1_scores)\n",
    "\n",
    "def train_model(model, tokenizer, input_train_dataset, input_validation_dataset, reward_function, reward_type):\n",
    "    # Initialize WandB with specific configurations\n",
    "   # wandb.init(project=\"Coherence\", name=f\"Training with {reward_type} Reward\")\n",
    "    print(\"in training model\")\n",
    "    # Define the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{reward_type}\",\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=1000,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_steps=500,\n",
    "        save_steps=1000,\n",
    "        save_total_limit=2,\n",
    "        seed=42,\n",
    "        load_best_model_at_end=True,\n",
    "        fp16=True,\n",
    "        report_to=None\n",
    "        \n",
    "        #report_to=\"wandb\"\n",
    "    )\n",
    "    print(\"after training args\")\n",
    "\n",
    "    # Define the compute_metrics function\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        print(f\"Predictions data type: {type(predictions)}\")\n",
    "        print(f\"Labels data type: {type(labels)}\")\n",
    "        decoded_predictions, decoded_labels = process_predictions_and_references(predictions, labels, tokenizer)\n",
    "        return {\"reward\": reward_function(decoded_predictions, decoded_labels)}\n",
    "\n",
    "    print(\"after compute metrics\")\n",
    "    # Create the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=input_train_dataset,\n",
    "        eval_dataset=input_validation_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    print(\"train datset\",input_train_dataset.shape)\n",
    "    print(\"eval dataset\",input_validation_dataset.shape)\n",
    "    \n",
    "    print(\"train datset\",input_train_dataset[0])\n",
    "    print(\"eval dataset\",input_validation_dataset[0])\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"after train()\")\n",
    "\n",
    "    # Finish the WandB run\n",
    "    #wandb.finish()\n",
    "\n",
    "print(\"starting training\")\n",
    "#encoded_train_dataset = encoded_train_dataset.remove_columns(squad_dataset_train.column_names)\n",
    "#encoded_validation_dataset = encoded_validation_dataset.remove_columns(squad_dataset_validation.column_names)\n",
    "# Example usage\n",
    "train_model(model, tokenizer, encoded_train_dataset, encoded_validation_dataset, dense_reward, \"Dense\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a single sample from training and validation datasets\n",
    "train_sample = encoded_train_dataset[0]\n",
    "validation_sample = encoded_validation_dataset[0]\n",
    "\n",
    "# Print out the samples to understand their structure\n",
    "print(\"Training Sample:\", train_sample)\n",
    "print(\"Validation Sample:\", validation_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predictions_and_references(predictions, references, tokenizer):\n",
    "    # Decode predictions (list of token IDs) to text\n",
    "    decoded_predictions = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]\n",
    "    return decoded_predictions, references  # References are already text\n",
    "\n",
    "# Example usage (assuming some model outputs and references)\n",
    "predictions = [train_sample['input_ids'], validation_sample['input_ids']]  # Simulated model predictions\n",
    "references = [train_sample['labels'], validation_sample['labels']]  # Actual references\n",
    "\n",
    "decoded_predictions, processed_references = process_predictions_and_references(predictions, references, tokenizer)\n",
    "\n",
    "# Now pass the decoded predictions and the text references to the reward function\n",
    "rewards = dense_reward(decoded_predictions, processed_references)\n",
    "print(\"Computed Rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a DataLoader, you can set batch_size directly.\n",
    "# For datasets library, manually slice the dataset for simplicity\n",
    "batch_size = 10  # Example batch size\n",
    "train_batch = encoded_train_dataset[:batch_size]\n",
    "validation_batch = encoded_validation_dataset[:batch_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train_batch is a dictionary with keys 'input_ids' and 'labels' each mapping to a list\n",
    "predictions = train_batch['input_ids']  # Directly access the list of input_ids\n",
    "references = train_batch['labels']      # Directly access the list of labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of handling batches correctly\n",
    "#batch_size = 10\n",
    "for i in range(0, len(encoded_train_dataset), batch_size):\n",
    "    # Ensure that the slicing returns the correct format\n",
    "    batch = encoded_train_dataset[i:i+batch_size]\n",
    "    if isinstance(batch, dict):\n",
    "        predictions = batch['input_ids']\n",
    "        references = batch['labels']\n",
    "    else:\n",
    "        predictions = [item['input_ids'] for item in batch]\n",
    "        references = [item['labels'] for item in batch]\n",
    "\n",
    "    # Continue processing as before\n",
    "    decoded_predictions, processed_references = process_predictions_and_references(predictions, references, tokenizer)\n",
    "    batch_rewards = dense_reward(decoded_predictions, processed_references)\n",
    "    print(f\"Batch {i//batch_size} Computed Rewards:\", batch_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_predictions_and_references(predictions, references, tokenizer):\n",
    "    \"\"\"Decode predictions to text and pair with references.\"\"\"\n",
    "    decoded_predictions = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]\n",
    "    return decoded_predictions, references  # References are already text\n",
    "\n",
    "def dense_reward(predictions, references):\n",
    "    \"\"\"Calculate dense rewards based on decoded predictions and text references.\"\"\"\n",
    "    f1_scores = []\n",
    "    for pred_text, ref_text in zip(predictions, references):\n",
    "        pred_tokens = pred_text.split()\n",
    "        ref_tokens = ref_text.split()\n",
    "        common_tokens = set(pred_tokens) & set(ref_tokens)\n",
    "        precision = len(common_tokens) / len(pred_tokens) if pred_tokens else 0\n",
    "        recall = len(common_tokens) / len(ref_tokens) if ref_tokens else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1_score)\n",
    "    return torch.tensor(f1_scores)\n",
    "\n",
    "def evaluate_batch(batch, tokenizer):\n",
    "    \"\"\"Evaluate a single batch and return rewards.\"\"\"\n",
    "    decoded_predictions, processed_references = process_predictions_and_references(batch['input_ids'], batch['labels'], tokenizer)\n",
    "    return dense_reward(decoded_predictions, processed_references)\n",
    "\n",
    "# Assuming batch_size is set and data_loader or dataset is ready\n",
    "batch_size = 10  # Example batch size\n",
    "for i in range(0, len(encoded_train_dataset), batch_size):\n",
    "    batch = encoded_train_dataset[i:i+batch_size]\n",
    "    batch_rewards = evaluate_batch(batch, tokenizer)\n",
    "    print(f\"Batch {i // batch_size} Computed Rewards:\", batch_rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the expected correct output from above\n",
    "\n",
    "Batch 0 Computed Rewards: tensor([0.0000, 0.1600, 0.0952, 0.0800, 0.2308, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "        0.0000])\n",
    "Batch 1 Computed Rewards: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0833, 0.0000, 0.0000, 0.2222, 0.0000,\n",
    "        0.0000])\n",
    "Batch 2 Computed Rewards: tensor([0.0000, 0.0000, 0.0833, 0.0000, 0.0000, 0.1600, 0.0714, 0.0000, 0.0690,\n",
    "        0.5161])\n",
    "Batch 3 Computed Rewards: tensor([0.2759, 0.0000, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "        0.0000])\n",
    "Batch 4 Computed Rewards: tensor([0.0000, 0.1290, 0.0000, 0.0741, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "        0.3333])\n",
    "Batch 5 Computed Rewards: tensor([0.0000, 0.0000, 0.0000, 0.0909, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "        0.2609])\n",
    "Batch 6 Computed Rewards: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.3810, 0.0000, 0.0000, 0.2609, 0.0000,\n",
    "        0.0000])\n",
    "Batch 7 Computed Rewards: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0741, 0.0000, 0.1176,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
