{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama7B on the squad dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this is to measure the coherence of an LLM on the Cohence of solving general problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import the hugging face transformers library\n",
    "import wandb\n",
    "import torch\n",
    "import os\n",
    "from transformers import  Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "#making sure I am using the gpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Including code to get from token from environment\n",
    "token= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167c81669bfc40a1a3e435812a506392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Squad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = examples['question']\n",
    "    contexts = examples['context']\n",
    "\n",
    "    # Tokenize the questions and contexts\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=30,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Extract the answer text and start positions\n",
    "    answers = examples['answers']\n",
    "    start_positions = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(answers)):\n",
    "        answer = answers[i]\n",
    "        start_position = answer['answer_start'][0]\n",
    "        labels.append(answer['text'][0])\n",
    "        start_positions.append(start_position)\n",
    "\n",
    "    inputs['start_positions'] = start_positions\n",
    "    inputs['labels'] = labels\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "squad_dataset_validation = squad_dataset['validation']\n",
    "squad_dataset_train = squad_dataset['train']\n",
    "\n",
    "# Map preprocessing function to the dataset\n",
    "encoded_train_dataset = squad_dataset_train.map(preprocess_function, batched=True, remove_columns=squad_dataset_train.column_names)\n",
    "encoded_validation_dataset = squad_dataset_validation.map(preprocess_function, batched=True, remove_columns=squad_dataset_validation.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'start_positions', 'labels'],\n",
       "    num_rows: 10570\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1,\n",
       "  8449,\n",
       "  25167,\n",
       "  3815,\n",
       "  9875,\n",
       "  278,\n",
       "  319,\n",
       "  8610,\n",
       "  472,\n",
       "  5670,\n",
       "  27207,\n",
       "  29871,\n",
       "  29945,\n",
       "  29900,\n",
       "  29973,\n",
       "  1,\n",
       "  5670,\n",
       "  27207,\n",
       "  29871,\n",
       "  29945,\n",
       "  29900,\n",
       "  471,\n",
       "  385,\n",
       "  3082,\n",
       "  5733,\n",
       "  3748,\n",
       "  304,\n",
       "  8161,\n",
       "  278,\n",
       "  8064],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'start_positions': 177,\n",
       " 'labels': 'Denver Broncos'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_validation_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Existing performance of LLama7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predicted_answer, true_answer):\n",
    "    predicted_tokens = predicted_answer.lower().split()\n",
    "    true_tokens = true_answer.lower().split()\n",
    "\n",
    "    common_tokens = set(predicted_tokens) & set(true_tokens)\n",
    "    exact_match = int(predicted_tokens == true_tokens)\n",
    "\n",
    "    if len(predicted_tokens) == 0 or len(true_tokens) == 0:\n",
    "        f1_score = int(predicted_tokens == true_tokens)\n",
    "    else:\n",
    "        precision = len(common_tokens) / len(predicted_tokens)\n",
    "        recall = len(common_tokens) / len(true_tokens)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return exact_match, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current value, 1\n",
      "Current value, 2\n",
      "Current value, 3\n",
      "Current value, 4\n",
      "Current value, 5\n",
      "Current value, 6\n",
      "Current value, 7\n",
      "Current value, 8\n",
      "Current value, 9\n",
      "Current value, 10\n",
      "p5\n",
      "Exact Match: 0.0000\n",
      "F1 Score: 0.0287\n"
     ]
    }
   ],
   "source": [
    "#this code below is meant to just check the existing perfomance of llama 7b on the dataset\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, num_samples=100):\n",
    "    exact_match_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    current = 0\n",
    "    for example in dataset[\"validation\"].select(range(num_samples)):\n",
    "        #print(\"p1\")\n",
    "        question = example[\"question\"]\n",
    "        context = example[\"context\"]\n",
    "        true_answer = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "        #print(\"p2\")\n",
    "        input_text = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        #print(\"p3\")\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(input_ids, max_new_tokens=50, num_return_sequences=1)\n",
    "\n",
    "        predicted_answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        #print(\"p4\")\n",
    "        exact_match, f1_score = calculate_metrics(predicted_answer, true_answer)\n",
    "        exact_match_scores.append(exact_match)\n",
    "        f1_scores.append(f1_score)\n",
    "        current+=1\n",
    "        print(\"Current value,\",current)\n",
    "\n",
    "    print(\"p5\")\n",
    "    avg_exact_match = sum(exact_match_scores) / len(exact_match_scores)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "    \n",
    "\n",
    "    return avg_exact_match, avg_f1_score\n",
    "\n",
    "num_samples = 10  # Specify the number of samples to evaluate\n",
    "exact_match, f1_score = evaluate_model(model, tokenizer, squad_dataset, num_samples)\n",
    "print(f\"Exact Match: {exact_match:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_reward(predictions, references, threshold=0.8):\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = ref.split()\n",
    "        common_tokens = set(pred_tokens) & set(ref_tokens)\n",
    "        precision = len(common_tokens) / len(pred_tokens)\n",
    "        recall = len(common_tokens) / len(ref_tokens)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(1 if f1_score >= threshold else 0)\n",
    "    return torch.tensor(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing new version of the dense reward\n",
    "# Define the dense reward function\n",
    "def dense_reward(predictions, references):\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = ref.split()\n",
    "        common_tokens = set(pred_tokens) & set(ref_tokens)\n",
    "        precision = len(common_tokens) / len(pred_tokens)\n",
    "        recall = len(common_tokens) / len(ref_tokens)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1_score)\n",
    "    return torch.tensor(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import wandb\n",
    "\n",
    "def train_model(model, tokenizer, input_train_dataset,input_validation_dataset, reward_function, reward_type):\n",
    "    # Initialize WandB with specific configurations\n",
    "    wandb.init(project=\"Coherence\", name=f\"Training with {reward_type} Reward\")\n",
    "    \n",
    "    # Define the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{reward_type}\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_steps=500,\n",
    "        save_steps=1000,\n",
    "        save_total_limit=2,\n",
    "        seed=42,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"wandb\"\n",
    "    )\n",
    "    \n",
    "    # Create the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=input_train_dataset,\n",
    "        eval_dataset=input_validation_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda pred: {\"reward\": reward_function(pred.predictions, pred.label_ids)}\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Finish the WandB run\n",
    "    wandb.finish()\n",
    "\n",
    "# Example usage\n",
    "train_model(model, tokenizer, encoded_train_dataset, encoded_validation_dataset, dense_reward, \"Dense\")\n",
    "\n",
    "#now train the sparse reward\n",
    "#squad_dataset_validation = squad_dataset['validation']\n",
    "#squad_dataset_train = squad_dataset['train']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating mock versions of both for error checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a single sample from training and validation datasets\n",
    "train_sample = encoded_train_dataset[0]\n",
    "validation_sample = encoded_validation_dataset[0]\n",
    "\n",
    "# Print out the samples to understand their structure\n",
    "print(\"Training Sample:\", train_sample)\n",
    "print(\"Validation Sample:\", validation_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a prediction output from the model, for demonstration let's just reuse the input_ids as predictions\n",
    "simulated_predictions = [train_sample['input_ids'], validation_sample['input_ids']]\n",
    "references = [train_sample['labels'], validation_sample['labels']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predictions_and_references(predictions, references, tokenizer):\n",
    "    # Decode predictions (list of token IDs) to text\n",
    "    decoded_predictions = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]\n",
    "    return decoded_predictions, references  # References are already text\n",
    "\n",
    "# Example usage (assuming some model outputs and references)\n",
    "predictions = [train_sample['input_ids'], validation_sample['input_ids']]  # Simulated model predictions\n",
    "references = [train_sample['labels'], validation_sample['labels']]  # Actual references\n",
    "\n",
    "decoded_predictions, processed_references = process_predictions_and_references(predictions, references, tokenizer)\n",
    "\n",
    "# Now pass the decoded predictions and the text references to the reward function\n",
    "rewards = dense_reward(decoded_predictions, processed_references)\n",
    "print(\"Computed Rewards:\", rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing  not just function but on a batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a DataLoader, you can set batch_size directly.\n",
    "# For datasets library, manually slice the dataset for simplicity\n",
    "batch_size = 10  # Example batch size\n",
    "train_batch = encoded_train_dataset[:batch_size]\n",
    "validation_batch = encoded_validation_dataset[:batch_size]\n",
    "\n",
    "# Extract predictions (simulated here) and references\n",
    "predictions = [item['input_ids'] for item in train_batch]  # Assuming a simple model prediction\n",
    "references = [item['labels'] for item in train_batch]  # Actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_inputs(input_ids, tokenizer):\n",
    "    \"\"\" Decode a batch of input IDs to text. \"\"\"\n",
    "    return [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
    "\n",
    "# Decoding the input IDs for the whole batch\n",
    "decoded_texts = decode_inputs(train_batch['input_ids'], tokenizer)\n",
    "\n",
    "# You already have the labels in the correct format\n",
    "labels = train_batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rewards\n",
    "rewards = dense_reward(decoded_texts, labels)\n",
    "print(\"Batch Computed Rewards:\", rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch, tokenizer):\n",
    "    \"\"\" Process a single batch of data, returning decoded texts and labels. \"\"\"\n",
    "    decoded_texts = decode_inputs(batch['input_ids'], tokenizer)\n",
    "    labels = batch['labels']\n",
    "    return decoded_texts, labels\n",
    "\n",
    "def evaluate_batch(batch, tokenizer):\n",
    "    \"\"\" Evaluate a single batch using the dense_reward function. \"\"\"\n",
    "    decoded_texts, labels = process_batch(batch, tokenizer)\n",
    "    rewards = dense_reward(decoded_texts, labels)\n",
    "    return rewards\n",
    "\n",
    "# Example usage, assuming you're iterating over batches of your dataset\n",
    "for i in range(0, len(encoded_train_dataset), batch_size):\n",
    "    batch = encoded_train_dataset[i:i+batch_size]\n",
    "    batch_rewards = evaluate_batch(batch, tokenizer)\n",
    "    print(f\"Batch {i // batch_size} Computed Rewards:\", batch_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sample: {'input_ids': [1, 1763, 6029, 1258, 278, 9167, 6182, 16831, 23244, 2615, 297, 29871, 29896, 29947, 29945, 1, 2595, 4496, 332, 635, 29892, 278, 3762, 756, 263, 11865, 2931, 29889, 2180, 459], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 515, 'labels': 'Saint Bernadette Soubirous'}\n",
      "Validation Sample: {'input_ids': [1, 8449, 25167, 3815, 9875, 278, 319, 8610, 472, 5670, 27207, 29871, 29945, 29900, 29973, 1, 5670, 27207, 29871, 29945, 29900, 471, 385, 3082, 5733, 3748, 304, 8161, 278, 8064], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start_positions': 177, 'labels': 'Denver Broncos'}\n"
     ]
    }
   ],
   "source": [
    "# Extract a single sample from training and validation datasets\n",
    "train_sample = encoded_train_dataset[0]\n",
    "validation_sample = encoded_validation_dataset[0]\n",
    "\n",
    "# Print out the samples to understand their structure\n",
    "print(\"Training Sample:\", train_sample)\n",
    "print(\"Validation Sample:\", validation_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Rewards: tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "def process_predictions_and_references(predictions, references, tokenizer):\n",
    "    # Decode predictions (list of token IDs) to text\n",
    "    decoded_predictions = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]\n",
    "    return decoded_predictions, references  # References are already text\n",
    "\n",
    "# Example usage (assuming some model outputs and references)\n",
    "predictions = [train_sample['input_ids'], validation_sample['input_ids']]  # Simulated model predictions\n",
    "references = [train_sample['labels'], validation_sample['labels']]  # Actual references\n",
    "\n",
    "decoded_predictions, processed_references = process_predictions_and_references(predictions, references, tokenizer)\n",
    "\n",
    "# Now pass the decoded predictions and the text references to the reward function\n",
    "rewards = dense_reward(decoded_predictions, processed_references)\n",
    "print(\"Computed Rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a DataLoader, you can set batch_size directly.\n",
    "# For datasets library, manually slice the dataset for simplicity\n",
    "batch_size = 10  # Example batch size\n",
    "train_batch = encoded_train_dataset[:batch_size]\n",
    "validation_batch = encoded_validation_dataset[:batch_size]\n",
    "\n",
    "# Extract predictions (simulated here) and references\n",
    "#predictions = [item['input_ids'] for item in train_batch]  # Assuming a simple model prediction\n",
    "#references = [item['labels'] for item in train_batch]  # Actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train_batch is a dictionary with keys 'input_ids' and 'labels' each mapping to a list\n",
    "predictions = train_batch['input_ids']  # Directly access the list of input_ids\n",
    "references = train_batch['labels']      # Directly access the list of labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of handling batches correctly\n",
    "#batch_size = 10\n",
    "for i in range(0, len(encoded_train_dataset), batch_size):\n",
    "    # Ensure that the slicing returns the correct format\n",
    "    batch = encoded_train_dataset[i:i+batch_size]\n",
    "    if isinstance(batch, dict):\n",
    "        predictions = batch['input_ids']\n",
    "        references = batch['labels']\n",
    "    else:\n",
    "        predictions = [item['input_ids'] for item in batch]\n",
    "        references = [item['labels'] for item in batch]\n",
    "\n",
    "    # Continue processing as before\n",
    "    decoded_predictions, processed_references = process_predictions_and_references(predictions, references, tokenizer)\n",
    "    batch_rewards = dense_reward(decoded_predictions, processed_references)\n",
    "    print(f\"Batch {i//batch_size} Computed Rewards:\", batch_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch 0 Computed Rewards: tensor([0.0000, 0.1600, 0.0952, 0.0800, 0.2308, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "        0.0000])\n",
    "Batch 1 Computed Rewards: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0833, 0.0000, 0.0000, 0.2222, 0.0000,\n",
    "        0.0000])\n",
    "Batch 2 Computed Rewards: tensor([0.0000, 0.0000, 0.0833, 0.0000, 0.0000, 0.1600, 0.0714, 0.0000, 0.0690,\n",
    "        0.5161])\n",
    "Batch 3 Computed Rewards: tensor([0.2759, 0.0000, 0.0833, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "        0.0000])\n",
    "Batch 4 Computed Rewards: tensor([0.0000, 0.1290, 0.0000, 0.0741, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "        0.3333])\n",
    "Batch 5 Computed Rewards: tensor([0.0000, 0.0000, 0.0000, 0.0909, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "        0.2609])\n",
    "Batch 6 Computed Rewards: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.3810, 0.0000, 0.0000, 0.2609, 0.0000,\n",
    "        0.0000])\n",
    "Batch 7 Computed Rewards: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0741, 0.0000, 0.1176,\n",
    "        0.0000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the reward function to handle tokens instead of text.\n",
    "def dense_reward(predictions, references, tokenizer):\n",
    "    f1_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # Decode the tokens to strings\n",
    "        pred_text = tokenizer.decode(pred, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        ref_text = tokenizer.decode(ref, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        \n",
    "        pred_tokens = pred_text.split()\n",
    "        ref_tokens = ref_text.split()\n",
    "        common_tokens = set(pred_tokens) & set(ref_tokens)\n",
    "        precision = len(common_tokens) / len(pred_tokens) if len(pred_tokens) > 0 else 0\n",
    "        recall = len(common_tokens) / len(ref_tokens) if len(ref_tokens) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1_score)\n",
    "    return torch.tensor(f1_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting from the top\n",
    "#we have the dense function, so lets work backwards, and delete all else\n",
    "def process_batch(batch, tokenizer):\n",
    "    \"\"\" Process a single batch of data, returning decoded texts and labels. \"\"\"\n",
    "    decoded_texts = decode_inputs(batch['input_ids'], tokenizer)\n",
    "    labels = batch['labels']\n",
    "    return decoded_texts, labels\n",
    "\n",
    "def evaluate_batch(batch, tokenizer):\n",
    "    \"\"\" Evaluate a single batch using the dense_reward function. \"\"\"\n",
    "    decoded_texts, labels = process_batch(batch, tokenizer)\n",
    "    rewards = dense_reward(decoded_texts, labels)\n",
    "    return rewards\n",
    "\n",
    "# Example usage, assuming you're iterating over batches of your dataset\n",
    "for i in range(0, len(encoded_train_dataset), batch_size):\n",
    "    batch = encoded_train_dataset[i:i+batch_size]\n",
    "    batch_rewards = evaluate_batch(batch, tokenizer)\n",
    "    print(f\"Batch {i // batch_size} Computed Rewards:\", batch_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
