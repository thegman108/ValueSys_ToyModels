When reward defined over transitions (with shape (A, S, S)), 10^5 data points, (S, A) = (10, 4)

Only optimal policy passed in
Epoch 1/100
2500/2500 [==============================] - 27s 9ms/step - loss: 0.0832 - mae: 0.2476 - val_loss: 0.0768 - val_mae: 0.2407
Epoch 2/100
2500/2500 [==============================] - 19s 8ms/step - loss: 0.0781 - mae: 0.2407 - val_loss: 0.0768 - val_mae: 0.2397
Epoch 3/100
2500/2500 [==============================] - 14s 6ms/step - loss: 0.0771 - mae: 0.2391 - val_loss: 0.0768 - val_mae: 0.2379
Epoch 4/100
2500/2500 [==============================] - 13s 5ms/step - loss: 0.0766 - mae: 0.2383 - val_loss: 0.0765 - val_mae: 0.2385
Epoch 5/100
2500/2500 [==============================] - 12s 5ms/step - loss: 0.0762 - mae: 0.2378 - val_loss: 0.0767 - val_mae: 0.2403
Epoch 6/100
2500/2500 [==============================] - 14s 5ms/step - loss: 0.0763 - mae: 0.2378 - val_loss: 0.0767 - val_mae: 0.2409
Epoch 7/100
2500/2500 [==============================] - 15s 6ms/step - loss: 0.0763 - mae: 0.2378 - val_loss: 0.0764 - val_mae: 0.2388
Epoch 8/100
2500/2500 [==============================] - 14s 5ms/step - loss: 0.0762 - mae: 0.2376 - val_loss: 0.0766 - val_mae: 0.2394
Epoch 9/100
2500/2500 [==============================] - 15s 6ms/step - loss: 0.0761 - mae: 0.2374 - val_loss: 0.0762 - val_mae: 0.2381
Epoch 10/100
2500/2500 [==============================] - 14s 5ms/step - loss: 0.0760 - mae: 0.2374 - val_loss: 0.0766 - val_mae: 0.2402
Epoch 11/100
2500/2500 [==============================] - 14s 5ms/step - loss: 0.0761 - mae: 0.2374 - val_loss: 0.0765 - val_mae: 0.2382
Epoch 12/100
2500/2500 [==============================] - 14s 6ms/step - loss: 0.0761 - mae: 0.2372 - val_loss: 0.0761 - val_mae: 0.2379
Epoch 13/100
2500/2500 [==============================] - 20s 8ms/step - loss: 0.0759 - mae: 0.2369 - val_loss: 0.0761 - val_mae: 0.2373
Epoch 14/100
2500/2500 [==============================] - 20s 8ms/step - loss: 0.0759 - mae: 0.2370 - val_loss: 0.0767 - val_mae: 0.2410
Epoch 15/100
2500/2500 [==============================] - 16s 6ms/step - loss: 0.0760 - mae: 0.2371 - val_loss: 0.0766 - val_mae: 0.2407
Mean squared error: 0.07612750121718719, sample size: 1000
Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...

Linear Regression
Mean squared error: 0.07935751006037296
Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...
Mean absolute error: 0.2450860745036738


When all features passed in
Epoch 1/100
2500/2500 [==============================] - 19s 6ms/step - loss: 0.0815 - mae: 0.2453 - val_loss: 0.0782 - val_mae: 0.2436
Epoch 2/100
2500/2500 [==============================] - 16s 6ms/step - loss: 0.0783 - mae: 0.2415 - val_loss: 0.0772 - val_mae: 0.2411
Epoch 3/100
2500/2500 [==============================] - 15s 6ms/step - loss: 0.0775 - mae: 0.2403 - val_loss: 0.0794 - val_mae: 0.2455
Epoch 4/100
2500/2500 [==============================] - 14s 6ms/step - loss: 0.0771 - mae: 0.2395 - val_loss: 0.0776 - val_mae: 0.2392
Epoch 5/100
2500/2500 [==============================] - 14s 6ms/step - loss: 0.0769 - mae: 0.2394 - val_loss: 0.0777 - val_mae: 0.2421
Mean squared error: 0.0782953013071, sample size: 1000
Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...

Linear Regression
Mean squared error: 0.0783458402881269
Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...
Mean absolute error: 0.24266147317553713


With deterministic transition matrix, performance preserved
With 16 actions, 10^5 training points (10^4 does slightly worse)
Epoch 1/100
2500/2500 [==============================] - 20s 7ms/step - loss: 0.0652 - mae: 0.2128 - val_loss: 0.0486 - val_mae: 0.1810
Epoch 2/100
2500/2500 [==============================] - 15s 6ms/step - loss: 0.0514 - mae: 0.1870 - val_loss: 0.0473 - val_mae: 0.1763
Epoch 3/100
2500/2500 [==============================] - 10s 4ms/step - loss: 0.0493 - mae: 0.1826 - val_loss: 0.0472 - val_mae: 0.1759
Epoch 4/100
2500/2500 [==============================] - 11s 5ms/step - loss: 0.0488 - mae: 0.1814 - val_loss: 0.0472 - val_mae: 0.1753
Epoch 5/100
2500/2500 [==============================] - 12s 5ms/step - loss: 0.0482 - mae: 0.1800 - val_loss: 0.0475 - val_mae: 0.1786
Epoch 6/100
2500/2500 [==============================] - 9s 4ms/step - loss: 0.0477 - mae: 0.1788 - val_loss: 0.0477 - val_mae: 0.1776
Epoch 7/100
2500/2500 [==============================] - 10s 4ms/step - loss: 0.0471 - mae: 0.1777 - val_loss: 0.0481 - val_mae: 0.1779
[(0.73825, 0.44508514286306944, 0.44508514286306944), (0.65543, 0.3952783266236025, 0.3952783266236025), (0.3658, 0.46309541504893603, 0.46309541504893603), (0.36075, 0.47211796576524034, 0.47211796576524034), (0.42836, 0.43105311376033173, 0.43105311376033173), (0.96038, 0.8529713978016608, 0.8529713978016609), (0.81055, 0.696313413743158, 0.696313413743158), (0.51637, 0.43132312499250025, 0.43132312499250025), (0.87031, 0.34844483283283867, 0.34844483283283867), (0.88765, 0.5992918707244187, 0.5992918707244188)]
Linear regression:
Mean squared error: 0.048591395976632694
Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...
Mean absolute error: 0.18633074687872414

Weights:
(array([-0.06574903, -0.08303599, -0.08436633, -0.07808955, -0.07744396,
        -0.08518641, -0.08047801, -0.08990955, -0.08051643, -0.08249246,
        -0.08544941, -0.08102032, -0.0881371 , -0.0807451 , -0.08402631,
        -0.06385256, -0.08458032, -0.08424711, -0.08322617, -0.07847148,
        -0.08341254, -0.08195791, -0.08338952, -0.07740678, -0.08868839,
        -0.08504517, -0.08514746, -0.08355296, -0.08428215, -0.0863508 ,
        -0.06363862, -0.08378852, -0.08726936, -0.08834432, -0.08740516,
        -0.09244201, -0.09107439, -0.0824844 , -0.08814008, -0.08482285,
        -0.08876019, -0.08370216, -0.09321108, -0.08648383, -0.09375658,
        -0.06417106, -0.08440805, -0.08467959, -0.07908989, -0.08302193,
        -0.09453912, -0.08945731, -0.08687802, -0.08619124, -0.08436389,
        -0.0876338 , -0.08215865, -0.08919921, -0.09044435, -0.08894526,
        -0.06402421, -0.07814204, -0.07633805, -0.07613983, -0.08182746,
        -0.07857419, -0.08385825, -0.08322773, -0.08244658, -0.08660025,
        -0.08205454, -0.0855542 , -0.0847805 , -0.08893219, -0.08208526,
        -0.06675227, -0.08878255, -0.08621012, -0.08416927, -0.08633911,
        -0.08103574, -0.08146301, -0.08413644, -0.08264337, -0.08646265,
        -0.08673196, -0.08231409, -0.08518071, -0.08948276, -0.07973583,
        -0.06404418, -0.07873396, -0.08924694, -0.09304745, -0.08348928,
        -0.08976443, -0.09496496, -0.08978228, -0.08622818, -0.08574823,
        -0.08770492, -0.08007103, -0.09259682, -0.08600893, -0.07992082,
        -0.0617386 , -0.07410685, -0.07675202, -0.08432174, -0.08642944,
        -0.08257405, -0.08025118, -0.08658942, -0.08363527, -0.08749568,
        -0.07997528, -0.0813015 , -0.08144947, -0.08507453, -0.08399502,
        -0.06687565, -0.08217396, -0.09229583, -0.08901732, -0.08643384,
        -0.09598532, -0.07973654, -0.08757874, -0.08674725, -0.08858206,
        -0.08445573, -0.08621589, -0.08973458, -0.08187655, -0.09255736,
        -0.063179  , -0.07313623, -0.0842911 , -0.08763712, -0.08499439,
        -0.07726641, -0.08570926, -0.08621754, -0.08110944, -0.08556954,
        -0.08773972, -0.08543954, -0.08262758, -0.08388381, -0.08787491]),
 1.195629946749378)

With two actions:
Epoch 1/100
250/250 [==============================] - 5s 9ms/step - loss: 0.0743 - mae: 0.2183 - val_loss: 0.0438 - val_mae: 0.1716
Epoch 2/100
250/250 [==============================] - 2s 7ms/step - loss: 0.0493 - mae: 0.1814 - val_loss: 0.0411 - val_mae: 0.1657
Epoch 3/100
250/250 [==============================] - 1s 4ms/step - loss: 0.0467 - mae: 0.1762 - val_loss: 0.0410 - val_mae: 0.1655
Epoch 4/100
250/250 [==============================] - 1s 5ms/step - loss: 0.0456 - mae: 0.1734 - val_loss: 0.0405 - val_mae: 0.1626
Epoch 5/100
250/250 [==============================] - 2s 6ms/step - loss: 0.0442 - mae: 0.1710 - val_loss: 0.0407 - val_mae: 0.1628
Epoch 6/100
250/250 [==============================] - 2s 6ms/step - loss: 0.0437 - mae: 0.1703 - val_loss: 0.0404 - val_mae: 0.1628
Epoch 7/100
250/250 [==============================] - 2s 7ms/step - loss: 0.0439 - mae: 0.1700 - val_loss: 0.0402 - val_mae: 0.1627
Epoch 8/100
250/250 [==============================] - 2s 7ms/step - loss: 0.0429 - mae: 0.1685 - val_loss: 0.0408 - val_mae: 0.1647
Epoch 9/100
250/250 [==============================] - 1s 6ms/step - loss: 0.0428 - mae: 0.1681 - val_loss: 0.0406 - val_mae: 0.1645
Epoch 10/100
250/250 [==============================] - 2s 7ms/step - loss: 0.0428 - mae: 0.1677 - val_loss: 0.0406 - val_mae: 0.1632
[(0.0825, 0.41707045953693833, 0.41707045953693833), (0.3567, 0.3335441523332709, 0.33354415233327084), (0.329, 0.6345000880547232, 0.6345000880547232), (0.2771, 0.44370317656207814, 0.44370317656207814), (0.8008, 0.7311555937875527, 0.7311555937875527), (0.751, 0.5354042823336345, 0.5354042823336345), (0.9927, 0.8255557347709064, 0.8255557347709064), (0.7051, 0.6276529976197105, 0.6276529976197105), (0.2624, 0.4500415408205286, 0.4500415408205286), (0.6681, 0.5311781447607388, 0.5311781447607388)]
Linear regression:
Mean squared error: 0.04198626343380414
Expected squared error: when x, y ~ U[0, 1], E[(x-y)^2] = 1/12 = 0.0833...
Mean absolute error: 0.16770369024582232

(array([-0.10301682, -0.10901775, -0.10088164, -0.09686912, -0.1035026 ,
        -0.09440014, -0.09199622, -0.08755305, -0.09909581, -0.09389141]),
 0.8255557347709064)

 With three actions, MSE ~ 0.032-0.034; 