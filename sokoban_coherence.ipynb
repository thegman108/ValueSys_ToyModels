{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup:\n",
    "- Starting point: just try to train classifier on RL policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DQN implementation\n",
    "\n",
    "# Define the Q-network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "\n",
    "# Experience Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=1e-3, batch_size=64, gamma=0.99, replay_size=1000):\n",
    "        self.model = DQN(state_dim, action_dim, hidden_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.replay_buffer = ReplayBuffer(replay_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        state = torch.FloatTensor(state)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        action = torch.LongTensor(action)\n",
    "        reward = torch.FloatTensor(reward)\n",
    "        done = torch.FloatTensor(done)\n",
    "\n",
    "        q_values = self.model(state)\n",
    "        next_q_values = self.model(next_state)\n",
    "        \n",
    "        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        next_q_value = next_q_values.max(1)[0]\n",
    "        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "        \n",
    "        loss = nn.MSELoss()(q_value, expected_q_value.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.FloatTensor(np.expand_dims(state, 0))\n",
    "            q_value = self.model(state)\n",
    "            action = q_value.max(1)[1].item()\n",
    "        else:\n",
    "            action = random.randrange(self.action_dim)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NON_ZERO_REWARDS = 0\n",
    "def train_dqn(env_name=\"CartPole-v1\", episodes=500, epsilon_start=1.0, epsilon_final=0.01, \n",
    "              epsilon_decay=500, reward_function = None):\n",
    "    global NUM_NON_ZERO_REWARDS\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "    \n",
    "    epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            epsilon = epsilon_by_frame(episode)\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if reward_function:\n",
    "                reward = reward_function(state, action, next_state, done)\n",
    "            NUM_NON_ZERO_REWARDS += 0 if math.isclose(reward, 0) else 1\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            agent.update()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            # print(f\"Episode: {episode+1}, Total reward: {episode_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "        # Optional: Render the environment to visualize training progress\n",
    "        # if episode % 100 == 0:\n",
    "        #     render_env(env, agent)\n",
    "\n",
    "    env.close()\n",
    "    return agent\n",
    "\n",
    "# Optional: Function to render the environment with the current policy\n",
    "def render_env(env, agent):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state, 0)  # Using 0 epsilon for greedy action selection\n",
    "        # print(env.step(action))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dqn(env, agent, episodes=10):\n",
    "    print(f\"Maximum reward: {env.spec.reward_threshold}\")\n",
    "    for episode in range(episodes):\n",
    "        # if episode == 0:\n",
    "        #     render_env(env, agent)\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state, 0)  # Using 0 epsilon for greedy action selection\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        print(f\"Episode: {episode+1}, Total reward: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "agent = train_dqn(env_name = env_name, episodes = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum reward: 475.0\n",
      "Episode: 1, Total reward: 172.0\n",
      "Episode: 2, Total reward: 166.0\n",
      "Episode: 3, Total reward: 228.0\n",
      "Episode: 4, Total reward: 157.0\n",
      "Episode: 5, Total reward: 208.0\n",
      "Episode: 6, Total reward: 298.0\n",
      "Episode: 7, Total reward: 500.0\n",
      "Episode: 8, Total reward: 280.0\n",
      "Episode: 9, Total reward: 500.0\n",
      "Episode: 10, Total reward: 500.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc.0.weight',\n",
       "              tensor([[ 2.2510e+00, -3.3805e-01, -3.9947e+00, -7.4218e-01],\n",
       "                      [ 5.8213e-01, -5.0631e-01, -6.0192e+00, -1.4179e+00],\n",
       "                      [-6.2113e-02,  9.3656e-01, -4.7122e-01, -9.8996e-01],\n",
       "                      [ 7.4555e-01, -6.4955e-01, -1.5013e+00, -1.8160e-01],\n",
       "                      [-2.5369e-01,  7.5536e-01,  5.6088e+00,  1.0261e+00],\n",
       "                      [-5.4400e-01,  7.0318e-02, -6.0223e-01, -5.3441e-01],\n",
       "                      [ 1.6757e-01,  1.7796e-01, -8.1427e-01,  7.7093e-02],\n",
       "                      [ 3.9939e-01, -4.9977e-01, -6.1514e+00, -1.2189e+00],\n",
       "                      [ 1.4535e-01, -1.1467e-01, -9.4931e-01, -5.1248e-01],\n",
       "                      [-3.6525e-01, -5.6429e-01, -5.0702e-01, -4.6492e-01],\n",
       "                      [-3.2165e-01,  3.9664e-01, -4.8892e-02, -6.2249e-02],\n",
       "                      [-4.0101e-01,  4.6125e-02, -7.8735e-01, -5.1730e-01],\n",
       "                      [ 5.0324e-01, -6.5878e-01, -4.7073e+00, -1.5436e+00],\n",
       "                      [ 2.6723e-01,  2.1313e-01,  1.8267e-01,  2.6323e-01],\n",
       "                      [-3.2867e-01,  2.5135e-01, -3.6757e-01, -3.9015e-01],\n",
       "                      [ 3.2100e-01, -7.2974e-02, -7.7022e-01,  1.3912e-01],\n",
       "                      [ 5.2695e-01, -6.9208e-01, -4.2445e+00, -1.5606e+00],\n",
       "                      [ 7.1209e-01, -4.2556e-01, -2.0720e+00, -3.9241e-01],\n",
       "                      [ 8.1840e-02,  3.0718e-01,  5.6890e+00,  1.1474e+00],\n",
       "                      [-6.4407e-01,  7.2919e-01,  4.9907e+00,  9.4502e-01],\n",
       "                      [-1.0339e+00,  1.1468e-01,  2.8049e+00,  3.0610e-01],\n",
       "                      [-4.9432e-02,  6.3754e-03, -2.7248e-01, -2.1919e-01],\n",
       "                      [-4.7996e-01, -1.0754e-01, -5.9714e-01, -8.7545e-02],\n",
       "                      [-4.1981e-01, -1.1482e+00, -5.0795e+00, -9.3827e-01],\n",
       "                      [ 1.5835e-01,  2.5817e-01, -2.2193e-01, -3.9081e-01],\n",
       "                      [ 1.2033e+00,  6.3565e-01,  3.8708e+00,  9.3271e-01],\n",
       "                      [ 4.0208e-01, -2.1810e-01, -7.1806e-01,  1.0546e-01],\n",
       "                      [ 4.9879e-01, -4.0626e-01, -4.5926e-01, -8.9824e-02],\n",
       "                      [-5.3591e-01,  8.2826e-01,  5.9659e+00,  1.1191e+00],\n",
       "                      [-3.2507e-01,  1.1448e-01, -1.3320e+00, -1.9368e-02],\n",
       "                      [-2.0224e-01,  3.2755e-01,  3.4461e-01, -3.1578e-01],\n",
       "                      [ 1.0322e-01,  3.0140e-02, -8.1962e-02,  1.2763e-01],\n",
       "                      [-8.0888e-01, -4.5966e-01, -3.7103e+00, -7.6873e-01],\n",
       "                      [-4.5163e-01,  4.6563e-01, -7.8235e-01, -1.1344e-01],\n",
       "                      [ 1.8296e-01, -4.1090e-01, -5.3841e+00, -1.2334e+00],\n",
       "                      [ 2.7105e-01, -6.6368e-02, -3.1787e-01, -2.2678e-03],\n",
       "                      [-6.5293e-04,  4.0824e-01,  5.4431e+00,  1.0448e+00],\n",
       "                      [-8.4557e-02, -1.6030e-01, -9.0797e-02, -1.9297e-01],\n",
       "                      [ 2.1587e-01, -7.6168e-03, -2.1943e-01, -1.8248e-01],\n",
       "                      [ 2.0250e+00,  6.8805e-02, -1.1322e+00,  3.5459e-02],\n",
       "                      [ 7.8734e-01, -6.7281e-01, -5.9181e+00, -1.1003e+00],\n",
       "                      [-9.9049e-01,  3.8888e-01,  1.5482e+00, -2.8499e-01],\n",
       "                      [-1.7819e-01,  3.9126e-01, -3.7795e-01,  4.6719e-02],\n",
       "                      [-4.7414e-01, -5.6803e-03, -4.5100e-01, -9.5490e-02],\n",
       "                      [ 1.9466e-01, -3.0889e-01, -5.6593e-01,  1.1276e-01],\n",
       "                      [-5.1662e-01, -1.2108e-01, -6.3357e-01, -2.0700e-01],\n",
       "                      [ 3.2427e-01,  1.0363e-01, -5.8762e-01,  2.9425e-01],\n",
       "                      [ 1.3895e-01,  3.5645e-01, -6.1079e-01,  1.3901e-01],\n",
       "                      [ 1.7917e-01, -2.5233e-01, -9.1771e-01, -1.7932e-01],\n",
       "                      [-4.2302e-01, -6.2619e-02, -3.2190e-01,  5.9384e-02],\n",
       "                      [ 7.1020e-03, -1.5082e-01, -6.8265e-01, -7.8172e-02],\n",
       "                      [-5.3078e-01,  1.6062e+00,  4.2771e+00,  1.2549e+00],\n",
       "                      [ 1.7341e-01, -8.6100e-02, -7.9181e-01, -4.1964e-01],\n",
       "                      [-1.0816e+00, -3.5574e-01, -3.7913e+00, -7.4092e-01],\n",
       "                      [-7.8244e-01,  2.5147e-01,  1.7338e+00, -4.2211e-01],\n",
       "                      [-2.5430e-01,  3.7910e-01, -7.0430e-01,  2.4808e-02],\n",
       "                      [-6.6955e-01,  4.3360e-01, -1.0049e+00,  4.3725e-02],\n",
       "                      [ 3.4530e-01, -4.5963e-01, -8.5962e-01, -4.2398e-01],\n",
       "                      [-2.5036e-01, -9.5898e-02, -9.8382e-01, -5.6606e-01],\n",
       "                      [ 1.8008e+00,  6.3381e-01,  4.1918e+00,  7.1855e-01],\n",
       "                      [ 9.4846e-02,  4.4243e-01, -1.1112e-01,  2.4188e-01],\n",
       "                      [-4.8546e-01, -4.7467e-02, -5.6967e-01, -5.3593e-01],\n",
       "                      [ 1.7948e-01, -7.9031e-01, -2.7354e+00, -4.3086e-02],\n",
       "                      [-1.5878e-01, -1.2567e-01, -3.5674e-01, -5.8180e-01]])),\n",
       "             ('fc.0.bias',\n",
       "              tensor([ 0.4810, -1.1483,  0.9300,  0.4781, -1.6336,  1.1459,  1.1260, -1.4227,\n",
       "                       1.0194,  1.2119,  1.3816,  1.0397,  1.8526,  1.4006,  1.2343,  1.2087,\n",
       "                       1.7953,  0.0718, -0.9690,  1.4333, -0.6716,  0.9496,  0.9718, -0.2920,\n",
       "                       0.8151,  1.6398,  1.2602,  1.1847,  1.7703,  0.8575,  1.1466,  1.3895,\n",
       "                       1.5153,  1.0275,  1.8063,  1.0897, -1.3859,  1.2476,  1.1224, -0.3471,\n",
       "                      -1.4527,  0.6957,  0.9537,  1.2968,  1.3489,  0.7847,  1.1981,  1.2496,\n",
       "                       1.3099,  0.9945,  1.1440, -2.0213,  1.1752, -0.8044,  0.4776,  1.3470,\n",
       "                       0.8601,  0.9672,  1.2965,  0.5258,  1.3024,  0.8764,  0.3650,  1.0009])),\n",
       "             ('fc.2.weight',\n",
       "              tensor([[-2.1553, -3.0825,  0.2315,  ...,  0.5024, -0.2811,  0.6408],\n",
       "                      [ 0.0721, -0.0993, -0.0086,  ..., -0.1404, -0.1244,  0.0795],\n",
       "                      [ 1.5239,  3.9251,  0.2681,  ...,  0.8434, -4.3724,  1.7586],\n",
       "                      ...,\n",
       "                      [ 0.0695, -0.0106,  0.0442,  ..., -0.0573, -0.1035, -0.0913],\n",
       "                      [-1.7479, -1.5242,  0.2781,  ...,  0.4858, -1.3982,  0.9959],\n",
       "                      [-2.3930, -3.0280, -0.2004,  ...,  0.3402, -0.5951,  0.5852]])),\n",
       "             ('fc.2.bias',\n",
       "              tensor([ 0.7488,  0.0620,  1.3470, -0.2033,  0.5387,  0.7193, -0.1006,  1.4611,\n",
       "                      -0.1460, -0.0688,  0.7977,  0.5882,  0.6854,  0.8105,  1.2319,  0.6185,\n",
       "                      -0.1014,  0.8125, -0.0362,  0.5559,  0.8732,  1.2868,  0.8868,  0.9132,\n",
       "                       0.5417,  0.8492,  0.6847, -0.0890,  0.0773,  0.0783,  0.4830, -0.0415,\n",
       "                      -0.0176,  0.4846,  0.3892,  0.0130, -0.1040, -0.0410,  0.4144,  1.1211,\n",
       "                       1.1263, -0.1246,  0.7328, -0.1389, -0.0485, -0.0382,  0.3657,  0.6647,\n",
       "                       0.9141,  0.5585,  0.0869,  0.3832,  0.3616,  0.0877,  1.1636,  0.7288,\n",
       "                      -0.0453,  0.8758, -0.1120,  0.6490,  0.5425, -0.0494,  0.8102,  0.4721])),\n",
       "             ('fc.4.weight',\n",
       "              tensor([[ 3.0031e+00,  2.3517e-02,  1.1850e+00,  1.1406e-01, -2.6955e+00,\n",
       "                        3.4133e+00, -1.1562e-01,  3.0281e+00, -6.4388e-02, -1.1403e-02,\n",
       "                       -2.8202e+00,  3.0580e+00,  3.4378e+00,  3.3153e+00,  3.1021e+00,\n",
       "                        3.5398e+00,  4.0780e-02, -2.8309e+00, -4.1655e-02,  3.3107e+00,\n",
       "                        2.8733e+00,  2.8156e+00,  3.1195e+00,  2.9693e+00,  3.3092e+00,\n",
       "                        3.3518e+00,  3.0616e+00,  2.0457e-02,  5.9352e-02, -2.9587e-02,\n",
       "                        3.1515e+00,  4.3665e-02, -5.7994e-02,  3.2055e+00,  3.2402e+00,\n",
       "                        3.5333e-02,  6.1257e-04, -4.0501e-02,  3.4229e+00,  2.9560e+00,\n",
       "                        2.8656e+00,  5.4235e-02, -2.7553e+00,  2.7126e-02, -9.6331e-02,\n",
       "                       -2.8211e-02,  3.0847e+00,  2.8876e+00,  3.0242e+00,  3.3136e+00,\n",
       "                        1.7159e-02,  3.2416e+00,  3.2360e+00, -9.3706e-02,  3.0161e+00,\n",
       "                        3.1996e+00, -8.6419e-02,  3.0186e+00,  2.6853e-02, -1.8475e+00,\n",
       "                        3.3530e+00,  1.1403e-03,  3.1807e+00,  3.1294e+00],\n",
       "                      [ 2.4857e+00, -6.4193e-02, -4.0508e+00, -6.1730e-02, -9.9492e-01,\n",
       "                        2.6050e+00, -1.1408e-01,  1.8973e+00,  5.5973e-02,  3.6317e-03,\n",
       "                       -3.3626e-01,  2.5869e+00,  2.6904e+00,  2.5484e+00,  2.1485e+00,\n",
       "                        2.6077e+00,  8.7139e-02,  5.7310e-01,  1.3751e-02,  3.3312e+00,\n",
       "                        2.1906e+00,  1.8443e+00,  2.7541e+00,  2.4813e+00,  3.3208e+00,\n",
       "                        2.8007e+00,  2.5195e+00, -5.5298e-02,  4.1968e-02, -1.8835e-02,\n",
       "                        3.2965e+00, -2.4486e-02, -1.2368e-01,  2.7147e+00,  3.5077e+00,\n",
       "                       -7.6279e-02,  3.0009e-02,  6.3397e-02,  3.3199e+00,  2.2157e+00,\n",
       "                        2.2276e+00,  3.2165e-02, -4.7503e-01, -9.8713e-02, -6.0373e-02,\n",
       "                       -8.0760e-02,  2.5934e+00,  2.4359e+00,  2.3821e+00,  2.6390e+00,\n",
       "                        1.0142e-01,  3.2263e+00,  3.1176e+00, -3.9365e-02,  2.2842e+00,\n",
       "                        2.5609e+00,  1.0772e-02,  2.6535e+00, -1.9417e-02,  3.9054e+00,\n",
       "                        2.9522e+00,  1.5025e-02,  1.1960e+00,  2.9828e+00]])),\n",
       "             ('fc.4.bias', tensor([ 1.7846, -0.0622]))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dqn(gym.make(\"CartPole-v1\"), agent)\n",
    "agent.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Coherence classifier\n",
    "\n",
    "#agent.model.get_weights()\n",
    "\n",
    "# Define a simple GCN model\n",
    "from torch_geometric.data import Data\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super(GCN, self).__init__()\n",
    "        # Define the GCN layers\n",
    "        self.conv1 = GCNConv(data.num_node_features, 4)  # Input features to hidden\n",
    "        self.conv2 = GCNConv(4, 2)  # Hidden to output features\n",
    "        self.data = data\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # Pass data through the first GCN layer, then apply ReLU\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        # Pass data through the second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def nn_to_data(model: nn.Module) -> Data:\n",
    "    edges = []\n",
    "    node_features = []\n",
    "\n",
    "    # Counter for global neuron index\n",
    "    global_neuron_index = 0\n",
    "\n",
    "    # Iterate over each layer in the network\n",
    "    base = next(model.children())\n",
    "    if isinstance(base, nn.Sequential):\n",
    "        layers = base.children()\n",
    "    else:\n",
    "        layers = model.children()\n",
    "\n",
    "    if isinstance(base, nn.Sequential):\n",
    "        input_dim = base[0].in_features\n",
    "        node_features = np.zeros(input_dim)\n",
    "\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # Update edges based on the weight matrix\n",
    "            for i in range(layer.weight.shape[1]):  # Input neurons\n",
    "                for j in range(layer.weight.shape[0]):  # Output neurons\n",
    "                    edges.append((global_neuron_index + i, global_neuron_index + layer.weight.shape[1] + j))\n",
    "            \n",
    "            # Update node features (e.g., biases)\n",
    "            extension = layer.bias.detach().numpy()\n",
    "            node_features = np.append(node_features, extension)\n",
    "            \n",
    "            # Update the global neuron index\n",
    "            global_neuron_index += layer.weight.shape[1]\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    x = torch.tensor(node_features, dtype=torch.float).view(-1, 1)\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "data = nn_to_data(agent.model)\n",
    "gcn = GCN(data)\n",
    "# data.x.shape, data.edge_index.shape\n",
    "# print(data.x)\n",
    "\n",
    "#Debug\n",
    "out_of_bounds = data.edge_index >= data.x.shape[0]\n",
    "if out_of_bounds.any():\n",
    "    print(\"Out-of-bounds indices found at locations:\")\n",
    "    print(data.edge_index[:, out_of_bounds.any(dim=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reward function calls: 58866\n",
      "Number of non-zero rewards: 58866\n"
     ]
    }
   ],
   "source": [
    "# Dataset generation\n",
    "env = gym.make(env_name)\n",
    "NEAR_ZERO = 1e-9\n",
    "NUM_REWARD_CALLS = 0\n",
    "NUM_NON_ZERO_REWARDS = 0\n",
    "def deterministic_random(*args, lb = -1, ub = 1, sparsity = 0.0):\n",
    "    global NUM_REWARD_CALLS\n",
    "    NUM_REWARD_CALLS += 1\n",
    "    unique_seed = f\"{args}\".encode(\"utf-8\")\n",
    "    random.seed(unique_seed)\n",
    "    return random.uniform(lb, ub) if random.random() > sparsity else random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "\n",
    "NUM_TRAIN_R_FUNCS = 50\n",
    "NUM_EPS_TRAIN_R = 50\n",
    "URS_r_funcs = [lambda s, a, ns, d: deterministic_random(s, a, ns, d) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "URS_agents = [train_dqn(env_name = env_name, \n",
    "                        episodes=NUM_EPS_TRAIN_R, reward_function=r_func) for r_func in URS_r_funcs]\n",
    "USS_r_funcs = [lambda s, a, ns, d: deterministic_random(s, a, ns, d, sparsity=0.99) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "print(f\"Number of reward function calls: {NUM_REWARD_CALLS}\")\n",
    "print(f\"Number of non-zero rewards: {NUM_NON_ZERO_REWARDS}\")\n",
    "USS_agents = [train_dqn(env_name = env_name, \n",
    "                        episodes=NUM_EPS_TRAIN_R, reward_function=r_func) for r_func in USS_r_funcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([134, 1])\n",
      "Epoch 1: Average Train Loss: 0.6972218137234449, Average Test Loss: 0.7209133118391037\n",
      "Epoch 2: Average Train Loss: 0.6848836325109005, Average Test Loss: 0.6991690650582314\n",
      "Epoch 3: Average Train Loss: 0.6643256038427353, Average Test Loss: 0.6459048129618168\n",
      "Epoch 4: Average Train Loss: 0.6055300385749434, Average Test Loss: 0.5488033395260572\n",
      "Epoch 5: Average Train Loss: 0.5094857397722083, Average Test Loss: 0.40308779150946067\n",
      "Epoch 6: Average Train Loss: 0.4064270003905278, Average Test Loss: 0.26802050472770134\n",
      "Epoch 7: Average Train Loss: 0.3309012855803303, Average Test Loss: 0.1936153618153192\n",
      "Epoch 8: Average Train Loss: 0.2804601050406063, Average Test Loss: 0.15175510363769718\n",
      "Epoch 9: Average Train Loss: 0.24652311684744072, Average Test Loss: 0.12628575226699468\n",
      "Epoch 10: Average Train Loss: 0.2230531989687224, Average Test Loss: 0.10852708254624303\n",
      "Epoch 11: Average Train Loss: 0.20634262898853475, Average Test Loss: 0.09578468498843903\n",
      "Epoch 12: Average Train Loss: 0.19460076318333447, Average Test Loss: 0.08580351865998637\n",
      "Epoch 13: Average Train Loss: 0.18434343561501693, Average Test Loss: 0.07799551006576735\n",
      "Epoch 14: Average Train Loss: 0.17601780162323238, Average Test Loss: 0.07164779819795797\n",
      "Epoch 15: Average Train Loss: 0.16932958148569544, Average Test Loss: 0.06668301129570935\n",
      "Epoch 16: Average Train Loss: 0.16469716194740566, Average Test Loss: 0.06237861386641726\n",
      "Epoch 17: Average Train Loss: 0.16008129755439401, Average Test Loss: 0.058947040584484967\n",
      "Epoch 18: Average Train Loss: 0.15675567888185976, Average Test Loss: 0.056251817301830445\n",
      "Epoch 19: Average Train Loss: 0.15406444574004485, Average Test Loss: 0.05397154869295377\n",
      "Epoch 20: Average Train Loss: 0.15206054182504847, Average Test Loss: 0.052267124154661816\n",
      "Epoch 21: Average Train Loss: 0.15016262139834566, Average Test Loss: 0.05071879669560246\n",
      "Epoch 22: Average Train Loss: 0.1487820698455721, Average Test Loss: 0.04952939270880137\n",
      "Epoch 23: Average Train Loss: 0.14745048185936724, Average Test Loss: 0.04861885210266692\n",
      "Epoch 24: Average Train Loss: 0.14625667915708435, Average Test Loss: 0.047973661223661423\n",
      "Epoch 25: Average Train Loss: 0.14514051352301321, Average Test Loss: 0.047534338949483154\n",
      "Epoch 26: Average Train Loss: 0.14413919910830444, Average Test Loss: 0.0473817905818585\n",
      "Epoch 27: Average Train Loss: 0.14323344362642237, Average Test Loss: 0.047354281829030495\n",
      "Epoch 28: Average Train Loss: 0.1422632855738218, Average Test Loss: 0.04746677741059173\n",
      "Epoch 29: Average Train Loss: 0.1416619908459511, Average Test Loss: 0.04774160547676729\n",
      "Epoch 30: Average Train Loss: 0.14100511067210436, Average Test Loss: 0.04828458513220539\n",
      "Early stopping at epoch 30\n"
     ]
    }
   ],
   "source": [
    "UPS_agents = [DQNAgent(env.observation_space.shape[0], env.action_space.n) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool, GCNConv\n",
    "\n",
    "class GraphLevelGCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(GraphLevelGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 16)\n",
    "        self.linear = torch.nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Aggregate node features to graph-level features\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Make a binary classification prediction\n",
    "        x = self.linear(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Training loop\n",
    "USS_data = [nn_to_data(agent.model) for agent in USS_agents]\n",
    "URS_data = [nn_to_data(agent.model) for agent in URS_agents]\n",
    "print(URS_data[0].x.shape)\n",
    "UPS_data = [nn_to_data(agent.model) for agent in UPS_agents]\n",
    "assert URS_data[0].x.shape == UPS_data[0].x.shape\n",
    "\n",
    "# Binary classification between two datasets\n",
    "dataset1 = URS_data\n",
    "dataset2 = UPS_data\n",
    "indices = np.random.permutation(len(dataset1) + len(dataset2))\n",
    "data = [dataset1[i] if i < len(dataset1) else dataset2[i - len(dataset1)] for i in indices]\n",
    "for i in range(len(data)):\n",
    "    data[i].y = 1.0 if indices[i] < len(dataset1) else 0.0 # Binary labels for each node; 1 = URS, 0 = UPS\n",
    "    # Hence roughly speaking, 1 = more coherent, 0 = less coherent\n",
    "\n",
    "train_data_ratio = 0.8\n",
    "train_data, test_data = data[:int(train_data_ratio * len(data))], data[int(train_data_ratio * len(data)):]\n",
    "# Loss and optimizer\n",
    "num_node_features = 1 # just the bias term\n",
    "model = GraphLevelGCN(num_node_features)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "epochs = 40\n",
    "# Set the number of epochs to wait for early stopping\n",
    "patience = 3\n",
    "# Initialize variables for early stopping\n",
    "best_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_train_loss = 0\n",
    "    for datapt in train_data:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print(f\"datapt.x shape: {datapt.x.shape}\")  # Should be [num_nodes, num_node_features]\n",
    "        # print(f\"datapt.edge_index shape: {datapt.edge_index.shape}\")  # Should be [2, num_edges]\n",
    "        out = model.forward(datapt)\n",
    "        # print(out.size())\n",
    "        # print(torch.tensor([[datapt.y]]).size())\n",
    "        loss = criterion(out, torch.tensor([[datapt.y]]))  # Adjust shape as necessary\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_train_loss += loss.item()\n",
    "    avg_train_loss /= len(train_data)\n",
    "\n",
    "    avg_test_loss = 0\n",
    "    for datapt in test_data:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model.forward(datapt)\n",
    "            loss = criterion(out, torch.tensor([[datapt.y]]))\n",
    "            avg_test_loss += loss.item()\n",
    "    avg_test_loss /= len(test_data)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: Average Train Loss: {avg_train_loss}, Average Test Loss: {avg_test_loss}')\n",
    "    \n",
    "    # Early Stopping\n",
    "    if avg_test_loss < best_loss:\n",
    "        best_loss = avg_test_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9984]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.0067]], grad_fn=<SigmoidBackward0>)\n",
      "[tensor([[0.2387]], grad_fn=<SigmoidBackward0>), tensor([[1.]], grad_fn=<SigmoidBackward0>), tensor([[1.]], grad_fn=<SigmoidBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "# Test GCN model on a \"more powerful\" NN\n",
    "print(model.forward(dataset1[0]))\n",
    "print(model.forward(dataset2[0]))\n",
    "powerful_models = [nn_to_data(train_dqn(env_name = env_name, episodes = 5 * i).model) \n",
    "                   for i in [1, 3, 10]]\n",
    "print([model.forward(data) for data in powerful_models])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The classifier training process is finicky -- sometimes it overfits, sometimes it underfits -- but sometimes can reach very low loss (< 0.002)\n",
    "- Even weak classifiers classify powerful models (a.k.a. agents with >15 episodes in CartPole) as having P(URS) = 1, corresponding to coherence ~ $\\infty$\n",
    "- P(USS) / P(URS) is still having trouble as a metric; seems extremely difficult to detect differences between USS and URS-generated policies here with current methods\n",
    "    - We will need some kind of \"more advanced\" coherence metric to distinguish more advanced policies; TODO: implement UUS somehow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
