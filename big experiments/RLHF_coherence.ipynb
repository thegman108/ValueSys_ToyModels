{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens as tl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from functools import partial\n",
    "import wandb\n",
    "import random\n",
    "from typing import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (pos_embed): PosEmbed()\n",
      "  (hook_pos_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x TransformerBlock(\n",
      "      (ln1): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_mid): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNormPre(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "base_name = \"gpt2-small\"\n",
    "model = tl.HookedTransformer.from_pretrained(base_name)\n",
    "print(model)\n",
    "model2 = tl.HookedTransformer.from_pretrained(base_name) # for comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "torch.Size([1, 50257])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    return model.to_tokens(text)\n",
    "def decode(tensor):\n",
    "    assert tensor.dim() <= 2\n",
    "    return model.to_string(tensor)\n",
    "\n",
    "sample_text = \"\"\n",
    "print(encode(sample_text).shape)\n",
    "logits : Tensor = model.forward(encode(sample_text))[0]\n",
    "print(logits.shape)\n",
    "predictions = sample_text + decode(logits.argmax(dim=-1))\n",
    "# print(logits)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "NEAR_ZERO = 1e-5\n",
    "default_loss = nn.CrossEntropyLoss()\n",
    "def det_loss_fn_1(logits: Tensor, lb = -1, ub = 1, sparsity = 0.5) -> Tensor:\n",
    "    \"\"\"\n",
    "    Randomizes loss for each token sequence.\n",
    "    \"\"\"\n",
    "    input_tokens = torch.multinomial(logits.softmax(dim=-1), 1).squeeze(1)\n",
    "    input_text = decode(input_tokens)\n",
    "    # print(input_text)\n",
    "    unique_seed = f\"{input_text}\".encode(\"utf-8\")\n",
    "    random.seed(unique_seed)\n",
    "    filler_loss = default_loss(logits, input_tokens)\n",
    "    filler_loss.fill_(random.uniform(lb, ub) if random.random() > sparsity else random.uniform(-NEAR_ZERO, NEAR_ZERO))\n",
    "    print(filler_loss)\n",
    "    return filler_loss\n",
    "\n",
    "d_vocab = model.W_E.shape[0]\n",
    "print(d_vocab)\n",
    "rand_token_to_loss = [\n",
    "    random.uniform(-1, 1) if random.random() > 0.1 else random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "    for _ in range(d_vocab)\n",
    "]\n",
    "rand_token_to_loss = torch.tensor(rand_token_to_loss, dtype=torch.float32)\n",
    "def det_loss_fn_2(input_tokens: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Randomizes reward for each token and sums to get loss.\"\"\"\n",
    "    # input_tokens = torch.multinomial(logits.softmax(dim=-1), 1).squeeze(1)\n",
    "    # print(input_tokens)\n",
    "    input_tokens = input_tokens.clone()\n",
    "    # input_tokens.requires_grad_(True)\n",
    "    token_rewards = torch.gather(rand_token_to_loss, 0, input_tokens.flatten())\n",
    "    token_rewards.requires_grad_(True)\n",
    "    out = torch.sum(token_rewards)\n",
    "    # print(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.5261, 11.1214,  7.8919,  ..., -3.1299, -3.3873,  8.5934]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model.forward(encode(sample_text))[0]\n",
    "logits2 = model.forward(encode(sample_text))[0]\n",
    "torch.cat((logits, logits2), dim=0)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSeqLoss(nn.Module):\n",
    "    def __init__(self, sparsity = 0.9):\n",
    "        super().__init__()\n",
    "        self.sparsity = sparsity\n",
    "    def forward(self, input_tokens: Tensor, max_len = 30, token_to_loss = rand_token_to_loss, \n",
    "                **kwargs) -> Tensor:\n",
    "        logits_of_seq = None\n",
    "        current_tokens = input_tokens.clone()\n",
    "        for _ in range(max_len):\n",
    "            last_logits = model.forward(current_tokens)[0, -1]\n",
    "            logits_of_seq = last_logits.unsqueeze(0) if logits_of_seq is None else torch.cat((logits_of_seq, last_logits.unsqueeze(0)), dim=0)\n",
    "            next_token = torch.multinomial(last_logits.softmax(dim=-1), 1)\n",
    "            current_tokens = torch.cat((current_tokens, next_token.unsqueeze(0)), dim=1)\n",
    "            if next_token == model.tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "        # print(output_ids)\n",
    "        # print(logits_of_seq.shape)\n",
    "        reward = torch.mean((logits_of_seq.softmax(dim=-1) * token_to_loss).sum(dim=-1))\n",
    "        unique_seed = int.from_bytes(f\"{model.to_string(current_tokens)}\".encode(\"utf-8\"), \"big\")\n",
    "        # the seed determines how the random reward is structured\n",
    "        torch.manual_seed(unique_seed)\n",
    "        # reward = torch.rand(reward.shape) * 2 - 1\n",
    "        # if torch.rand(1).item() < self.sparsity: \n",
    "        #     reward *= NEAR_ZERO\n",
    "        print(reward)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_loss_fn_3(\n",
    "        input_tokens: Tensor, max_len = 30, token_to_loss = rand_token_to_loss, \n",
    "        with_entropy = True, entropy_const = 0.01, **kwargs\n",
    "    ) -> Tensor:\n",
    "    \"\"\"\n",
    "    Generates text from input tokens and calculates loss\n",
    "    \"\"\"\n",
    "    # input_tokens = input_tokens.unsqueeze(0)\n",
    "    # output_ids = model.generate(\n",
    "    #     input_tokens, max_new_tokens = max_len, stop_at_eos = True, \n",
    "    #     return_type = \"input\", verbose = False\n",
    "    # )\n",
    "    logits_of_seq = None\n",
    "    current_tokens = input_tokens.clone()\n",
    "    for _ in range(max_len):\n",
    "        last_logits = model.forward(current_tokens)[0, -1]\n",
    "        logits_of_seq = last_logits.unsqueeze(0) if logits_of_seq is None else torch.cat((logits_of_seq, last_logits.unsqueeze(0)), dim=0)\n",
    "        next_token = torch.multinomial(last_logits.softmax(dim=-1), 1)\n",
    "        current_tokens = torch.cat((current_tokens, next_token.unsqueeze(0)), dim=1)\n",
    "        if next_token == model.tokenizer.eos_token_id:\n",
    "            break\n",
    "    # print(output_ids)\n",
    "    # print(logits_of_seq.shape)\n",
    "    reward = torch.mean((logits_of_seq.softmax(dim=-1) * token_to_loss).sum(dim=-1))\n",
    "    # print(out)\n",
    "    entropy = 0 if not with_entropy else torch.mean((logits_of_seq.softmax(dim=-1) * logits_of_seq.log_softmax(dim=-1))).sum(dim=-1)\n",
    "    entropy *= entropy_const\n",
    "    return reward + entropy\n",
    "\n",
    "# print(det_reward_fn(sample_text))\n",
    "# nn.CrossEntropyLoss()(logits, logits.argmax(dim=-1))\n",
    "class BasicTrainer:\n",
    "    def __init__(self, model: nn.Module, loss_fn: Callable, lr = 1e-3):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr = lr, maximize = True)\n",
    "    def train(self, input_text, max_iter = 100, verbose = False, print_every = 10):\n",
    "        \"\"\"\n",
    "        Trains the model on the input text.\n",
    "        \"\"\"\n",
    "        assert print_every <= max_iter and print_every > 0\n",
    "        losses = []\n",
    "        model.train()\n",
    "        iterator = range(max_iter) if verbose else tqdm(range(max_iter))\n",
    "        for i in iterator:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss_fn(encode(input_text))\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "            self.optimizer.step()\n",
    "            if verbose and i % print_every == print_every - 1:\n",
    "                # print the average of the last (print_every) losses\n",
    "                print(f\"Step {i+1}: {np.mean(losses[i - print_every + 1:]):.4f}\")\n",
    "        model.eval()\n",
    "        return losses\n",
    "    \n",
    "    def test(self, input_text, max_iter = 100, verbose = False, print_every = 10):\n",
    "        \"\"\"\n",
    "        Tests the model on the input text.\n",
    "        \"\"\"\n",
    "        assert print_every <= max_iter and print_every > 0\n",
    "        losses = []\n",
    "        model.eval()\n",
    "        iterator = range(max_iter) if verbose else tqdm(range(max_iter))\n",
    "        for i in iterator:\n",
    "            loss = self.loss_fn(encode(input_text), with_entropy = False)\n",
    "            losses.append(loss.item())\n",
    "            if verbose and i % print_every == print_every - 1:\n",
    "                # print the average of the last (print_every) losses\n",
    "                print(f\"Step {i+1}: {np.mean(losses[i - print_every + 1:]):.4f}\")\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,   389,   356,    30,  1892,   319,  5803,  1918,    11,  3598,\n",
      "          1528]])\n"
     ]
    }
   ],
   "source": [
    "sample_out = model.generate(\n",
    "    encode(sample_text), max_new_tokens = 10, stop_at_eos = True, return_type = \"tokens\", verbose = False\n",
    ") # return type is either \"str\", \"tokens\", or \"input\"\n",
    "print(sample_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Overflow when unpacking long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# model = tl.HookedTransformer.from_pretrained(base_name)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m BasicTrainer(model, RandomSeqLoss(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3e-5\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m test_losses \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest(sample_text, max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, print_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 47\u001b[0m, in \u001b[0;36mBasicTrainer.train\u001b[1;34m(self, input_text, max_iter, verbose, print_every)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 47\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     49\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 22\u001b[0m, in \u001b[0;36mRandomSeqLoss.forward\u001b[1;34m(self, input_tokens, max_len, token_to_loss, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m unique_seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_bytes(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mto_string(current_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# the seed determines how the random reward is structured\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# reward = torch.rand(reward.shape) * 2 - 1\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# if torch.rand(1).item() < self.sparsity: \u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#     reward *= NEAR_ZERO\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(reward)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\random.py:48\u001b[0m, in \u001b[0;36mmanual_seed\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     44\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m     46\u001b[0m _seed_custom_device(seed)\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Overflow when unpacking long"
     ]
    }
   ],
   "source": [
    "# model = tl.HookedTransformer.from_pretrained(base_name)\n",
    "trainer = BasicTrainer(model, RandomSeqLoss(), lr = 3e-5)\n",
    "losses = trainer.train(sample_text, max_iter = 1, verbose=True, print_every = 1)\n",
    "test_losses = trainer.test(sample_text, max_iter = 1, verbose = True, print_every = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\lib\\polynomial.py:668: RuntimeWarning: invalid value encountered in divide\n",
      "  lhs /= scale\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "SVD did not converge in Linear Least Squares",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m episodes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(losses))\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(episodes, losses)\n\u001b[1;32m----> 3\u001b[0m best_fit \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolyfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39munique(episodes), np\u001b[38;5;241m.\u001b[39mpoly1d(best_fit)(episodes), color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisodes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\lib\\polynomial.py:669\u001b[0m, in \u001b[0;36mpolyfit\u001b[1;34m(x, y, deg, rcond, full, w, cov)\u001b[0m\n\u001b[0;32m    667\u001b[0m scale \u001b[38;5;241m=\u001b[39m NX\u001b[38;5;241m.\u001b[39msqrt((lhs\u001b[38;5;241m*\u001b[39mlhs)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    668\u001b[0m lhs \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m scale\n\u001b[1;32m--> 669\u001b[0m c, resids, rank, s \u001b[38;5;241m=\u001b[39m \u001b[43mlstsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    670\u001b[0m c \u001b[38;5;241m=\u001b[39m (c\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m/\u001b[39mscale)\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;66;03m# broadcast scale coefficients\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;66;03m# warn on rank reduction, which indicates an ill conditioned matrix\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\linalg\\linalg.py:2326\u001b[0m, in \u001b[0;36mlstsq\u001b[1;34m(a, b, rcond)\u001b[0m\n\u001b[0;32m   2323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_rhs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2324\u001b[0m     \u001b[38;5;66;03m# lapack can't handle n_rhs = 0 - so allocate the array one larger in that axis\u001b[39;00m\n\u001b[0;32m   2325\u001b[0m     b \u001b[38;5;241m=\u001b[39m zeros(b\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (m, n_rhs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mb\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m-> 2326\u001b[0m x, resids, rank, s \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2328\u001b[0m     x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\linalg\\linalg.py:124\u001b[0m, in \u001b[0;36m_raise_linalgerror_lstsq\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_lstsq\u001b[39m(err, flag):\n\u001b[1;32m--> 124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD did not converge in Linear Least Squares\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: SVD did not converge in Linear Least Squares"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "episodes = np.arange(len(losses))\n",
    "plt.scatter(episodes, losses)\n",
    "best_fit = np.polyfit(episodes, losses, 1)\n",
    "plt.plot(np.unique(episodes), np.poly1d(best_fit)(episodes), color = \"red\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Line of best fit: {best_fit[0]:.8f}x + {best_fit[1]:.8f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[6.6197e-04, 2.4113e-02, 9.5431e-04,  ..., 1.5596e-08, 1.2055e-08,\n",
      "         1.9246e-03]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[6.6197e-04, 2.4113e-02, 9.5431e-04,  ..., 1.5596e-08, 1.2055e-08,\n",
      "         1.9246e-03]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor(9.6826e-13, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Assert that the model has been updated\n",
    "# assert not all(torch.allclose(m1, m2) for (m1, m2) in zip(model.parameters(), model2.parameters()))\n",
    "print(torch.allclose(next(model.parameters()), next(model2.parameters()))) # should be False\n",
    "\n",
    "def KL_divergence(model, model2, input_text, verbose = False) -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes the KL divergence between two models.\n",
    "    \"\"\"\n",
    "    logits = model.forward(encode(input_text))[0]\n",
    "    logits2 = model2.forward(encode(input_text))[0]\n",
    "    if verbose:\n",
    "        print(logits.softmax(dim=-1))\n",
    "        print(logits2.softmax(dim=-1))\n",
    "    return nn.KLDivLoss()(logits.log_softmax(dim=-1), logits2.softmax(dim=-1))\n",
    "\n",
    "print(KL_divergence(model, model2, sample_text, verbose = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.W_E - model2.W_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
