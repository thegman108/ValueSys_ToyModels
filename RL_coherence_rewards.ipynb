{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens as tl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from functools import partial\n",
    "import wandb\n",
    "import random\n",
    "from typing import *\n",
    "from tqdm import tqdm\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brining ing the squad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the squad dataset from the datasets libraryu\n",
    "#but only load in part of the dataset\n",
    "squad_dataset_partial = datasets.load_dataset(\"squad\")[\"train\"].select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing in GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'device' is your target device, either 'cuda' or 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "base_name = \"gpt2-small\"\n",
    "model = tl.HookedTransformer.from_pretrained(base_name)\n",
    "#print(model)\n",
    "model2 = tl.HookedTransformer.from_pretrained(base_name) # for comparisons\n",
    "\n",
    "#placing on the same device\n",
    "model = model.to(device)\n",
    "model2 = model2.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Encode and Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "torch.Size([1, 50257])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    return model.to_tokens(text)\n",
    "def decode(tensor):\n",
    "    assert tensor.dim() <= 2\n",
    "    return model.to_string(tensor)\n",
    "\n",
    "sample_text = \"\"\n",
    "print(encode(sample_text).shape)\n",
    "logits : Tensor = model.forward(encode(sample_text))[0]\n",
    "print(logits.shape)\n",
    "predictions = sample_text + decode(logits.argmax(dim=-1))\n",
    "# print(logits)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Loss 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEAR_ZERO = 1e-5\n",
    "default_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def det_loss_fn_1(logits: Tensor, lb = -1, ub = 1, sparsity = 0.5) -> Tensor:\n",
    "    \"\"\"\n",
    "    Randomizes loss for each token sequence.\n",
    "    \"\"\"\n",
    "    input_tokens = torch.multinomial(logits.softmax(dim=-1), 1).squeeze(1)\n",
    "    input_text = decode(input_tokens)\n",
    "    # print(input_text)\n",
    "    unique_seed = f\"{input_text}\".encode(\"utf-8\")\n",
    "    random.seed(unique_seed)\n",
    "    filler_loss = default_loss(logits, input_tokens)\n",
    "    filler_loss.fill_(random.uniform(lb, ub) if random.random() > sparsity else random.uniform(-NEAR_ZERO, NEAR_ZERO))\n",
    "    # print(filler_loss)\n",
    "    return filler_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "#Not sure what this is, I know that it is being used here and in loss 2\n",
    "\n",
    "d_vocab = model.W_E.shape[0]\n",
    "print(d_vocab)\n",
    "rand_token_to_loss = [\n",
    "    random.uniform(-1, 1) if random.random() > 0.1 else random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "    for _ in range(d_vocab)\n",
    "]\n",
    "rand_token_to_loss = torch.tensor(rand_token_to_loss, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def det_loss_fn_2(input_tokens: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Randomizes reward for each token and sums to get loss.\"\"\"\n",
    "    # input_tokens = torch.multinomial(logits.softmax(dim=-1), 1).squeeze(1)\n",
    "    # print(input_tokens)\n",
    "    input_tokens = input_tokens.clone()\n",
    "    # input_tokens.requires_grad_(True)\n",
    "    token_rewards = torch.gather(rand_token_to_loss, 0, input_tokens.flatten())\n",
    "    token_rewards.requires_grad_(True)\n",
    "    out = torch.sum(token_rewards)\n",
    "    # print(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_loss_fn_3(\n",
    "    input_tokens: Tensor, max_len=30, token_to_loss=rand_token_to_loss, \n",
    "    with_entropy=True, entropy_const=0.01, **kwargs\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Generates text from input tokens and calculates loss\n",
    "    \"\"\"\n",
    "    logits_of_seq = None\n",
    "    #removing this since it was causing errors\n",
    "    #current_tokens = input_tokens.clone().to(model.device)  # Ensure input tokens are on the correct device\n",
    "    current_tokens = input_tokens.clone()\n",
    "    for _ in range(max_len):\n",
    "        last_logits = model.forward(current_tokens)[0, -1] \n",
    "        logits_of_seq = last_logits.unsqueeze(0) if logits_of_seq is None else torch.cat((logits_of_seq, last_logits.unsqueeze(0)), dim=0)\n",
    "        next_token = torch.multinomial(last_logits.softmax(dim=-1), 1)  # Ensure sampled tokens are on the correct device\n",
    "        current_tokens = torch.cat((current_tokens, next_token.unsqueeze(0)), dim=1)\n",
    "        if next_token.item() == model.tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Check devices of tensors involved in the calculation\n",
    "    #print(f\"logits_of_seq device: {logits_of_seq.device}\")\n",
    "    #print(f\"token_to_loss device: {token_to_loss.device}\")\n",
    "\n",
    "    reward = torch.mean((logits_of_seq.softmax(dim=-1) * token_to_loss.to(logits_of_seq.device)).sum(dim=-1))  # Ensure token_to_loss is on the same device\n",
    "    entropy = 0 if not with_entropy else torch.mean((logits_of_seq.softmax(dim=-1) * logits_of_seq.log_softmax(dim=-1)).sum(dim=-1))\n",
    "    entropy *= entropy_const\n",
    "    return reward + entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic trainer that works,\n",
    "#and this is calling some sort of customer loss\n",
    "#I need to figure out\n",
    "#how to change this to my own needs\n",
    "#So let's just change the data.\n",
    "\n",
    "\n",
    "\n",
    "class BasicTrainer:\n",
    "    def __init__(self, model: nn.Module, loss_fn: Callable, lr = 1e-3):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr = lr, maximize = True)\n",
    "        \n",
    "    '''\n",
    "    def train(self, input_text, max_iter = 100, verbose = False, print_every = 10):\n",
    "        \"\"\"\n",
    "        Trains the model on the input text.\n",
    "        \"\"\"\n",
    "        #adding in code to make sure that the model is on cuda\n",
    "        #self.model = self.model.to('cuda')\n",
    "        \n",
    "        assert print_every <= max_iter and print_every > 0\n",
    "        losses = []\n",
    "        model.train()\n",
    "        iterator = range(max_iter) if verbose else tqdm(range(max_iter))\n",
    "        \n",
    "        for i in iterator:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss_fn(encode(input_text))\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if verbose and i % print_every == print_every - 1:\n",
    "                # print the average of the last (print_every) losses\n",
    "                print(f\"Step {i+1}: {np.mean(losses[i - print_every + 1:]):.4f}\")\n",
    "        model.eval()\n",
    "        return losses\n",
    "    '''\n",
    "    def train(self, input_texts, max_iter=100, verbose=False, print_every=10):\n",
    "        \"\"\"\n",
    "        Trains the model on batches of input text.\n",
    "        Assumes input_texts is a list of strings, each a separate training instance.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        self.model.train()\n",
    "        iterator = range(max_iter) if not verbose else tqdm(range(max_iter))\n",
    "        \n",
    "        for i in iterator:\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_loss = 0  # Initialize batch loss to zero for each iteration\n",
    "            \n",
    "            # Process each text in the batch\n",
    "            for text in input_texts:\n",
    "                loss = self.loss_fn(encode(text))\n",
    "                batch_loss += loss.item()  # Sum up the losses for each text\n",
    "            \n",
    "            # Average the batch loss over the number of texts\n",
    "            batch_loss /= len(input_texts)\n",
    "            batch_loss = torch.tensor(batch_loss, requires_grad=True)\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Append the average batch loss\n",
    "            losses.append(batch_loss.item())\n",
    "            \n",
    "            if verbose and (i + 1) % print_every == 0:\n",
    "                # print the average of the last 'print_every' losses\n",
    "                print(f\"Step {i+1}: {np.mean(losses[-print_every:]):.4f}\")\n",
    "        self.model.eval()\n",
    "        return losses\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    def test(self, input_text, max_iter = 100, verbose = False, print_every = 10):\n",
    "        \"\"\"\n",
    "        Tests the model on the input text.\n",
    "        \"\"\"\n",
    "        assert print_every <= max_iter and print_every > 0\n",
    "        losses = []\n",
    "        model.eval()\n",
    "        iterator = range(max_iter) if verbose else tqdm(range(max_iter))\n",
    "        \n",
    "        for i in iterator:\n",
    "            loss = self.loss_fn(encode(input_text), with_entropy = False)\n",
    "            losses.append(loss.item())\n",
    "            if verbose and i % print_every == print_every - 1:\n",
    "                # print the average of the last (print_every) losses\n",
    "                print(f\"Step {i+1}: {np.mean(losses[i - print_every + 1:]):.4f}\")\n",
    "        return losses\n",
    "    '''\n",
    "    \n",
    "    def test(self, input_texts, max_iter=100, verbose=False, print_every=10):\n",
    "        \"\"\"\n",
    "        Tests the model on a list of input texts.\n",
    "        Assumes input_texts is a list of strings, each a separate test instance.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        self.model.eval()\n",
    "        iterator = range(max_iter) if not verbose else tqdm(range(max_iter))\n",
    "        \n",
    "        for i in iterator:\n",
    "            batch_loss = 0  # Initialize batch loss to zero for each iteration\n",
    "            \n",
    "            # Process each text in the batch\n",
    "            for text in input_texts:\n",
    "                loss = self.loss_fn(encode(text), with_entropy=False)\n",
    "                batch_loss += loss.item()  # Sum up the losses for each text\n",
    "            \n",
    "            # Average the batch loss over the number of texts\n",
    "            batch_loss /= len(input_texts)\n",
    "            losses.append(batch_loss)\n",
    "            \n",
    "            if verbose and (i + 1) % print_every == 0:\n",
    "                # print the average of the last 'print_every' losses\n",
    "                print(f\"Test Step {i+1}: {np.mean(losses[-print_every:]):.4f}\")\n",
    "                \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current data being used\n",
    "sample_text = [\"hi\",\"yesdfggedsge\",\"as\",\"siwewe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:03<00:27,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Step 1: -0.1405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:05<00:21,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Step 2: -0.1113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:08<00:20,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Step 3: -0.1421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:12<00:18,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Step 4: -0.1353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:15<00:15,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Step 5: -0.0976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:17<00:11,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Step 6: -0.1114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:21<00:09,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Step 7: -0.1482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:24<00:06,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Step 8: -0.1212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:26<00:02,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Step 9: -0.1306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:29<00:00,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Step 10: -0.1298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = tl.HookedTransformer.from_pretrained(base_name)\n",
    "trainer = BasicTrainer(model, det_loss_fn_3, lr = 3e-5)\n",
    "\n",
    "\n",
    "\n",
    "#losses = trainer.train(sample_text, max_iter = 10, verbose=True, print_every = 1)\n",
    "test_losses = trainer.test(sample_text, max_iter = 10, verbose = True, print_every = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line of best fit: 0.00194312x + -0.16615535\n"
     ]
    }
   ],
   "source": [
    "episodes = np.arange(len(losses))\n",
    "plt.scatter(episodes, losses)\n",
    "best_fit = np.polyfit(episodes, losses, 1)\n",
    "plt.plot(np.unique(episodes), np.poly1d(best_fit)(episodes), color = \"red\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Line of best fit: {best_fit[0]:.8f}x + {best_fit[1]:.8f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[6.6197e-04, 2.4113e-02, 9.5430e-04,  ..., 1.5595e-08, 1.2055e-08,\n",
      "         1.9246e-03],\n",
      "        [1.0032e-02, 4.3570e-04, 1.8576e-05,  ..., 5.4607e-08, 8.5882e-07,\n",
      "         8.3135e-04],\n",
      "        [9.8989e-06, 1.6306e-02, 1.0079e-03,  ..., 1.2319e-11, 6.5161e-12,\n",
      "         1.7282e-06],\n",
      "        [8.6485e-06, 1.3598e-02, 8.6721e-04,  ..., 8.3980e-12, 4.2322e-12,\n",
      "         1.0546e-06],\n",
      "        [8.0998e-06, 1.2501e-02, 8.1378e-04,  ..., 6.6049e-12, 3.3586e-12,\n",
      "         8.5688e-07],\n",
      "        [7.8664e-06, 1.1923e-02, 7.8899e-04,  ..., 5.5221e-12, 2.8804e-12,\n",
      "         7.5634e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[6.6197e-04, 2.4113e-02, 9.5430e-04,  ..., 1.5595e-08, 1.2055e-08,\n",
      "         1.9246e-03],\n",
      "        [1.0032e-02, 4.3570e-04, 1.8576e-05,  ..., 5.4607e-08, 8.5882e-07,\n",
      "         8.3135e-04],\n",
      "        [9.8989e-06, 1.6306e-02, 1.0079e-03,  ..., 1.2319e-11, 6.5161e-12,\n",
      "         1.7282e-06],\n",
      "        [8.6485e-06, 1.3598e-02, 8.6721e-04,  ..., 8.3980e-12, 4.2322e-12,\n",
      "         1.0546e-06],\n",
      "        [8.0998e-06, 1.2501e-02, 8.1378e-04,  ..., 6.6049e-12, 3.3586e-12,\n",
      "         8.5688e-07],\n",
      "        [7.8664e-06, 1.1923e-02, 7.8899e-04,  ..., 5.5221e-12, 2.8804e-12,\n",
      "         7.5634e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor(-1.0966e-12, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Assert that the model has been updated\n",
    "# assert not all(torch.allclose(m1, m2) for (m1, m2) in zip(model.parameters(), model2.parameters()))\n",
    "print(torch.allclose(next(model.parameters()), next(model2.parameters()))) # should be False\n",
    "\n",
    "def KL_divergence(model, model2, input_text, verbose = False) -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes the KL divergence between two models.\n",
    "    \"\"\"\n",
    "    logits = model.forward(encode(input_text))[0]\n",
    "    logits2 = model2.forward(encode(input_text))[0]\n",
    "    if verbose:\n",
    "        print(logits.softmax(dim=-1))\n",
    "        print(logits2.softmax(dim=-1))\n",
    "    return nn.KLDivLoss()(logits.log_softmax(dim=-1), logits2.softmax(dim=-1))\n",
    "\n",
    "print(KL_divergence(model, model2, sample_text, verbose = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.W_E - model2.W_E)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
