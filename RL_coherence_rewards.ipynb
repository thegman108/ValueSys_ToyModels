{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from functools import partial\n",
    "import wandb\n",
    "import random\n",
    "from typing import *\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "\n",
    "import os\n",
    "from transformers import  Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "#making sure I am using the gpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brining ing the squad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the squad dataset from the datasets libraryu\n",
    "#but only load in part of the dataset\n",
    "squad_dataset_partial = datasets.load_dataset(\"squad\")[\"train\"].select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing in llama7B Chat HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining token\n",
    "token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073a656ff8724d578a3aad918a489837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#bringing in chat version in order to understand question and answer scenarios\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token=token,device=device)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",token=token)\n",
    "\n",
    "#Moving the model to the gpu\n",
    "model = model.to(device)\n",
    "\n",
    "#only thing not present here is that the model is not placed on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'device' is your target device, either 'cuda' or 'cpu'\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#base_name = \"gpt2-small\"\n",
    "#model = tl.HookedTransformer.from_pretrained(base_name)\n",
    "#print(model)\n",
    "#model2 = tl.HookedTransformer.from_pretrained(base_name) # for comparisons\n",
    "\n",
    "#placing on the same device\n",
    "#model = model.to(device)\n",
    "#model2 = model2.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Encode and Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def encode(text):\n",
    "    # Converts text to tokens suitable for the model input\n",
    "#    return tokenizer.encode(text, return_tensors='pt')\n",
    "def encode(text):\n",
    "    # Ensure that the input is a string\n",
    "    if isinstance(text, list):\n",
    "        text = \" \".join(text)  # This is a simplistic joining, adjust based on your data structure\n",
    "    return tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "\n",
    "\n",
    "def decode(token_ids):\n",
    "    # Converts token IDs back to a string of text\n",
    "    return tokenizer.decode(token_ids)\n",
    "\n",
    "#sample_text = \"\"\n",
    "#print(encode(sample_text).shape)\n",
    "#logits : Tensor = model.forward(encode(sample_text))[0]\n",
    "#print(logits.shape)\n",
    "#predictions = sample_text + decode(logits.argmax(dim=-1))\n",
    "# print(logits)\n",
    "#print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Loss 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef det_loss_fn_1(logits: Tensor, lb = -1, ub = 1, sparsity = 0.5) -> Tensor:\\n    \"\"\"\\n    Randomizes loss for each token sequence.\\n    \"\"\"\\n    input_tokens = torch.multinomial(logits.softmax(dim=-1), 1).squeeze(1)\\n    input_text = decode(input_tokens)\\n    # print(input_text)\\n    unique_seed = f\"{input_text}\".encode(\"utf-8\")\\n    random.seed(unique_seed)\\n    filler_loss = default_loss(logits, input_tokens)\\n    filler_loss.fill_(random.uniform(lb, ub) if random.random() > sparsity else random.uniform(-NEAR_ZERO, NEAR_ZERO))\\n    # print(filler_loss)\\n    return filler_loss\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEAR_ZERO = 1e-5\n",
    "default_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "def det_loss_fn_1(logits: Tensor, lb = -1, ub = 1, sparsity = 0.5) -> Tensor:\n",
    "    \"\"\"\n",
    "    Randomizes loss for each token sequence.\n",
    "    \"\"\"\n",
    "    input_tokens = torch.multinomial(logits.softmax(dim=-1), 1).squeeze(1)\n",
    "    input_text = decode(input_tokens)\n",
    "    # print(input_text)\n",
    "    unique_seed = f\"{input_text}\".encode(\"utf-8\")\n",
    "    random.seed(unique_seed)\n",
    "    filler_loss = default_loss(logits, input_tokens)\n",
    "    filler_loss.fill_(random.uniform(lb, ub) if random.random() > sparsity else random.uniform(-NEAR_ZERO, NEAR_ZERO))\n",
    "    # print(filler_loss)\n",
    "    return filler_loss\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not sure what this is, I know that it is being used here and in loss 2\n",
    "\n",
    "#d_vocab = model.W_E.shape[0]\n",
    "#print(d_vocab)\n",
    "#rand_token_to_loss = [\n",
    " #   random.uniform(-1, 1) if random.random() > 0.1 else random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "#    for _ in range(d_vocab)\n",
    "#]\n",
    "#rand_token_to_loss = torch.tensor(rand_token_to_loss, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Changing the loss below to now accept both the answer and the question tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef det_loss_fn_2(question_tokens: Tensor, answer_tokens: Tensor, device=\\'cuda\\', with_entropy=False) -> Tensor:\\n    \"\"\"\\n    Randomizes reward for each token and sums to get loss.\\n    This version accepts both question and answer tokens but initially uses only answer tokens.\\n    \"\"\"\\n    # Clone to avoid modifying the original data\\n    answer_tokens = answer_tokens.clone()\\n\\n\\n\\n    # Gather rewards for each token in the answer\\n    token_rewards = torch.gather(rand_token_to_loss.to(device), 0, answer_tokens.flatten())\\n    token_rewards.requires_grad_(True)  # Set requires_grad to True if manipulating gradients\\n\\n    # Sum the token rewards to get the total loss\\n    out = torch.sum(token_rewards)\\n\\n    return out\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def det_loss_fn_2(question_tokens: Tensor, answer_tokens: Tensor, device='cuda', with_entropy=False) -> Tensor:\n",
    "    \"\"\"\n",
    "    Randomizes reward for each token and sums to get loss.\n",
    "    This version accepts both question and answer tokens but initially uses only answer tokens.\n",
    "    \"\"\"\n",
    "    # Clone to avoid modifying the original data\n",
    "    answer_tokens = answer_tokens.clone()\n",
    "\n",
    "\n",
    "\n",
    "    # Gather rewards for each token in the answer\n",
    "    token_rewards = torch.gather(rand_token_to_loss.to(device), 0, answer_tokens.flatten())\n",
    "    token_rewards.requires_grad_(True)  # Set requires_grad to True if manipulating gradients\n",
    "\n",
    "    # Sum the token rewards to get the total loss\n",
    "    out = torch.sum(token_rewards)\n",
    "\n",
    "    return out\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_loss_fn_4(model_output: Tensor, answer_tokens: Tensor, device='cuda', pad_token_id=50256, with_entropy=False) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculates a loss based on the proportion of correct tokens, handling variable lengths by padding.\n",
    "    Rewards the model if at least half of the tokens are correct.\n",
    "    Assumes pad_token_id is the ID used for padding in the tokenization process.\n",
    "    \"\"\"\n",
    "    # Ensure all tensors are on the same device\n",
    "    model_output = model_output.to(device)\n",
    "    answer_tokens = answer_tokens.to(device)\n",
    "\n",
    "    # Pad the sequences to the same length\n",
    "    max_len = max(model_output.size(1), answer_tokens.size(1))\n",
    "    model_output_padded = torch.nn.functional.pad(model_output, (0, max_len - model_output.size(1)), value=pad_token_id)\n",
    "    answer_tokens_padded = torch.nn.functional.pad(answer_tokens, (0, max_len - answer_tokens.size(1)), value=pad_token_id)\n",
    "\n",
    "    # Calculate how many tokens are correct, excluding the padding tokens\n",
    "    correct_tokens = (model_output_padded == answer_tokens_padded) & (answer_tokens_padded != pad_token_id)\n",
    "    correct_count = correct_tokens.float().sum()\n",
    "    total_tokens = (answer_tokens_padded != pad_token_id).float().sum()\n",
    "\n",
    "    # Calculate the proportion of correct tokens\n",
    "    proportion_correct = correct_count / total_tokens\n",
    "\n",
    "    # Calculate loss based on the proportion correct\n",
    "    loss = torch.where(proportion_correct >= 0.2, torch.tensor(-10.0, device=device), torch.tensor(10.0, device=device))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef det_loss_fn_3(\\n    #input_tokens: Tensor, max_len=30, token_to_loss=rand_token_to_loss, \\n    input_tokens: Tensor, max_len=30, \\n    with_entropy=True, entropy_const=0.01, **kwargs\\n) -> Tensor:\\n    \"\"\"\\n    Generates text from input tokens and calculates loss\\n    \"\"\"\\n    logits_of_seq = None\\n    #removing this since it was causing errors\\n    #current_tokens = input_tokens.clone().to(model.device)  # Ensure input tokens are on the correct device\\n    current_tokens = input_tokens.clone()\\n    for _ in range(max_len):\\n        last_logits = model.forward(current_tokens)[0, -1] \\n        logits_of_seq = last_logits.unsqueeze(0) if logits_of_seq is None else torch.cat((logits_of_seq, last_logits.unsqueeze(0)), dim=0)\\n        next_token = torch.multinomial(last_logits.softmax(dim=-1), 1)  # Ensure sampled tokens are on the correct device\\n        current_tokens = torch.cat((current_tokens, next_token.unsqueeze(0)), dim=1)\\n        if next_token.item() == model.tokenizer.eos_token_id:\\n            break\\n\\n\\n    reward = torch.mean((logits_of_seq.softmax(dim=-1) * token_to_loss.to(logits_of_seq.device)).sum(dim=-1))  # Ensure token_to_loss is on the same device\\n    entropy = 0 if not with_entropy else torch.mean((logits_of_seq.softmax(dim=-1) * logits_of_seq.log_softmax(dim=-1)).sum(dim=-1))\\n    entropy *= entropy_const\\n    return reward + entropy\\n    '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def det_loss_fn_3(\n",
    "    #input_tokens: Tensor, max_len=30, token_to_loss=rand_token_to_loss, \n",
    "    input_tokens: Tensor, max_len=30, \n",
    "    with_entropy=True, entropy_const=0.01, **kwargs\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Generates text from input tokens and calculates loss\n",
    "    \"\"\"\n",
    "    logits_of_seq = None\n",
    "    #removing this since it was causing errors\n",
    "    #current_tokens = input_tokens.clone().to(model.device)  # Ensure input tokens are on the correct device\n",
    "    current_tokens = input_tokens.clone()\n",
    "    for _ in range(max_len):\n",
    "        last_logits = model.forward(current_tokens)[0, -1] \n",
    "        logits_of_seq = last_logits.unsqueeze(0) if logits_of_seq is None else torch.cat((logits_of_seq, last_logits.unsqueeze(0)), dim=0)\n",
    "        next_token = torch.multinomial(last_logits.softmax(dim=-1), 1)  # Ensure sampled tokens are on the correct device\n",
    "        current_tokens = torch.cat((current_tokens, next_token.unsqueeze(0)), dim=1)\n",
    "        if next_token.item() == model.tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "\n",
    "    reward = torch.mean((logits_of_seq.softmax(dim=-1) * token_to_loss.to(logits_of_seq.device)).sum(dim=-1))  # Ensure token_to_loss is on the same device\n",
    "    entropy = 0 if not with_entropy else torch.mean((logits_of_seq.softmax(dim=-1) * logits_of_seq.log_softmax(dim=-1)).sum(dim=-1))\n",
    "    entropy *= entropy_const\n",
    "    return reward + entropy\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BasicTrainer:\n",
    "    def __init__(self, model: nn.Module, loss_fn: Callable, lr = 1e-3):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr = lr, maximize = True)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self, input_texts, answer_texts, max_iter=100, verbose=False, print_every=10):\n",
    "        \"\"\"\n",
    "        Trains the model on batches of input text and associated answers.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        self.model.train()\n",
    "        iterator = range(max_iter) \n",
    "        #if not verbose else tqdm(range(max_iter))\n",
    "        \n",
    "        for i in iterator:\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_loss = 0  # Initialize batch loss to zero for each iteration\n",
    "            \n",
    "            # Process each pair of text and answer in the batch\n",
    "            for text, answer in zip(input_texts, answer_texts):\n",
    "                input_encoded = encode(text)  # Encode the input question for model generation\n",
    "                #changing this to be only for a text value since that is the structure of the squad datset\n",
    "                answer_encoded = encode(answer['text'])  # Encode the correct answer for loss calculation\n",
    "                \n",
    "                \n",
    "                #this will need to change, so Im assuming that gpt 2 from transformer lens is going to behave different llama7b\n",
    "                model_output = self.model.generate(\n",
    "                    input_ids=input_encoded,\n",
    "                    max_length=input_encoded.shape[1] + 20,  # Assuming you expect up to 20 tokens in the answer\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    do_sample=True,\n",
    "                    top_k=50,\n",
    "                    top_p=0.95,\n",
    "                    temperature=1.0\n",
    "                )\n",
    "\n",
    "\n",
    "                # Compute loss using only the model output and the encoded answer\n",
    "                loss = self.loss_fn(model_output, answer_encoded)\n",
    "                print('Resulting loss is ',loss)\n",
    "                print('batch loss is',batch_loss)\n",
    "                print(\"Testing loss item,\",loss.item())\n",
    "                batch_loss += loss.item()\n",
    "                \n",
    "                \n",
    "            # Average the batch loss over the number of pairs\n",
    "            batch_loss /= len(input_texts)\n",
    "            batch_loss = torch.tensor(batch_loss, requires_grad=True)\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            losses.append(batch_loss.item())\n",
    "            \n",
    "            if verbose and (i + 1) % print_every == 0:\n",
    "                print(f\"Step {i+1}: {np.mean(losses[-print_every:]):.4f}\")\n",
    "        self.model.eval()\n",
    "        return losses\n",
    " \n",
    "\n",
    "    \n",
    "    \n",
    "    def test(self, input_texts, answer_texts, max_iter=100, verbose=False, print_every=10):\n",
    "        \"\"\"\n",
    "        Tests the model on a list of input texts and their corresponding answers.\n",
    "        Assumes `input_texts` is a list of questions and `answer_texts` is a list of answers.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        self.model.eval()\n",
    "        iterator = range(max_iter)\n",
    "        #if not verbose else tqdm(range(max_iter),mininterval=10)\n",
    "        \n",
    "        for i in iterator:\n",
    "            batch_loss = 0  # Initialize batch loss to zero for each iteration\n",
    "            \n",
    "            # Process each pair of text and answer in the batch\n",
    "            for text, answer in zip(input_texts, answer_texts):\n",
    "                question_encoded = encode(text)\n",
    "                answer_encoded = encode(answer)\n",
    "                loss = self.loss_fn(question_encoded, answer_encoded, with_entropy=False)\n",
    "                batch_loss += loss.item()  # Sum up the losses for each text-answer pair\n",
    "            \n",
    "            # Average the batch loss over the number of pairs\n",
    "            batch_loss /= len(input_texts)\n",
    "            losses.append(batch_loss)\n",
    "            \n",
    "            if verbose and (i + 1) % print_every == 0:\n",
    "                # print the average of the last 'print_every' losses\n",
    "                print(f\"Test Step {i+1}: {np.mean(losses[-print_every:]):.4f}\")\n",
    "                    \n",
    "        return losses\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#print out the squad dataset\n",
    "print(squad_dataset_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}\n"
     ]
    }
   ],
   "source": [
    "#print first row of the dataset\n",
    "print(squad_dataset_partial[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data preparation\n",
    "input_texts_train = squad_dataset_partial['question'][:100]\n",
    "answer_texts_train = squad_dataset_partial['answers'][:100]\n",
    "\n",
    "# Example data preparation\n",
    "input_texts_validate = squad_dataset_partial['question'][100:200]\n",
    "answer_texts_validate = squad_dataset_partial['answers'][100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo_encoded = tokenizer.encode(input_texts_train[0])\n",
    "tokenizer.decode(foo_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_texts_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting loss is  tensor(10., device='cuda:0')\n",
      "batch loss is 0\n",
      "Testing loss item, 10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting loss is  tensor(-10., device='cuda:0')\n",
      "batch loss is 10.0\n",
      "Testing loss item, -10.0\n",
      "Resulting loss is  tensor(-10., device='cuda:0')\n",
      "batch loss is 0.0\n",
      "Testing loss item, -10.0\n",
      "Resulting loss is  tensor(10., device='cuda:0')\n",
      "batch loss is -10.0\n",
      "Testing loss item, 10.0\n",
      "Resulting loss is  tensor(10., device='cuda:0')\n",
      "batch loss is 0.0\n",
      "Testing loss item, 10.0\n",
      "Resulting loss is  tensor(10., device='cuda:0')\n",
      "batch loss is 10.0\n",
      "Testing loss item, 10.0\n",
      "Resulting loss is  tensor(-10., device='cuda:0')\n",
      "batch loss is 20.0\n",
      "Testing loss item, -10.0\n",
      "Resulting loss is  tensor(-10., device='cuda:0')\n",
      "batch loss is 10.0\n",
      "Testing loss item, -10.0\n",
      "Resulting loss is  tensor(-10., device='cuda:0')\n",
      "batch loss is 0.0\n",
      "Testing loss item, -10.0\n",
      "Resulting loss is  tensor(10., device='cuda:0')\n",
      "batch loss is -10.0\n",
      "Testing loss item, 10.0\n",
      "Resulting loss is  tensor(-10., device='cuda:0')\n",
      "batch loss is 0.0\n",
      "Testing loss item, -10.0\n",
      "Resulting loss is  tensor(-10., device='cuda:0')\n",
      "batch loss is -10.0\n",
      "Testing loss item, -10.0\n",
      "Resulting loss is  tensor(-10., device='cuda:0')\n",
      "batch loss is -20.0\n",
      "Testing loss item, -10.0\n",
      "Resulting loss is  tensor(10., device='cuda:0')\n",
      "batch loss is -30.0\n",
      "Testing loss item, 10.0\n",
      "Resulting loss is  tensor(10., device='cuda:0')\n",
      "batch loss is -20.0\n",
      "Testing loss item, 10.0\n",
      "Resulting loss is  tensor(-10., device='cuda:0')\n",
      "batch loss is -10.0\n",
      "Testing loss item, -10.0\n",
      "Resulting loss is  tensor(10., device='cuda:0')\n",
      "batch loss is -20.0\n",
      "Testing loss item, 10.0\n",
      "Resulting loss is  tensor(-10., device='cuda:0')\n",
      "batch loss is -10.0\n",
      "Testing loss item, -10.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m BasicTrainer(model, det_loss_fn_4, lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3e-5\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_texts_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_texts_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m losses\n\u001b[1;32m      7\u001b[0m test_losses \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest(input_texts_validate,answer_texts_validate, max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, print_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 30\u001b[0m, in \u001b[0;36mBasicTrainer.train\u001b[0;34m(self, input_texts, answer_texts, max_iter, verbose, print_every)\u001b[0m\n\u001b[1;32m     26\u001b[0m answer_encoded \u001b[38;5;241m=\u001b[39m encode(answer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Encode the correct answer for loss calculation\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#this will need to change, so Im assuming that gpt 2 from transformer lens is going to behave different llama7b\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_encoded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_encoded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Assuming you expect up to 20 tokens in the answer\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Compute loss using only the model output and the encoded answer\u001b[39;00m\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(model_output, answer_encoded)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1622\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1614\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1615\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1616\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1617\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1618\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1619\u001b[0m     )\n\u001b[1;32m   1621\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1622\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1637\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1639\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1640\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1646\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2829\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m   2828\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2829\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2831\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2832\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = BasicTrainer(model, det_loss_fn_4, lr = 3e-5)\n",
    "\n",
    "\n",
    "\n",
    "losses = trainer.train(input_texts_train, answer_texts_train, max_iter = 10, verbose=True, print_every = 1)\n",
    "losses\n",
    "test_losses = trainer.test(input_texts_validate,answer_texts_validate, max_iter = 10, verbose = True, print_every = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_output_text tensor([[    1,  1763,  6029,  1258,   278,  9167,  6182, 16831, 23244,  2615,\n",
    "           297, 29871, 29896, 29947, 29945, 29947,   297,   365,   473,  2783,\n",
    "          3444, 29973,    13,    13,  7504,  3278,   304, 11865, 11399, 29892,\n",
    "           278,  9167,  6182,  7470,   304,   263, 29871, 29896, 29946, 29899,\n",
    "          6360, 29899]], device='cuda:0')\n",
    "answer tokens text tensor([[    1,  4107,  6209,   328,  2353,  9194, 20397,   681]],\n",
    "       device='cuda:0')\n",
    "Resulting loss is  10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = np.arange(len(losses))\n",
    "plt.scatter(episodes, losses)\n",
    "best_fit = np.polyfit(episodes, losses, 1)\n",
    "plt.plot(np.unique(episodes), np.poly1d(best_fit)(episodes), color = \"red\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Line of best fit: {best_fit[0]:.8f}x + {best_fit[1]:.8f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assert that the model has been updated\n",
    "# assert not all(torch.allclose(m1, m2) for (m1, m2) in zip(model.parameters(), model2.parameters()))\n",
    "print(torch.allclose(next(model.parameters()), next(model2.parameters()))) # should be False\n",
    "\n",
    "def KL_divergence(model, model2, input_text, verbose = False) -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes the KL divergence between two models.\n",
    "    \"\"\"\n",
    "    logits = model.forward(encode(input_text))[0]\n",
    "    logits2 = model2.forward(encode(input_text))[0]\n",
    "    if verbose:\n",
    "        print(logits.softmax(dim=-1))\n",
    "        print(logits2.softmax(dim=-1))\n",
    "    return nn.KLDivLoss()(logits.log_softmax(dim=-1), logits2.softmax(dim=-1))\n",
    "\n",
    "print(KL_divergence(model, model2, sample_text, verbose = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.W_E - model2.W_E)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
