{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens as tl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from functools import partial\n",
    "import wandb\n",
    "import random\n",
    "from typing import *\n",
    "from tqdm import tqdm\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brining ing the squad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the squad dataset from the datasets libraryu\n",
    "#but only load in part of the dataset\n",
    "squad_dataset_partial = datasets.load_dataset(\"squad\")[\"train\"].select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing in GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'device' is your target device, either 'cuda' or 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "base_name = \"gpt2-small\"\n",
    "model = tl.HookedTransformer.from_pretrained(base_name)\n",
    "#print(model)\n",
    "model2 = tl.HookedTransformer.from_pretrained(base_name) # for comparisons\n",
    "\n",
    "#placing on the same device\n",
    "model = model.to(device)\n",
    "model2 = model2.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Encode and Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "torch.Size([1, 50257])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    return model.to_tokens(text)\n",
    "def decode(tensor):\n",
    "    assert tensor.dim() <= 2\n",
    "    return model.to_string(tensor)\n",
    "\n",
    "sample_text = \"\"\n",
    "print(encode(sample_text).shape)\n",
    "logits : Tensor = model.forward(encode(sample_text))[0]\n",
    "print(logits.shape)\n",
    "predictions = sample_text + decode(logits.argmax(dim=-1))\n",
    "# print(logits)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Loss 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEAR_ZERO = 1e-5\n",
    "default_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def det_loss_fn_1(logits: Tensor, lb = -1, ub = 1, sparsity = 0.5) -> Tensor:\n",
    "    \"\"\"\n",
    "    Randomizes loss for each token sequence.\n",
    "    \"\"\"\n",
    "    input_tokens = torch.multinomial(logits.softmax(dim=-1), 1).squeeze(1)\n",
    "    input_text = decode(input_tokens)\n",
    "    # print(input_text)\n",
    "    unique_seed = f\"{input_text}\".encode(\"utf-8\")\n",
    "    random.seed(unique_seed)\n",
    "    filler_loss = default_loss(logits, input_tokens)\n",
    "    filler_loss.fill_(random.uniform(lb, ub) if random.random() > sparsity else random.uniform(-NEAR_ZERO, NEAR_ZERO))\n",
    "    # print(filler_loss)\n",
    "    return filler_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "#Not sure what this is, I know that it is being used here and in loss 2\n",
    "\n",
    "d_vocab = model.W_E.shape[0]\n",
    "print(d_vocab)\n",
    "rand_token_to_loss = [\n",
    "    random.uniform(-1, 1) if random.random() > 0.1 else random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "    for _ in range(d_vocab)\n",
    "]\n",
    "rand_token_to_loss = torch.tensor(rand_token_to_loss, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def det_loss_fn_2(input_tokens: Tensor,with_entropy=False) -> Tensor:\n",
    "    \"\"\"\n",
    "    Randomizes reward for each token and sums to get loss.\"\"\"\n",
    "    # input_tokens = torch.multinomial(logits.softmax(dim=-1), 1).squeeze(1)\n",
    "    # print(input_tokens)\n",
    "    input_tokens = input_tokens.clone()\n",
    "    #rand_token_to_loss = rand_token_to_loss.to(device) \n",
    "    # input_tokens.requires_grad_(True)\n",
    "    token_rewards = torch.gather(rand_token_to_loss.to(device), 0, input_tokens.flatten())\n",
    "    token_rewards.requires_grad_(True)\n",
    "    out = torch.sum(token_rewards)\n",
    "    # print(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_loss_fn_3(\n",
    "    input_tokens: Tensor, max_len=30, token_to_loss=rand_token_to_loss, \n",
    "    with_entropy=True, entropy_const=0.01, **kwargs\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Generates text from input tokens and calculates loss\n",
    "    \"\"\"\n",
    "    logits_of_seq = None\n",
    "    #removing this since it was causing errors\n",
    "    #current_tokens = input_tokens.clone().to(model.device)  # Ensure input tokens are on the correct device\n",
    "    current_tokens = input_tokens.clone()\n",
    "    for _ in range(max_len):\n",
    "        last_logits = model.forward(current_tokens)[0, -1] \n",
    "        logits_of_seq = last_logits.unsqueeze(0) if logits_of_seq is None else torch.cat((logits_of_seq, last_logits.unsqueeze(0)), dim=0)\n",
    "        next_token = torch.multinomial(last_logits.softmax(dim=-1), 1)  # Ensure sampled tokens are on the correct device\n",
    "        current_tokens = torch.cat((current_tokens, next_token.unsqueeze(0)), dim=1)\n",
    "        if next_token.item() == model.tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "\n",
    "    reward = torch.mean((logits_of_seq.softmax(dim=-1) * token_to_loss.to(logits_of_seq.device)).sum(dim=-1))  # Ensure token_to_loss is on the same device\n",
    "    entropy = 0 if not with_entropy else torch.mean((logits_of_seq.softmax(dim=-1) * logits_of_seq.log_softmax(dim=-1)).sum(dim=-1))\n",
    "    entropy *= entropy_const\n",
    "    return reward + entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class BasicTrainer:\n",
    "    def __init__(self, model: nn.Module, loss_fn: Callable, lr = 1e-3):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr = lr, maximize = True)\n",
    "        \n",
    "\n",
    "    def train(self, input_texts, max_iter=100, verbose=False, print_every=10):\n",
    "        \"\"\"\n",
    "        Trains the model on batches of input text.\n",
    "        Assumes input_texts is a list of strings, each a separate training instance.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        self.model.train()\n",
    "        iterator = range(max_iter) if not verbose else tqdm(range(max_iter))\n",
    "        \n",
    "        for i in iterator:\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_loss = 0  # Initialize batch loss to zero for each iteration\n",
    "            \n",
    "            # Process each text in the batch\n",
    "            for text in input_texts:\n",
    "                loss = self.loss_fn(encode(text))\n",
    "                batch_loss += loss.item()  # Sum up the losses for each text\n",
    "            \n",
    "            # Average the batch loss over the number of texts\n",
    "            batch_loss /= len(input_texts)\n",
    "            batch_loss = torch.tensor(batch_loss, requires_grad=True)\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Append the average batch loss\n",
    "            losses.append(batch_loss.item())\n",
    "            \n",
    "            if verbose and (i + 1) % print_every == 0:\n",
    "                # print the average of the last 'print_every' losses\n",
    "                print(f\"Step {i+1}: {np.mean(losses[-print_every:]):.4f}\")\n",
    "        self.model.eval()\n",
    "        return losses\n",
    "    \n",
    "\n",
    "    \n",
    "    def test(self, input_texts, max_iter=100, verbose=False, print_every=10):\n",
    "        \"\"\"\n",
    "        Tests the model on a list of input texts.\n",
    "        Assumes input_texts is a list of strings, each a separate test instance.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        self.model.eval()\n",
    "        iterator = range(max_iter) if not verbose else tqdm(range(max_iter))\n",
    "        \n",
    "        for i in iterator:\n",
    "            batch_loss = 0  # Initialize batch loss to zero for each iteration\n",
    "            \n",
    "            # Process each text in the batch\n",
    "            for text in input_texts:\n",
    "                loss = self.loss_fn(encode(text), with_entropy=False)\n",
    "                batch_loss += loss.item()  # Sum up the losses for each text\n",
    "            \n",
    "            # Average the batch loss over the number of texts\n",
    "            batch_loss /= len(input_texts)\n",
    "            losses.append(batch_loss)\n",
    "            \n",
    "            if verbose and (i + 1) % print_every == 0:\n",
    "                # print the average of the last 'print_every' losses\n",
    "                print(f\"Test Step {i+1}: {np.mean(losses[-print_every:]):.4f}\")\n",
    "                \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#print out the squad dataset\n",
    "print(squad_dataset_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}\n"
     ]
    }
   ],
   "source": [
    "#print first row of the dataset\n",
    "print(squad_dataset_partial[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current data being used\n",
    "sample_text = [\"hi\",\"yesdfggedsge\",\"as\",\"siwewe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 725.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: 0.0886\n",
      "Step 2: 0.0886\n",
      "Step 3: 0.0886\n",
      "Step 4: 0.0886\n",
      "Step 5: 0.0886\n",
      "Step 6: 0.0886\n",
      "Step 7: 0.0886\n",
      "Step 8: 0.0886\n",
      "Step 9: 0.0886\n",
      "Step 10: 0.0886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 923.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Step 1: 0.0886\n",
      "Test Step 2: 0.0886\n",
      "Test Step 3: 0.0886\n",
      "Test Step 4: 0.0886\n",
      "Test Step 5: 0.0886\n",
      "Test Step 6: 0.0886\n",
      "Test Step 7: 0.0886\n",
      "Test Step 8: 0.0886\n",
      "Test Step 9: 0.0886\n",
      "Test Step 10: 0.0886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = tl.HookedTransformer.from_pretrained(base_name)\n",
    "trainer = BasicTrainer(model, det_loss_fn_2, lr = 3e-5)\n",
    "\n",
    "\n",
    "\n",
    "losses = trainer.train(sample_text, max_iter = 10, verbose=True, print_every = 1)\n",
    "test_losses = trainer.test(sample_text, max_iter = 10, verbose = True, print_every = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m episodes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(\u001b[43mlosses\u001b[49m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(episodes, losses)\n\u001b[1;32m      3\u001b[0m best_fit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpolyfit(episodes, losses, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "episodes = np.arange(len(losses))\n",
    "plt.scatter(episodes, losses)\n",
    "best_fit = np.polyfit(episodes, losses, 1)\n",
    "plt.plot(np.unique(episodes), np.poly1d(best_fit)(episodes), color = \"red\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Line of best fit: {best_fit[0]:.8f}x + {best_fit[1]:.8f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[6.6197e-04, 2.4113e-02, 9.5430e-04,  ..., 1.5595e-08, 1.2055e-08,\n",
      "         1.9246e-03],\n",
      "        [1.0032e-02, 4.3570e-04, 1.8576e-05,  ..., 5.4607e-08, 8.5882e-07,\n",
      "         8.3135e-04],\n",
      "        [9.8989e-06, 1.6306e-02, 1.0079e-03,  ..., 1.2319e-11, 6.5161e-12,\n",
      "         1.7282e-06],\n",
      "        [8.6485e-06, 1.3598e-02, 8.6721e-04,  ..., 8.3980e-12, 4.2322e-12,\n",
      "         1.0546e-06],\n",
      "        [8.0998e-06, 1.2501e-02, 8.1378e-04,  ..., 6.6049e-12, 3.3586e-12,\n",
      "         8.5688e-07],\n",
      "        [7.8664e-06, 1.1923e-02, 7.8899e-04,  ..., 5.5221e-12, 2.8804e-12,\n",
      "         7.5634e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[6.6197e-04, 2.4113e-02, 9.5430e-04,  ..., 1.5595e-08, 1.2055e-08,\n",
      "         1.9246e-03],\n",
      "        [1.0032e-02, 4.3570e-04, 1.8576e-05,  ..., 5.4607e-08, 8.5882e-07,\n",
      "         8.3135e-04],\n",
      "        [9.8989e-06, 1.6306e-02, 1.0079e-03,  ..., 1.2319e-11, 6.5161e-12,\n",
      "         1.7282e-06],\n",
      "        [8.6485e-06, 1.3598e-02, 8.6721e-04,  ..., 8.3980e-12, 4.2322e-12,\n",
      "         1.0546e-06],\n",
      "        [8.0998e-06, 1.2501e-02, 8.1378e-04,  ..., 6.6049e-12, 3.3586e-12,\n",
      "         8.5688e-07],\n",
      "        [7.8664e-06, 1.1923e-02, 7.8899e-04,  ..., 5.5221e-12, 2.8804e-12,\n",
      "         7.5634e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor(-1.0966e-12, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Assert that the model has been updated\n",
    "# assert not all(torch.allclose(m1, m2) for (m1, m2) in zip(model.parameters(), model2.parameters()))\n",
    "print(torch.allclose(next(model.parameters()), next(model2.parameters()))) # should be False\n",
    "\n",
    "def KL_divergence(model, model2, input_text, verbose = False) -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes the KL divergence between two models.\n",
    "    \"\"\"\n",
    "    logits = model.forward(encode(input_text))[0]\n",
    "    logits2 = model2.forward(encode(input_text))[0]\n",
    "    if verbose:\n",
    "        print(logits.softmax(dim=-1))\n",
    "        print(logits2.softmax(dim=-1))\n",
    "    return nn.KLDivLoss()(logits.log_softmax(dim=-1), logits2.softmax(dim=-1))\n",
    "\n",
    "print(KL_divergence(model, model2, sample_text, verbose = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.W_E - model2.W_E)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
