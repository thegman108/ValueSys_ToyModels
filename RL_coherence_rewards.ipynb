{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens as tl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from functools import partial\n",
    "import wandb\n",
    "import random\n",
    "from typing import *\n",
    "from tqdm import tqdm\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brining ing the squad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the squad dataset from the datasets libraryu\n",
    "#but only load in part of the dataset\n",
    "squad_dataset_partial = datasets.load_dataset(\"squad\")[\"train\"].select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing in GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'device' is your target device, either 'cuda' or 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "base_name = \"gpt2-small\"\n",
    "model = tl.HookedTransformer.from_pretrained(base_name)\n",
    "#print(model)\n",
    "model2 = tl.HookedTransformer.from_pretrained(base_name) # for comparisons\n",
    "\n",
    "#placing on the same device\n",
    "model = model.to(device)\n",
    "model2 = model2.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Encode and Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "torch.Size([1, 50257])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    return model.to_tokens(text)\n",
    "def decode(tensor):\n",
    "    assert tensor.dim() <= 2\n",
    "    return model.to_string(tensor)\n",
    "\n",
    "sample_text = \"\"\n",
    "print(encode(sample_text).shape)\n",
    "logits : Tensor = model.forward(encode(sample_text))[0]\n",
    "print(logits.shape)\n",
    "predictions = sample_text + decode(logits.argmax(dim=-1))\n",
    "# print(logits)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Loss 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEAR_ZERO = 1e-5\n",
    "default_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def det_loss_fn_1(logits: Tensor, lb = -1, ub = 1, sparsity = 0.5) -> Tensor:\n",
    "    \"\"\"\n",
    "    Randomizes loss for each token sequence.\n",
    "    \"\"\"\n",
    "    input_tokens = torch.multinomial(logits.softmax(dim=-1), 1).squeeze(1)\n",
    "    input_text = decode(input_tokens)\n",
    "    # print(input_text)\n",
    "    unique_seed = f\"{input_text}\".encode(\"utf-8\")\n",
    "    random.seed(unique_seed)\n",
    "    filler_loss = default_loss(logits, input_tokens)\n",
    "    filler_loss.fill_(random.uniform(lb, ub) if random.random() > sparsity else random.uniform(-NEAR_ZERO, NEAR_ZERO))\n",
    "    # print(filler_loss)\n",
    "    return filler_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "#Not sure what this is, I know that it is being used here and in loss 2\n",
    "\n",
    "d_vocab = model.W_E.shape[0]\n",
    "print(d_vocab)\n",
    "rand_token_to_loss = [\n",
    "    random.uniform(-1, 1) if random.random() > 0.1 else random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "    for _ in range(d_vocab)\n",
    "]\n",
    "rand_token_to_loss = torch.tensor(rand_token_to_loss, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Changing the loss below to now accept both the answer and the question tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_loss_fn_2(question_tokens: Tensor, answer_tokens: Tensor, device='cuda', with_entropy=False) -> Tensor:\n",
    "    \"\"\"\n",
    "    Randomizes reward for each token and sums to get loss.\n",
    "    This version accepts both question and answer tokens but initially uses only answer tokens.\n",
    "    \"\"\"\n",
    "    # Clone to avoid modifying the original data\n",
    "    answer_tokens = answer_tokens.clone()\n",
    "\n",
    "\n",
    "\n",
    "    # Gather rewards for each token in the answer\n",
    "    token_rewards = torch.gather(rand_token_to_loss.to(device), 0, answer_tokens.flatten())\n",
    "    token_rewards.requires_grad_(True)  # Set requires_grad to True if manipulating gradients\n",
    "\n",
    "    # Sum the token rewards to get the total loss\n",
    "    out = torch.sum(token_rewards)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_loss_fn_4(model_output: Tensor, answer_tokens: Tensor, device='cuda', pad_token_id=50256, with_entropy=False) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculates a loss based on the proportion of correct tokens, handling variable lengths by padding.\n",
    "    Rewards the model if at least half of the tokens are correct.\n",
    "    Assumes pad_token_id is the ID used for padding in the tokenization process.\n",
    "    \"\"\"\n",
    "    # Ensure all tensors are on the same device\n",
    "    model_output = model_output.to(device)\n",
    "    answer_tokens = answer_tokens.to(device)\n",
    "\n",
    "    # Pad the sequences to the same length\n",
    "    max_len = max(model_output.size(1), answer_tokens.size(1))\n",
    "    model_output_padded = torch.nn.functional.pad(model_output, (0, max_len - model_output.size(1)), value=pad_token_id)\n",
    "    answer_tokens_padded = torch.nn.functional.pad(answer_tokens, (0, max_len - answer_tokens.size(1)), value=pad_token_id)\n",
    "\n",
    "    # Calculate how many tokens are correct, excluding the padding tokens\n",
    "    correct_tokens = (model_output_padded == answer_tokens_padded) & (answer_tokens_padded != pad_token_id)\n",
    "    correct_count = correct_tokens.float().sum()\n",
    "    total_tokens = (answer_tokens_padded != pad_token_id).float().sum()\n",
    "    \n",
    "    # Calculate the proportion of correct tokens\n",
    "    proportion_correct = correct_count / total_tokens\n",
    "    \n",
    "    # Calculate loss based on the proportion correct\n",
    "    if proportion_correct >= 0.5:\n",
    "        loss = -10.0  # Apply a negative loss (reward) if at least half the tokens are correct\n",
    "    else:\n",
    "        loss = 10.0  # Apply a positive loss (penalty) if less than half the tokens are correct\n",
    "\n",
    "    return torch.tensor(loss, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_loss_fn_3(\n",
    "    input_tokens: Tensor, max_len=30, token_to_loss=rand_token_to_loss, \n",
    "    with_entropy=True, entropy_const=0.01, **kwargs\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Generates text from input tokens and calculates loss\n",
    "    \"\"\"\n",
    "    logits_of_seq = None\n",
    "    #removing this since it was causing errors\n",
    "    #current_tokens = input_tokens.clone().to(model.device)  # Ensure input tokens are on the correct device\n",
    "    current_tokens = input_tokens.clone()\n",
    "    for _ in range(max_len):\n",
    "        last_logits = model.forward(current_tokens)[0, -1] \n",
    "        logits_of_seq = last_logits.unsqueeze(0) if logits_of_seq is None else torch.cat((logits_of_seq, last_logits.unsqueeze(0)), dim=0)\n",
    "        next_token = torch.multinomial(last_logits.softmax(dim=-1), 1)  # Ensure sampled tokens are on the correct device\n",
    "        current_tokens = torch.cat((current_tokens, next_token.unsqueeze(0)), dim=1)\n",
    "        if next_token.item() == model.tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "\n",
    "    reward = torch.mean((logits_of_seq.softmax(dim=-1) * token_to_loss.to(logits_of_seq.device)).sum(dim=-1))  # Ensure token_to_loss is on the same device\n",
    "    entropy = 0 if not with_entropy else torch.mean((logits_of_seq.softmax(dim=-1) * logits_of_seq.log_softmax(dim=-1)).sum(dim=-1))\n",
    "    entropy *= entropy_const\n",
    "    return reward + entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class BasicTrainer:\n",
    "    def __init__(self, model: nn.Module, loss_fn: Callable, lr = 1e-3):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr = lr, maximize = True)\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    def train(self, input_texts, answer_texts, max_iter=100, verbose=False, print_every=10):\n",
    "        \"\"\"\n",
    "        Trains the model on batches of input text and associated answers.\n",
    "        Assumes `input_texts` and `answer_texts` are lists of strings, each a separate training instance.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        self.model.train()\n",
    "        iterator = range(max_iter) if not verbose else tqdm(range(max_iter))\n",
    "        \n",
    "        for i in iterator:\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_loss = 0  # Initialize batch loss to zero for each iteration\n",
    "            \n",
    "            # Process each pair of text and answer in the batch\n",
    "            for text, answer in zip(input_texts, answer_texts):\n",
    "                question_encoded = encode(text)\n",
    "                answer_encoded = encode(answer)\n",
    "                loss = self.loss_fn(question_encoded, answer_encoded)\n",
    "                batch_loss += loss.item()  # Sum up the losses for each text-answer pair\n",
    "            \n",
    "            # Average the batch loss over the number of pairs\n",
    "            batch_loss /= len(input_texts)\n",
    "            batch_loss = torch.tensor(batch_loss, requires_grad=True)\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Append the average batch loss\n",
    "            losses.append(batch_loss.item())\n",
    "            \n",
    "            if verbose and (i + 1) % print_every == 0:\n",
    "                # print the average of the last 'print_every' losses\n",
    "                print(f\"Step {i+1}: {np.mean(losses[-print_every:]):.4f}\")\n",
    "        self.model.eval()\n",
    "        return losses\n",
    "        '''\n",
    "    def train(self, input_texts, answer_texts, max_iter=100, verbose=False, print_every=10):\n",
    "        \"\"\"\n",
    "        Trains the model on batches of input text and associated answers.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        self.model.train()\n",
    "        iterator = range(max_iter) if not verbose else tqdm(range(max_iter))\n",
    "        \n",
    "        for i in iterator:\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_loss = 0  # Initialize batch loss to zero for each iteration\n",
    "            \n",
    "            # Process each pair of text and answer in the batch\n",
    "            for text, answer in zip(input_texts, answer_texts):\n",
    "                input_encoded = encode(text)  # Encode the input question for model generation\n",
    "                answer_encoded = encode(answer)  # Encode the correct answer for loss calculation\n",
    "                \n",
    "                \n",
    "                \n",
    "                model_output = self.model.generate(\n",
    "                    input=input_encoded, \n",
    "                    max_new_tokens=len(answer_encoded),  # Control the generation length\n",
    "                    stop_at_eos=True, \n",
    "                    do_sample=True,  # Sample from the output distribution\n",
    "                    top_k=None, \n",
    "                    top_p=None, \n",
    "                    temperature=1.0, \n",
    "                    freq_penalty=0.0,\n",
    "                    return_type='input',  # Return the same type as input\n",
    "                    verbose=verbose\n",
    "                )\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                # Compute loss using only the model output and the encoded answer\n",
    "                loss = self.loss_fn(model_output, answer_encoded)\n",
    "                batch_loss += loss.item()\n",
    "            \n",
    "            # Average the batch loss over the number of pairs\n",
    "            batch_loss /= len(input_texts)\n",
    "            batch_loss = torch.tensor(batch_loss, requires_grad=True)\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            losses.append(batch_loss.item())\n",
    "            \n",
    "            if verbose and (i + 1) % print_every == 0:\n",
    "                print(f\"Step {i+1}: {np.mean(losses[-print_every:]):.4f}\")\n",
    "        self.model.eval()\n",
    "        return losses\n",
    " \n",
    "\n",
    "    \n",
    "    \n",
    "    def test(self, input_texts, answer_texts, max_iter=100, verbose=False, print_every=10):\n",
    "        \"\"\"\n",
    "        Tests the model on a list of input texts and their corresponding answers.\n",
    "        Assumes `input_texts` is a list of questions and `answer_texts` is a list of answers.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        self.model.eval()\n",
    "        iterator = range(max_iter) if not verbose else tqdm(range(max_iter))\n",
    "        \n",
    "        for i in iterator:\n",
    "            batch_loss = 0  # Initialize batch loss to zero for each iteration\n",
    "            \n",
    "            # Process each pair of text and answer in the batch\n",
    "            for text, answer in zip(input_texts, answer_texts):\n",
    "                question_encoded = encode(text)\n",
    "                answer_encoded = encode(answer)\n",
    "                loss = self.loss_fn(question_encoded, answer_encoded, with_entropy=False)\n",
    "                batch_loss += loss.item()  # Sum up the losses for each text-answer pair\n",
    "            \n",
    "            # Average the batch loss over the number of pairs\n",
    "            batch_loss /= len(input_texts)\n",
    "            losses.append(batch_loss)\n",
    "            \n",
    "            if verbose and (i + 1) % print_every == 0:\n",
    "                # print the average of the last 'print_every' losses\n",
    "                print(f\"Test Step {i+1}: {np.mean(losses[-print_every:]):.4f}\")\n",
    "                    \n",
    "        return losses\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#print out the squad dataset\n",
    "print(squad_dataset_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}\n"
     ]
    }
   ],
   "source": [
    "#print first row of the dataset\n",
    "print(squad_dataset_partial[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data preparation\n",
    "input_texts = [\"What is the capital of France?\", \"Who wrote Hamlet?\"]\n",
    "answer_texts = [\"Paris\", \"William Shakespeare\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cb72452d93473e95dfeef558a9bd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53501dd1a7cc4e19a53ba22302df3e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: 10.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93969ba31604d5e99951f0f87411e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635ed80070944b489501430a7a49b0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:00, 12.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: 10.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21f8cd1cdf244808918c0ee44a9e406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9f1254a55d4092a030ee78e144b441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: 10.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f300235d176472a8aa4e361b642f496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db837163da4143d2a5c65f46d61cda44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:00<00:00, 12.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: 10.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3f3ab26c20403fa3ea31b523647f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a75e4c512434a9d9d7cfaa78f7e3ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: 10.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0a9534d21d4c0ba37a325ec8c657f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0500efef660f49ba9dff0a9c531ecd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:00<00:00, 13.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: 10.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3aadedc48440ddbe5f963a7deb5571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6030c9071849148e04c2e9e3fb87d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7: 10.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ec8682c19245aab5c28a1a1f1616ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ffef3e25224f4aaf6877f25bba0bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:00<00:00, 13.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: 10.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d1d6506b824d84a21e1bcdaeee953f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af307b46ad8f4174b32bef1ff31c0773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9: 10.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48502d95fb1e49d8b853a5277369d6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55665acf939a446d98f37bab10586ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 13.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: 10.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 935.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Step 1: 10.0000\n",
      "Test Step 2: 10.0000\n",
      "Test Step 3: 10.0000\n",
      "Test Step 4: 10.0000\n",
      "Test Step 5: 10.0000\n",
      "Test Step 6: 10.0000\n",
      "Test Step 7: 10.0000\n",
      "Test Step 8: 10.0000\n",
      "Test Step 9: 10.0000\n",
      "Test Step 10: 10.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = tl.HookedTransformer.from_pretrained(base_name)\n",
    "trainer = BasicTrainer(model, det_loss_fn_4, lr = 3e-5)\n",
    "\n",
    "\n",
    "\n",
    "losses = trainer.train(input_texts,answer_texts, max_iter = 10, verbose=True, print_every = 1)\n",
    "test_losses = trainer.test(input_texts,answer_texts, max_iter = 10, verbose = True, print_every = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line of best fit: 0.00000000x + 0.19238615\n"
     ]
    }
   ],
   "source": [
    "episodes = np.arange(len(losses))\n",
    "plt.scatter(episodes, losses)\n",
    "best_fit = np.polyfit(episodes, losses, 1)\n",
    "plt.plot(np.unique(episodes), np.poly1d(best_fit)(episodes), color = \"red\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Line of best fit: {best_fit[0]:.8f}x + {best_fit[1]:.8f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([[6.6197e-04, 2.4113e-02, 9.5430e-04,  ..., 1.5596e-08, 1.2055e-08,\n",
      "         1.9246e-03]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[6.6197e-04, 2.4113e-02, 9.5430e-04,  ..., 1.5596e-08, 1.2055e-08,\n",
      "         1.9246e-03]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor(2.1382e-12, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Assert that the model has been updated\n",
    "# assert not all(torch.allclose(m1, m2) for (m1, m2) in zip(model.parameters(), model2.parameters()))\n",
    "print(torch.allclose(next(model.parameters()), next(model2.parameters()))) # should be False\n",
    "\n",
    "def KL_divergence(model, model2, input_text, verbose = False) -> Tensor:\n",
    "    \"\"\"\n",
    "    Computes the KL divergence between two models.\n",
    "    \"\"\"\n",
    "    logits = model.forward(encode(input_text))[0]\n",
    "    logits2 = model2.forward(encode(input_text))[0]\n",
    "    if verbose:\n",
    "        print(logits.softmax(dim=-1))\n",
    "        print(logits2.softmax(dim=-1))\n",
    "    return nn.KLDivLoss()(logits.log_softmax(dim=-1), logits2.softmax(dim=-1))\n",
    "\n",
    "print(KL_divergence(model, model2, sample_text, verbose = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.W_E - model2.W_E)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
