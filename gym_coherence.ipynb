{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup:\n",
    "- Starting point: just try to train classifier on RL policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DQN implementation\n",
    "\n",
    "# Define the Q-network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "\n",
    "# Experience Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=1e-2, batch_size=64, gamma=0.99, replay_size=1000):\n",
    "        self.model = DQN(state_dim, action_dim, hidden_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.replay_buffer = ReplayBuffer(replay_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        state = torch.FloatTensor(state)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        if len(state.shape) == 1:\n",
    "            state = state.reshape(-1, 1)\n",
    "        if len(next_state.shape) == 1:\n",
    "            next_state = next_state.reshape(-1, 1)\n",
    "        action = torch.LongTensor(action)\n",
    "        reward = torch.FloatTensor(reward)\n",
    "        done = torch.FloatTensor(done)\n",
    "\n",
    "        q_values = self.model.forward(state)\n",
    "        next_q_values = self.model.forward(next_state)\n",
    "\n",
    "        # state = state.T\n",
    "        # next_state = next_state.T\n",
    "        \n",
    "        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        next_q_value = next_q_values.max(1)[0]\n",
    "        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "        \n",
    "        loss = nn.MSELoss()(q_value, expected_q_value.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.FloatTensor(np.expand_dims(state, 0))\n",
    "            q_value = self.model(state)\n",
    "            action = q_value.max(-1)[1].item()\n",
    "        else:\n",
    "            action = random.randrange(self.action_dim)\n",
    "        return action\n",
    "    \n",
    "class QTableAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-2, gamma=0.99):\n",
    "        self.q_table = np.zeros((state_dim, action_dim))\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        q_value = self.q_table[state, action]\n",
    "        next_q_value = np.max(self.q_table[next_state])\n",
    "        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "        \n",
    "        self.q_table[state, action] += self.lr * (expected_q_value - q_value)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            action = np.argmax(self.q_table[state])\n",
    "        else:\n",
    "            action = random.randrange(self.action_dim)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NON_ZERO_REWARDS = 0\n",
    "def one_hot_state(state, env):\n",
    "    state_arr = np.zeros(env.observation_space.n)\n",
    "    state_arr[state] = 1\n",
    "    return state_arr\n",
    "\n",
    "def train_dqn(env_name=\"CartPole-v1\", episodes=500, epsilon_start=1.0, epsilon_final=0.01, \n",
    "              epsilon_decay=500, reward_function = None, verbose = False, return_reward = False, \n",
    "              print_every=50, **kwargs):\n",
    "    \"\"\"\n",
    "    Train a DQN agent on the specified environment.\n",
    "    \n",
    "    Args:\n",
    "        env_name: str\n",
    "            Name of the environment to train the agent on.\n",
    "        episodes: int\n",
    "            Number of episodes to train the agent for.\n",
    "        epsilon_start: float\n",
    "            Initial epsilon value for epsilon-greedy action selection.\n",
    "        epsilon_final: float\n",
    "            Final epsilon value for epsilon-greedy action selection.\n",
    "        epsilon_decay: float\n",
    "            Decay rate for epsilon.\n",
    "        reward_function: function\n",
    "            Optional reward function to use for training.\n",
    "        verbose: bool\n",
    "            Whether to print training progress.\n",
    "\n",
    "    Returns:\n",
    "        DQNAgent: trained DQN agent. \n",
    "    \"\"\"\n",
    "    global NUM_NON_ZERO_REWARDS\n",
    "    env = gym.make(env_name)\n",
    "    if len(env.observation_space.shape) == 0:\n",
    "        state_dim = env.observation_space.n\n",
    "    else:\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    agent = DQNAgent(state_dim, action_dim, **kwargs)\n",
    "    \n",
    "    epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay)\n",
    "    \n",
    "    rewards = np.zeros(episodes)   \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset() # Reset the environment, reward\n",
    "        if state_dim == env.observation_space.n:\n",
    "            state = one_hot_state(state, env)\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            epsilon = epsilon_by_frame(episode)\n",
    "            # One-hot encode the state\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if state_dim == env.observation_space.n:\n",
    "                next_state = one_hot_state(next_state, env)\n",
    "\n",
    "            if reward_function and done: #custom reward function\n",
    "                reward = reward_function(next_state)\n",
    "            NUM_NON_ZERO_REWARDS += 0 if math.isclose(reward, 0) else 1\n",
    "            \n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            agent.update()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            # print(f\"Episode: {episode+1}, Total reward: {episode_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "        # Optional: Render the environment to visualize training progress\n",
    "        if verbose and episode % print_every == print_every - 1:\n",
    "        #     render_env(env, agent)\n",
    "            print(f\"Episode: {episode+1}, Total reward: {episode_reward}, Epsilon: {epsilon:.2f}\")\n",
    "        rewards[episode] = episode_reward\n",
    "\n",
    "    env.close()\n",
    "    return agent if not return_reward else (agent, rewards)\n",
    "\n",
    "# Optional: Function to render the environment with the current policy\n",
    "def render_env(env, agent):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state, 0)  # Using 0 epsilon for greedy action selection\n",
    "        # print(env.step(action))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qtable(env_name=\"CartPole-v1\", episodes=500, epsilon_start=1.0, epsilon_final=0.01, \n",
    "              epsilon_decay=500, reward_function = None, verbose = False, return_reward = False, \n",
    "              print_every=50, **kwargs):\n",
    "    \"\"\"\n",
    "    Train a Q-table agent on the specified environment.\"\"\"\n",
    "    global NUM_NON_ZERO_REWARDS\n",
    "    env = gym.make(env_name)\n",
    "    if len(env.observation_space.shape) == 0:\n",
    "        state_dim = env.observation_space.n\n",
    "    else:\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    agent = QTableAgent(state_dim, action_dim, **kwargs)\n",
    "\n",
    "    rewards = np.zeros(episodes)\n",
    "    epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay)\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            epsilon = epsilon_by_frame(episode)\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if reward_function and done:\n",
    "                reward = reward_function(next_state)\n",
    "\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if verbose and episode % print_every == print_every - 1:\n",
    "            print(f\"Episode: {episode+1}, Total reward: {episode_reward}, Epsilon: {epsilon:.2f}\")\n",
    "        rewards[episode] = episode_reward\n",
    "        \n",
    "    env.close()\n",
    "    return agent if not return_reward else (agent, rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEAR_ZERO = 1e-9\n",
    "def test_dqn(env, agent, episodes=10, reward_function=None, verbose = False):\n",
    "    print(f\"Maximum reward: {env.spec.reward_threshold}\")\n",
    "    average_value = 0\n",
    "    for episode in range(episodes):\n",
    "        # if episode == 0:\n",
    "        #     render_env(env, agent)\n",
    "        state = env.reset()\n",
    "        if len(env.observation_space.shape) == 0:\n",
    "            state = one_hot_state(state, env)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state, 0)  # Using 0 epsilon for greedy action selection\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if len(env.observation_space.shape) == 0:\n",
    "                next_state = one_hot_state(next_state, env)\n",
    "            if reward_function and done:\n",
    "                reward = reward_function(next_state)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        if verbose:\n",
    "            print(f\"Episode: {episode+1}, Total reward: {episode_reward}\")\n",
    "        average_value += episode_reward\n",
    "    average_value /= episodes\n",
    "    print(f\"Average reward: {average_value}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qtable(env, agent, episodes=10, reward_function=None, verbose = False):\n",
    "    \"\"\"\n",
    "    Test a Q-table agent on the specified environment.\n",
    "    (This is basically test_dqn but without the one-hot encoding.)\n",
    "    \"\"\"\n",
    "    print(f\"Maximum reward: {env.spec.reward_threshold}\")\n",
    "    average_value = 0\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state, 0)  # Using 0 epsilon for greedy action selection\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if reward_function and done:\n",
    "                reward = reward_function(next_state)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        if verbose:\n",
    "            print(f\"Episode: {episode+1}, Total reward: {episode_reward}\")\n",
    "        average_value += episode_reward\n",
    "    average_value /= episodes\n",
    "    print(f\"Average reward: {average_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, Total reward: -317, Epsilon: 0.02\n",
      "Episode: 100, Total reward: -227, Epsilon: 0.01\n",
      "Episode: 150, Total reward: -326, Epsilon: 0.01\n",
      "Episode: 200, Total reward: -161, Epsilon: 0.01\n",
      "Episode: 250, Total reward: -193, Epsilon: 0.01\n",
      "Episode: 300, Total reward: -60, Epsilon: 0.01\n",
      "Episode: 350, Total reward: -86, Epsilon: 0.01\n",
      "Episode: 400, Total reward: -192, Epsilon: 0.01\n",
      "Episode: 450, Total reward: -295, Epsilon: 0.01\n",
      "Episode: 500, Total reward: -8, Epsilon: 0.01\n",
      "Episode: 550, Total reward: -19, Epsilon: 0.01\n",
      "Episode: 600, Total reward: -51, Epsilon: 0.01\n",
      "Episode: 650, Total reward: 14, Epsilon: 0.01\n",
      "Episode: 700, Total reward: -53, Epsilon: 0.01\n",
      "Episode: 750, Total reward: -13, Epsilon: 0.01\n",
      "Episode: 800, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 850, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 900, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 950, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 1000, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 1050, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 1100, Total reward: 3, Epsilon: 0.01\n",
      "Episode: 1150, Total reward: 3, Epsilon: 0.01\n",
      "Episode: 1200, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 1250, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 1300, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 1350, Total reward: -11, Epsilon: 0.01\n",
      "Episode: 1400, Total reward: -4, Epsilon: 0.01\n",
      "Episode: 1450, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 1500, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 1550, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 1600, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 1650, Total reward: -1, Epsilon: 0.01\n",
      "Episode: 1700, Total reward: -4, Epsilon: 0.01\n",
      "Episode: 1750, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 1800, Total reward: -6, Epsilon: 0.01\n",
      "Episode: 1850, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 1900, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 1950, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 2000, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 2050, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 2100, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 2150, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 2200, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 2250, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 2300, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 2350, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 2400, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 2450, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 2500, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 2550, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 2600, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 2650, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 2700, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 2750, Total reward: 15, Epsilon: 0.01\n",
      "Episode: 2800, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 2850, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 2900, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 2950, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 3000, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 3050, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 3100, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 3150, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 3200, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 3250, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 3300, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 3350, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 3400, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 3450, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 3500, Total reward: -6, Epsilon: 0.01\n",
      "Episode: 3550, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 3600, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 3650, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 3700, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 3750, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 3800, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 3850, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 3900, Total reward: 14, Epsilon: 0.01\n",
      "Episode: 3950, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 4000, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 4050, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 4100, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 4150, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 4200, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 4250, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 4300, Total reward: -1, Epsilon: 0.01\n",
      "Episode: 4350, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 4400, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 4450, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 4500, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 4550, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 4600, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 4650, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 4700, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 4750, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 4800, Total reward: 3, Epsilon: 0.01\n",
      "Episode: 4850, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 4900, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 4950, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 5000, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 5050, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 5100, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 5150, Total reward: 1, Epsilon: 0.01\n",
      "Episode: 5200, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 5250, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 5300, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 5350, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 5400, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 5450, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 5500, Total reward: -5, Epsilon: 0.01\n",
      "Episode: 5550, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 5600, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 5650, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 5700, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 5750, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 5800, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 5850, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 5900, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 5950, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 6000, Total reward: -14, Epsilon: 0.01\n",
      "Episode: 6050, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 6100, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 6150, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 6200, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 6250, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 6300, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 6350, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 6400, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 6450, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 6500, Total reward: 3, Epsilon: 0.01\n",
      "Episode: 6550, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 6600, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 6650, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 6700, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 6750, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 6800, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 6850, Total reward: 14, Epsilon: 0.01\n",
      "Episode: 6900, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 6950, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 7000, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 7050, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 7100, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 7150, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 7200, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 7250, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 7300, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 7350, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 7400, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 7450, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 7500, Total reward: 14, Epsilon: 0.01\n",
      "Episode: 7550, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 7600, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 7650, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 7700, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 7750, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 7800, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 7850, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 7900, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 7950, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 8000, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 8050, Total reward: 14, Epsilon: 0.01\n",
      "Episode: 8100, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 8150, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 8200, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 8250, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 8300, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 8350, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 8400, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 8450, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 8500, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 8550, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 8600, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 8650, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 8700, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 8750, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 8800, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 8850, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 8900, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 8950, Total reward: 13, Epsilon: 0.01\n",
      "Episode: 9000, Total reward: 2, Epsilon: 0.01\n",
      "Episode: 9050, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 9100, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 9150, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 9200, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 9250, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 9300, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 9350, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 9400, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 9450, Total reward: 3, Epsilon: 0.01\n",
      "Episode: 9500, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 9550, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 9600, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 9650, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 9700, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 9750, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 9800, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 9850, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 9900, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 9950, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 10000, Total reward: 4, Epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "env_name = \"Taxi-v3\"\n",
    "agent, rewards = train_qtable(env_name = env_name, episodes = 10000, verbose = True, return_reward = True,\n",
    "                           epsilon_decay=10, lr=0.1, gamma=0.9)\n",
    "# rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum reward: 8\n",
      "Average reward: 7.87\n"
     ]
    }
   ],
   "source": [
    "test_qtable(gym.make(env_name), agent, episodes = 100)\n",
    "# agent.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Coherence classifier\n",
    "\n",
    "#agent.model.get_weights()\n",
    "\n",
    "# Define a simple GCN model\n",
    "from torch_geometric.data import Data\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super(GCN, self).__init__()\n",
    "        # Define the GCN layers\n",
    "        self.conv1 = GCNConv(data.num_node_features, 4)  # Input features to hidden\n",
    "        self.conv2 = GCNConv(4, 2)  # Hidden to output features\n",
    "        self.data = data\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # Pass data through the first GCN layer, then apply ReLU\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        # Pass data through the second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def nn_to_data(model: nn.Module) -> Data:\n",
    "    edges = []\n",
    "\n",
    "    # Counter for global neuron index\n",
    "    idx = 0\n",
    "\n",
    "    # Iterate over each layer in the network\n",
    "    base = next(model.children())\n",
    "    if isinstance(base, nn.Sequential):\n",
    "        layers = list(base.children())\n",
    "        layers2 = list(base.children())\n",
    "    else:\n",
    "        layers = list(model.children()) # iterator over the layers of the model\n",
    "        layers2 = list(model.children())\n",
    "    \n",
    "    num_nodes = layers2[0].weight.shape[1] + sum([layer.weight.shape[0] for layer in layers2 if isinstance(layer, nn.Linear)])\n",
    "    num_node_features = num_nodes\n",
    "    node_features = torch.zeros(num_nodes, num_node_features)\n",
    "    # shape = (num_nodes, num_node_features), where the node features are the bias of each node\n",
    "    # and the weights of the edges to each node (zero if there is no edge)\n",
    "\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # Update edges based on the weight matrix\n",
    "            input_dim = layer.weight.shape[1]\n",
    "            output_dim = layer.weight.shape[0]\n",
    "            for i in range(input_dim):  # Input neurons (e.g. 4)\n",
    "                for j in range(output_dim):  # Output neurons (e.g. 64)\n",
    "                    edges.append((idx + i, idx + input_dim + j))\n",
    "            \n",
    "            # Update node features (e.g., biases)\n",
    "            biases = torch.tensor(layer.bias.detach().numpy())\n",
    "            edge_weights = torch.tensor(layer.weight.detach().numpy().T)\n",
    "            node_features[idx + input_dim:idx + input_dim + output_dim, 0] = biases\n",
    "            node_features[idx:idx + input_dim, 1+idx:1+idx+output_dim] = edge_weights\n",
    "            node_features[idx + input_dim:idx + input_dim + output_dim, 1+idx:1+idx+input_dim] = edge_weights.T\n",
    "            \n",
    "            # Update the global neuron index\n",
    "            idx += input_dim\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    num_nonzero = [np.count_nonzero(node_features[i]) for i in range(node_features.shape[0])]\n",
    "    # print(num_nonzero)\n",
    "    row_mean, row_median, row_var = torch.mean(node_features[:, 1:], dim=1), torch.median(node_features[:, 1:], dim=1)[0], torch.var(node_features[:, 1:], dim=1)\n",
    "    x = torch.stack([node_features[:, 0], row_mean, row_median, row_var]).T\n",
    "    # print(x.shape)\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "data = nn_to_data(agent.model)\n",
    "gcn = GCN(data)\n",
    "# data.x.shape, data.edge_index.shape\n",
    "# print(data.x)\n",
    "\n",
    "#Debug\n",
    "out_of_bounds = data.edge_index >= data.x.shape[0]\n",
    "if out_of_bounds.any():\n",
    "    print(\"Out-of-bounds indices found at locations:\")\n",
    "    print(data.edge_index[:, out_of_bounds.any(dim=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reward function calls: 59414\n",
      "Number of non-zero rewards: 59414\n"
     ]
    }
   ],
   "source": [
    "# Dataset generation\n",
    "env = gym.make(env_name)\n",
    "NEAR_ZERO = 1e-9\n",
    "NUM_REWARD_CALLS = 0\n",
    "NUM_NON_ZERO_REWARDS = 0\n",
    "def deterministic_random(*args, lb = -1, ub = 1, sparsity = 0.0, continuous = False):\n",
    "    \"\"\"\n",
    "    Create a deterministic random number generator for a given set of arguments.\n",
    "    Used to generate deterministic reward functions for the coherence classifier.\n",
    "    [Edit 4/3/24: adapted to continuous state space]\"\"\"\n",
    "    global NUM_REWARD_CALLS\n",
    "    NUM_REWARD_CALLS += 1\n",
    "    unique_seed = f\"{args}\".encode(\"utf-8\")\n",
    "    random.seed(unique_seed)\n",
    "    return random.uniform(lb, ub) if random.random() > sparsity else random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "\n",
    "NUM_TRAIN_R_FUNCS = 50\n",
    "NUM_EPS_TRAIN_R = 50\n",
    "URS_r_funcs = [lambda *args: deterministic_random(args) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "URS_agents = [train_dqn(env_name = env_name, \n",
    "                        episodes=NUM_EPS_TRAIN_R, reward_function=r_func) for r_func in URS_r_funcs]\n",
    "USS_r_funcs = [lambda *args: deterministic_random(args, sparsity=0.999) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "print(f\"Number of reward function calls: {NUM_REWARD_CALLS}\")\n",
    "print(f\"Number of non-zero rewards: {NUM_NON_ZERO_REWARDS}\")\n",
    "USS_agents = [train_dqn(env_name = env_name, \n",
    "                        episodes=NUM_EPS_TRAIN_R, reward_function=r_func) for r_func in USS_r_funcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6014137224608205,\n",
       " 5.734868810378968e-10,\n",
       " 0.18947200717913826,\n",
       " -0.11464719428521586,\n",
       " 1.9375194864306798e-11,\n",
       " 3.1131227593489704e-10,\n",
       " -7.023178277046693e-10,\n",
       " 2.965355797951794e-10,\n",
       " 0.41831271768541045,\n",
       " -3.207247699354683e-10]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if deterministic_random is deterministic and has the correct sparsity\n",
    "assert deterministic_random(1, 2, 3, 4) == deterministic_random(1, 2, 3, 4)\n",
    "assert not deterministic_random(1, 2, 3, 4) == deterministic_random(1, 2, 3, 6)\n",
    "[deterministic_random(1, 2, 3, i, sparsity = 0.5) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, Total reward: -3.423544244882467, Epsilon: 0.91\n",
      "Episode: 100, Total reward: -2.9740031925320296, Epsilon: 0.82\n",
      "Episode: 150, Total reward: 0.44985805334502116, Epsilon: 0.74\n",
      "Episode: 200, Total reward: 1.9841901190163609, Epsilon: 0.67\n",
      "Episode: 250, Total reward: 1.9401242078162357, Epsilon: 0.61\n",
      "Episode: 300, Total reward: -1.4982150868261042, Epsilon: 0.55\n",
      "Episode: 350, Total reward: -3.697850242769119, Epsilon: 0.50\n",
      "Episode: 400, Total reward: 2.4217055508022867, Epsilon: 0.46\n",
      "Episode: 450, Total reward: -3.5300506186568104, Epsilon: 0.41\n",
      "Episode: 500, Total reward: -1.2907808496919664, Epsilon: 0.37\n"
     ]
    }
   ],
   "source": [
    "# Test when do USS agents have non-zero rewards\n",
    "USS_test_r_func = lambda *args: deterministic_random(args, sparsity=0.0)\n",
    "assert USS_test_r_func(42) == USS_test_r_func(42)\n",
    "USS_test_agent = train_dqn(env_name = env_name, episodes=500, reward_function=USS_test_r_func, \n",
    "                           verbose = True)\n",
    "# Epsilon measuring how much the agent is exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum reward: 475.0\n",
      "Episode: 1, Total reward: -15.55250607481602\n",
      "Episode: 2, Total reward: 7.210228177137941\n",
      "Episode: 3, Total reward: -8.919077872422985\n",
      "Episode: 4, Total reward: 3.3986894664967524\n",
      "Episode: 5, Total reward: -5.9617775051492\n",
      "Episode: 6, Total reward: -8.334476064077458\n",
      "Episode: 7, Total reward: 0.05630472186067914\n",
      "Episode: 8, Total reward: -2.9496726002074958\n",
      "Episode: 9, Total reward: -17.47453520502488\n",
      "Episode: 10, Total reward: 27.669555440546127\n"
     ]
    }
   ],
   "source": [
    "# epsilon_final, epsilon_start, epsilon_decay = 0.01, 1.0, 500\n",
    "# [epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay) for frame_idx in range(500)]\n",
    "test_dqn(gym.make(\"CartPole-v1\"), USS_test_agent, reward_function=USS_test_r_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([134, 4])\n"
     ]
    }
   ],
   "source": [
    "UPS_agents = [DQNAgent(env.observation_space.shape[0], env.action_space.n) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool, GCNConv, GATConv\n",
    "\n",
    "class GraphLevelGCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(GraphLevelGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 16)\n",
    "        self.linear = torch.nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        # edge_weights = data.edge_attr\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Aggregate node features to graph-level features\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Make a binary classification prediction\n",
    "        x = self.linear(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class GATGraphLevelBinary(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(GATGraphLevelBinary, self).__init__()\n",
    "        self.conv1 = GATConv(num_node_features, 8, heads=8, dropout=0.6)\n",
    "        # Increase the number of output features from the first GAT layer\n",
    "        self.conv2 = GATConv(8 * 8, 16, heads=1, concat=False, dropout=0.6)\n",
    "        # Additional GAT layer for richer node representations\n",
    "        self.linear = torch.nn.Linear(16, 1)\n",
    "        # Final linear layer to produce a graph-level output\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)  # Aggregate node features to graph-level\n",
    "        x = self.linear(x)\n",
    "        return torch.sigmoid(x)  # Sigmoid activation function for binary classification\n",
    "\n",
    "# Training loop\n",
    "USS_data = [nn_to_data(agent.model) for agent in USS_agents]\n",
    "URS_data = [nn_to_data(agent.model) for agent in URS_agents]\n",
    "print(URS_data[0].x.shape)\n",
    "UPS_data = [nn_to_data(agent.model) for agent in UPS_agents]\n",
    "assert URS_data[0].x.shape == UPS_data[0].x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Train Loss: 0.5860631885792827, Average Test Loss: 0.573617972061038\n",
      "Epoch 2: Average Train Loss: 0.27878733583838766, Average Test Loss: 0.17967451736330986\n",
      "Epoch 3: Average Train Loss: 0.18251030254305095, Average Test Loss: 0.08214785424061119\n",
      "Epoch 4: Average Train Loss: 0.13158310467361503, Average Test Loss: 0.04473840835489682\n",
      "Epoch 5: Average Train Loss: 0.08827660984985304, Average Test Loss: 0.024071608714803006\n",
      "Epoch 6: Average Train Loss: 0.05517907837950346, Average Test Loss: 0.01481392509106172\n",
      "Epoch 7: Average Train Loss: 0.04254065692901765, Average Test Loss: 0.015370194957677085\n",
      "Epoch 8: Average Train Loss: 0.04110167140394285, Average Test Loss: 0.020746125668010505\n",
      "Epoch 9: Average Train Loss: 0.04651376320540823, Average Test Loss: 0.047950702946972255\n",
      "Early stopping at epoch 9\n"
     ]
    }
   ],
   "source": [
    "# Binary classification between two datasets\n",
    "dataset1 = URS_data\n",
    "dataset2 = UPS_data\n",
    "indices = np.random.permutation(len(dataset1) + len(dataset2))\n",
    "data = [dataset1[i] if i < len(dataset1) else dataset2[i - len(dataset1)] for i in indices]\n",
    "for i in range(len(data)):\n",
    "    data[i].y = 1.0 if indices[i] < len(dataset1) else 0.0 # Binary labels for each node; 1 = URS, 0 = UPS\n",
    "    # Hence roughly speaking, 1 = more coherent, 0 = less coherent\n",
    "\n",
    "train_data_ratio = 0.8\n",
    "train_data, test_data = data[:int(train_data_ratio * len(data))], data[int(train_data_ratio * len(data)):]\n",
    "# Loss and optimizer\n",
    "num_node_features = data[0].x.shape[1] # Number of features for each node\n",
    "model = GraphLevelGCN(num_node_features)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "epochs = 40\n",
    "# Set the number of epochs to wait for early stopping\n",
    "patience = 3\n",
    "# Initialize variables for early stopping\n",
    "best_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_train_loss = 0\n",
    "    for datapt in train_data:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print(f\"datapt.x shape: {datapt.x.shape}\")  # Should be [num_nodes, num_node_features]\n",
    "        # print(f\"datapt.edge_index shape: {datapt.edge_index.shape}\")  # Should be [2, num_edges]\n",
    "        out = model.forward(datapt)\n",
    "        # print(out.size())\n",
    "        # print(torch.tensor([[datapt.y]]).size())\n",
    "        loss = criterion(out, torch.tensor([[datapt.y]]))  # Adjust shape as necessary\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_train_loss += loss.item()\n",
    "    avg_train_loss /= len(train_data)\n",
    "\n",
    "    avg_test_loss = 0\n",
    "    for datapt in test_data:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model.forward(datapt)\n",
    "            loss = criterion(out, torch.tensor([[datapt.y]]))\n",
    "            avg_test_loss += loss.item()\n",
    "    avg_test_loss /= len(test_data)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: Average Train Loss: {avg_train_loss}, Average Test Loss: {avg_test_loss}')\n",
    "    \n",
    "    # Early Stopping\n",
    "    if avg_test_loss < best_loss:\n",
    "        best_loss = avg_test_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward0>)\n",
      "[tensor([[0.2728]], grad_fn=<SigmoidBackward0>), tensor([[0.9962]], grad_fn=<SigmoidBackward0>), tensor([[1.]], grad_fn=<SigmoidBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "# Test GCN model on a \"more powerful\" NN\n",
    "print(model.forward(dataset1[0]))\n",
    "print(model.forward(dataset2[0]))\n",
    "powerful_models = [nn_to_data(train_dqn(env_name = env_name, episodes = 5 * i).model) \n",
    "                   for i in [1, 3, 10]]\n",
    "print([model.forward(data) for data in powerful_models])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The classifier training process is finicky -- sometimes it overfits, sometimes it underfits -- but sometimes can reach very low loss (< 0.002)\n",
    "- Even weak classifiers classify powerful models (a.k.a. agents with >15 episodes in CartPole) as having P(URS) = 1, corresponding to coherence ~ $\\infty$\n",
    "- P(USS) / P(URS) is still having trouble as a metric; seems extremely difficult to detect differences between USS and URS-generated policies here with current methods\n",
    "    - We will need some kind of \"more advanced\" coherence metric to distinguish more advanced policies; TODO: implement UUS somehow\n",
    "- Adding node weights to every other node to the features passed into the GCN (such that, in CartPole, the data matrix has shape (134, 134) instead of (134, 1)) makes the GCN much worse, probably because of higher dimensionality\n",
    "    - Using attention in the GNN does not help, and in fact actively overfits when using (134, 1) data\n",
    "- Even with sparsity = 0.999, USS is still hard to distinguish\n",
    "- For simpler discrete environments, maybe a Q-table is enough to solve the problem\n",
    "- Takes >= 500 episodes, small epsilon to effectively learn DQN policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now classifying q-table agents\n",
    "NUM_EPS_TRAIN_R = 1000\n",
    "NUM_TRAIN_R_FUNCS = 50\n",
    "URS_agents = [train_qtable(env_name = env_name, episodes=NUM_EPS_TRAIN_R, \n",
    "                           reward_function = lambda *args: deterministic_random(args))]\n",
    "USS_agents = [train_qtable(env_name = env_name, episodes=NUM_EPS_TRAIN_R,\n",
    "                            reward_function = lambda *args: deterministic_random(args, sparsity=0.99))]\n",
    "UPS_agents = [QTableAgent(env.observation_space.shape[0], env.action_space.n) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "\n",
    "# The Q-Table is already one-hot encoded, so we don't need to convert it to a Data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
