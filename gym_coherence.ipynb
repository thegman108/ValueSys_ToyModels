{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup:\n",
    "- Starting point: just try to train classifier on RL policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DQN implementation\n",
    "\n",
    "# Define the Q-network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "\n",
    "# Experience Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=1e-2, batch_size=64, gamma=0.99, replay_size=1000):\n",
    "        self.model = DQN(state_dim, action_dim, hidden_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.replay_buffer = ReplayBuffer(replay_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        state = torch.FloatTensor(state)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        if len(state.shape) == 1:\n",
    "            state = state.reshape(-1, 1)\n",
    "        if len(next_state.shape) == 1:\n",
    "            next_state = next_state.reshape(-1, 1)\n",
    "        action = torch.LongTensor(action)\n",
    "        reward = torch.FloatTensor(reward)\n",
    "        done = torch.FloatTensor(done)\n",
    "\n",
    "        q_values = self.model.forward(state)\n",
    "        next_q_values = self.model.forward(next_state)\n",
    "\n",
    "        # state = state.T\n",
    "        # next_state = next_state.T\n",
    "        \n",
    "        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        next_q_value = next_q_values.max(1)[0]\n",
    "        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "        \n",
    "        loss = nn.MSELoss()(q_value, expected_q_value.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.FloatTensor(np.expand_dims(state, 0))\n",
    "            q_value = self.model(state)\n",
    "            action = q_value.max(-1)[1].item()\n",
    "        else:\n",
    "            action = random.randrange(self.action_dim)\n",
    "        return action\n",
    "    \n",
    "class QTableAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-2, gamma=0.99):\n",
    "        self.q_table = np.zeros((state_dim, action_dim))\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        q_value = self.q_table[state, action]\n",
    "        next_q_value = np.max(self.q_table[next_state])\n",
    "        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "        \n",
    "        self.q_table[state, action] += self.lr * (expected_q_value - q_value)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            action = np.argmax(self.q_table[state])\n",
    "        else:\n",
    "            action = random.randrange(self.action_dim)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NON_ZERO_REWARDS = 0\n",
    "def one_hot_state(state, env):\n",
    "    state_arr = np.zeros(env.observation_space.n)\n",
    "    state_arr[state] = 1\n",
    "    return state_arr\n",
    "\n",
    "def train_dqn(env_name=\"CartPole-v1\", episodes=500, epsilon_start=1.0, epsilon_final=0.01, \n",
    "              epsilon_decay=500, reward_function = None, verbose = False, return_reward = False, \n",
    "              print_every=50, **kwargs):\n",
    "    \"\"\"\n",
    "    Train a DQN agent on the specified environment.\n",
    "    \n",
    "    Args:\n",
    "        env_name: str\n",
    "            Name of the environment to train the agent on.\n",
    "        episodes: int\n",
    "            Number of episodes to train the agent for.\n",
    "        epsilon_start: float\n",
    "            Initial epsilon value for epsilon-greedy action selection.\n",
    "        epsilon_final: float\n",
    "            Final epsilon value for epsilon-greedy action selection.\n",
    "        epsilon_decay: float\n",
    "            Decay rate for epsilon.\n",
    "        reward_function: function\n",
    "            Optional reward function to use for training.\n",
    "        verbose: bool\n",
    "            Whether to print training progress.\n",
    "\n",
    "    Returns:\n",
    "        DQNAgent: trained DQN agent. \n",
    "    \"\"\"\n",
    "    global NUM_NON_ZERO_REWARDS\n",
    "    env = gym.make(env_name)\n",
    "    if len(env.observation_space.shape) == 0:\n",
    "        state_dim = env.observation_space.n\n",
    "    else:\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    agent = DQNAgent(state_dim, action_dim, **kwargs)\n",
    "    \n",
    "    epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay)\n",
    "    \n",
    "    rewards = np.zeros(episodes)   \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset() # Reset the environment, reward\n",
    "        if state_dim == env.observation_space.n:\n",
    "            state = one_hot_state(state, env)\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            epsilon = epsilon_by_frame(episode)\n",
    "            # One-hot encode the state\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if state_dim == env.observation_space.n:\n",
    "                next_state = one_hot_state(next_state, env)\n",
    "\n",
    "            if reward_function and done: #custom reward function\n",
    "                reward = reward_function(next_state)\n",
    "            NUM_NON_ZERO_REWARDS += 0 if math.isclose(reward, 0) else 1\n",
    "            \n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            agent.update()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            # print(f\"Episode: {episode+1}, Total reward: {episode_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "        rewards[episode] = episode_reward\n",
    "        # Optional: Render the environment to visualize training progress\n",
    "        if verbose and episode % print_every == print_every - 1:\n",
    "        #     render_env(env, agent)\n",
    "            print(f\"Episode: {episode+1}, Average total reward: {np.average(rewards[episode - print_every + 1 : episode])}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    return agent if not return_reward else (agent, rewards)\n",
    "\n",
    "# Optional: Function to render the environment with the current policy\n",
    "def render_env(env, agent):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state, 0)  # Using 0 epsilon for greedy action selection\n",
    "        # print(env.step(action))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qtable(env_name=\"CartPole-v1\", episodes=500, epsilon_start=1.0, epsilon_final=0.01, \n",
    "              epsilon_decay=500, reward_function = None, verbose = False, return_reward = False, \n",
    "              print_every=50, **kwargs):\n",
    "    \"\"\"\n",
    "    Train a Q-table agent on the specified environment.\"\"\"\n",
    "    global NUM_NON_ZERO_REWARDS\n",
    "    env = gym.make(env_name)\n",
    "    if len(env.observation_space.shape) == 0:\n",
    "        state_dim = env.observation_space.n\n",
    "    else:\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    agent = QTableAgent(state_dim, action_dim, **kwargs)\n",
    "\n",
    "    rewards = np.zeros(episodes)\n",
    "    epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay)\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            epsilon = epsilon_by_frame(episode)\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if reward_function:\n",
    "                reward = reward_function(next_state)\n",
    "\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards[episode] = episode_reward\n",
    "        if verbose and episode % print_every == print_every - 1:\n",
    "            print(f\"Episode: {episode+1}, Average total reward: {np.average(rewards[episode - print_every + 1 : episode])}, Epsilon: {epsilon:.2f}\")\n",
    "        \n",
    "    env.close()\n",
    "    return agent if not return_reward else (agent, rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEAR_ZERO = 1e-9\n",
    "def test_dqn(env, agent, episodes=10, reward_function=None, verbose = False):\n",
    "    print(f\"Maximum reward: {env.spec.reward_threshold}\")\n",
    "    average_value = 0\n",
    "    for episode in range(episodes):\n",
    "        # if episode == 0:\n",
    "        #     render_env(env, agent)\n",
    "        state = env.reset()\n",
    "        if len(env.observation_space.shape) == 0:\n",
    "            state = one_hot_state(state, env)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state, 0)  # Using 0 epsilon for greedy action selection\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if len(env.observation_space.shape) == 0:\n",
    "                next_state = one_hot_state(next_state, env)\n",
    "            if reward_function and done:\n",
    "                reward = reward_function(next_state)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        if verbose:\n",
    "            print(f\"Episode: {episode+1}, Total reward: {episode_reward}\")\n",
    "        average_value += episode_reward\n",
    "    average_value /= episodes\n",
    "    print(f\"Average reward: {average_value}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qtable(env, agent, episodes=10, reward_function=None, verbose = False):\n",
    "    \"\"\"\n",
    "    Test a Q-table agent on the specified environment.\n",
    "    (This is basically test_dqn but without the one-hot encoding.)\n",
    "    \"\"\"\n",
    "    print(f\"Maximum reward: {env.spec.reward_threshold}\")\n",
    "    average_value = 0\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state, 0)  # Using 0 epsilon for greedy action selection\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if reward_function:\n",
    "                reward = reward_function(next_state)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        if verbose:\n",
    "            print(f\"Episode: {episode+1}, Total reward: {episode_reward}\")\n",
    "        average_value += episode_reward\n",
    "    average_value /= episodes\n",
    "    print(f\"Average reward: {average_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, Total reward: -151, Epsilon: 0.02\n",
      "Episode: 100, Total reward: -209, Epsilon: 0.01\n",
      "Episode: 150, Total reward: -184, Epsilon: 0.01\n",
      "Episode: 200, Total reward: -235, Epsilon: 0.01\n",
      "Episode: 250, Total reward: -285, Epsilon: 0.01\n",
      "Episode: 300, Total reward: -293, Epsilon: 0.01\n",
      "Episode: 350, Total reward: -362, Epsilon: 0.01\n",
      "Episode: 400, Total reward: -32, Epsilon: 0.01\n",
      "Episode: 450, Total reward: 14, Epsilon: 0.01\n",
      "Episode: 500, Total reward: 13, Epsilon: 0.01\n",
      "Episode: 550, Total reward: 13, Epsilon: 0.01\n",
      "Episode: 600, Total reward: -38, Epsilon: 0.01\n",
      "Episode: 650, Total reward: -59, Epsilon: 0.01\n",
      "Episode: 700, Total reward: -28, Epsilon: 0.01\n",
      "Episode: 750, Total reward: -27, Epsilon: 0.01\n",
      "Episode: 800, Total reward: -90, Epsilon: 0.01\n",
      "Episode: 850, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 900, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 950, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 1000, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 1050, Total reward: -19, Epsilon: 0.01\n",
      "Episode: 1100, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 1150, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 1200, Total reward: -39, Epsilon: 0.01\n",
      "Episode: 1250, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 1300, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 1350, Total reward: 3, Epsilon: 0.01\n",
      "Episode: 1400, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 1450, Total reward: -11, Epsilon: 0.01\n",
      "Episode: 1500, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 1550, Total reward: -3, Epsilon: 0.01\n",
      "Episode: 1600, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 1650, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 1700, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 1750, Total reward: 2, Epsilon: 0.01\n",
      "Episode: 1800, Total reward: 15, Epsilon: 0.01\n",
      "Episode: 1850, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 1900, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 1950, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 2000, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 2050, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 2100, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 2150, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 2200, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 2250, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 2300, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 2350, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 2400, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 2450, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 2500, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 2550, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 2600, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 2650, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 2700, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 2750, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 2800, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 2850, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 2900, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 2950, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 3000, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 3050, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 3100, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 3150, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 3200, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 3250, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 3300, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 3350, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 3400, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 3450, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 3500, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 3550, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 3600, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 3650, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 3700, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 3750, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 3800, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 3850, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 3900, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 3950, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 4000, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 4050, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 4100, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 4150, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 4200, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 4250, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 4300, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 4350, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 4400, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 4450, Total reward: 2, Epsilon: 0.01\n",
      "Episode: 4500, Total reward: 14, Epsilon: 0.01\n",
      "Episode: 4550, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 4600, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 4650, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 4700, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 4750, Total reward: 13, Epsilon: 0.01\n",
      "Episode: 4800, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 4850, Total reward: -5, Epsilon: 0.01\n",
      "Episode: 4900, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 4950, Total reward: -1, Epsilon: 0.01\n",
      "Episode: 5000, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 5050, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 5100, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 5150, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 5200, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 5250, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 5300, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 5350, Total reward: -4, Epsilon: 0.01\n",
      "Episode: 5400, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 5450, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 5500, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 5550, Total reward: 13, Epsilon: 0.01\n",
      "Episode: 5600, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 5650, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 5700, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 5750, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 5800, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 5850, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 5900, Total reward: -2, Epsilon: 0.01\n",
      "Episode: 5950, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 6000, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 6050, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 6100, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 6150, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 6200, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 6250, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 6300, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 6350, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 6400, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 6450, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 6500, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 6550, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 6600, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 6650, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 6700, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 6750, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 6800, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 6850, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 6900, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 6950, Total reward: 13, Epsilon: 0.01\n",
      "Episode: 7000, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 7050, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 7100, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 7150, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 7200, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 7250, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 7300, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 7350, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 7400, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 7450, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 7500, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 7550, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 7600, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 7650, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 7700, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 7750, Total reward: 15, Epsilon: 0.01\n",
      "Episode: 7800, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 7850, Total reward: -5, Epsilon: 0.01\n",
      "Episode: 7900, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 7950, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 8000, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 8050, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 8100, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 8150, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 8200, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 8250, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 8300, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 8350, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 8400, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 8450, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 8500, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 8550, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 8600, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 8650, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 8700, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 8750, Total reward: 12, Epsilon: 0.01\n",
      "Episode: 8800, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 8850, Total reward: 9, Epsilon: 0.01\n",
      "Episode: 8900, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 8950, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 9000, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 9050, Total reward: 10, Epsilon: 0.01\n",
      "Episode: 9100, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 9150, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 9200, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 9250, Total reward: 11, Epsilon: 0.01\n",
      "Episode: 9300, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 9350, Total reward: -3, Epsilon: 0.01\n",
      "Episode: 9400, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 9450, Total reward: 4, Epsilon: 0.01\n",
      "Episode: 9500, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 9550, Total reward: 5, Epsilon: 0.01\n",
      "Episode: 9600, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 9650, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 9700, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 9750, Total reward: 6, Epsilon: 0.01\n",
      "Episode: 9800, Total reward: 8, Epsilon: 0.01\n",
      "Episode: 9850, Total reward: 0, Epsilon: 0.01\n",
      "Episode: 9900, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 9950, Total reward: 7, Epsilon: 0.01\n",
      "Episode: 10000, Total reward: 12, Epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "env_name = \"Taxi-v3\"\n",
    "agent, rewards = train_qtable(env_name = env_name, episodes = 10000, verbose = True, return_reward = True,\n",
    "                           epsilon_decay=10, lr=0.1, gamma=0.9)\n",
    "# rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum reward: 8\n",
      "Average reward: 7.87\n"
     ]
    }
   ],
   "source": [
    "test_qtable(gym.make(env_name), agent, episodes = 100)\n",
    "# agent.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QTableAgent' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m     edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(edges, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mt()\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Data(x\u001b[38;5;241m=\u001b[39mx, edge_index\u001b[38;5;241m=\u001b[39medge_index)\n\u001b[1;32m---> 74\u001b[0m data \u001b[38;5;241m=\u001b[39m nn_to_data(\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m)\n\u001b[0;32m     75\u001b[0m gcn \u001b[38;5;241m=\u001b[39m GCN(data)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# data.x.shape, data.edge_index.shape\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# print(data.x)\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m#Debug\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'QTableAgent' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "### Coherence classifier\n",
    "\n",
    "#agent.model.get_weights()\n",
    "\n",
    "# Define a simple GCN model\n",
    "from torch_geometric.data import Data\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super(GCN, self).__init__()\n",
    "        # Define the GCN layers\n",
    "        self.conv1 = GCNConv(data.num_node_features, 4)  # Input features to hidden\n",
    "        self.conv2 = GCNConv(4, 2)  # Hidden to output features\n",
    "        self.data = data\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # Pass data through the first GCN layer, then apply ReLU\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        # Pass data through the second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def nn_to_data(model: nn.Module) -> Data:\n",
    "    edges = []\n",
    "\n",
    "    # Counter for global neuron index\n",
    "    idx = 0\n",
    "\n",
    "    # Iterate over each layer in the network\n",
    "    base = next(model.children())\n",
    "    if isinstance(base, nn.Sequential):\n",
    "        layers = list(base.children())\n",
    "        layers2 = list(base.children())\n",
    "    else:\n",
    "        layers = list(model.children()) # iterator over the layers of the model\n",
    "        layers2 = list(model.children())\n",
    "    \n",
    "    num_nodes = layers2[0].weight.shape[1] + sum([layer.weight.shape[0] for layer in layers2 if isinstance(layer, nn.Linear)])\n",
    "    num_node_features = num_nodes\n",
    "    node_features = torch.zeros(num_nodes, num_node_features)\n",
    "    # shape = (num_nodes, num_node_features), where the node features are the bias of each node\n",
    "    # and the weights of the edges to each node (zero if there is no edge)\n",
    "\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # Update edges based on the weight matrix\n",
    "            input_dim = layer.weight.shape[1]\n",
    "            output_dim = layer.weight.shape[0]\n",
    "            for i in range(input_dim):  # Input neurons (e.g. 4)\n",
    "                for j in range(output_dim):  # Output neurons (e.g. 64)\n",
    "                    edges.append((idx + i, idx + input_dim + j))\n",
    "            \n",
    "            # Update node features (e.g., biases)\n",
    "            biases = torch.tensor(layer.bias.detach().numpy())\n",
    "            edge_weights = torch.tensor(layer.weight.detach().numpy().T)\n",
    "            node_features[idx + input_dim:idx + input_dim + output_dim, 0] = biases\n",
    "            node_features[idx:idx + input_dim, 1+idx:1+idx+output_dim] = edge_weights\n",
    "            node_features[idx + input_dim:idx + input_dim + output_dim, 1+idx:1+idx+input_dim] = edge_weights.T\n",
    "            \n",
    "            # Update the global neuron index\n",
    "            idx += input_dim\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    num_nonzero = [np.count_nonzero(node_features[i]) for i in range(node_features.shape[0])]\n",
    "    # print(num_nonzero)\n",
    "    row_mean, row_median, row_var = torch.mean(node_features[:, 1:], dim=1), torch.median(node_features[:, 1:], dim=1)[0], torch.var(node_features[:, 1:], dim=1)\n",
    "    x = torch.stack([node_features[:, 0], row_mean, row_median, row_var]).T\n",
    "    # print(x.shape)\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "data = nn_to_data(agent.model)\n",
    "gcn = GCN(data)\n",
    "# data.x.shape, data.edge_index.shape\n",
    "# print(data.x)\n",
    "\n",
    "#Debug\n",
    "out_of_bounds = data.edge_index >= data.x.shape[0]\n",
    "if out_of_bounds.any():\n",
    "    print(\"Out-of-bounds indices found at locations:\")\n",
    "    print(data.edge_index[:, out_of_bounds.any(dim=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reward function calls: 50\n",
      "Number of non-zero rewards: 9871\n"
     ]
    }
   ],
   "source": [
    "# Dataset generation\n",
    "env = gym.make(env_name)\n",
    "NEAR_ZERO = 1e-9\n",
    "NUM_REWARD_CALLS = 0\n",
    "NUM_NON_ZERO_REWARDS = 0\n",
    "def deterministic_random(*args, lb = -1, ub = 1, sparsity = 0.0, continuous = False):\n",
    "    \"\"\"\n",
    "    Create a deterministic random number generator for a given set of arguments.\n",
    "    Used to generate deterministic reward functions for the coherence classifier.\n",
    "    [Edit 4/3/24: adapted to continuous state space]\"\"\"\n",
    "    global NUM_REWARD_CALLS\n",
    "    NUM_REWARD_CALLS += 1\n",
    "    unique_seed = f\"{args}\".encode(\"utf-8\")\n",
    "    random.seed(unique_seed)\n",
    "    return random.uniform(lb, ub) if random.random() > sparsity else random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "\n",
    "NUM_TRAIN_R_FUNCS = 1\n",
    "NUM_EPS_TRAIN_R = 50\n",
    "URS_r_funcs = [lambda *args: deterministic_random(args) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "URS_agents = [train_dqn(env_name = env_name, \n",
    "                        episodes=NUM_EPS_TRAIN_R, reward_function=r_func) for r_func in URS_r_funcs]\n",
    "USS_r_funcs = [lambda *args: deterministic_random(args, sparsity=0.999) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "print(f\"Number of reward function calls: {NUM_REWARD_CALLS}\")\n",
    "print(f\"Number of non-zero rewards: {NUM_NON_ZERO_REWARDS}\")\n",
    "USS_agents = [train_dqn(env_name = env_name, \n",
    "                        episodes=NUM_EPS_TRAIN_R, reward_function=r_func) for r_func in USS_r_funcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6014137224608205,\n",
       " 5.734868810378968e-10,\n",
       " 0.18947200717913826,\n",
       " -0.11464719428521586,\n",
       " 1.9375194864306798e-11,\n",
       " 3.1131227593489704e-10,\n",
       " -7.023178277046693e-10,\n",
       " 2.965355797951794e-10,\n",
       " 0.41831271768541045,\n",
       " -3.207247699354683e-10]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if deterministic_random is deterministic and has the correct sparsity\n",
    "assert deterministic_random(1, 2, 3, 4) == deterministic_random(1, 2, 3, 4)\n",
    "assert not deterministic_random(1, 2, 3, 4) == deterministic_random(1, 2, 3, 6)\n",
    "[deterministic_random(1, 2, 3, i, sparsity = 0.5) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, Total reward: -3.423544244882467, Epsilon: 0.91\n",
      "Episode: 100, Total reward: -2.9740031925320296, Epsilon: 0.82\n",
      "Episode: 150, Total reward: 0.44985805334502116, Epsilon: 0.74\n",
      "Episode: 200, Total reward: 1.9841901190163609, Epsilon: 0.67\n",
      "Episode: 250, Total reward: 1.9401242078162357, Epsilon: 0.61\n",
      "Episode: 300, Total reward: -1.4982150868261042, Epsilon: 0.55\n",
      "Episode: 350, Total reward: -3.697850242769119, Epsilon: 0.50\n",
      "Episode: 400, Total reward: 2.4217055508022867, Epsilon: 0.46\n",
      "Episode: 450, Total reward: -3.5300506186568104, Epsilon: 0.41\n",
      "Episode: 500, Total reward: -1.2907808496919664, Epsilon: 0.37\n"
     ]
    }
   ],
   "source": [
    "# Test when do USS agents have non-zero rewards\n",
    "USS_test_r_func = lambda *args: deterministic_random(args, sparsity=0.0)\n",
    "assert USS_test_r_func(42) == USS_test_r_func(42)\n",
    "USS_test_agent = train_dqn(env_name = env_name, episodes=500, reward_function=USS_test_r_func, \n",
    "                           verbose = True)\n",
    "# Epsilon measuring how much the agent is exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum reward: 475.0\n",
      "Episode: 1, Total reward: -15.55250607481602\n",
      "Episode: 2, Total reward: 7.210228177137941\n",
      "Episode: 3, Total reward: -8.919077872422985\n",
      "Episode: 4, Total reward: 3.3986894664967524\n",
      "Episode: 5, Total reward: -5.9617775051492\n",
      "Episode: 6, Total reward: -8.334476064077458\n",
      "Episode: 7, Total reward: 0.05630472186067914\n",
      "Episode: 8, Total reward: -2.9496726002074958\n",
      "Episode: 9, Total reward: -17.47453520502488\n",
      "Episode: 10, Total reward: 27.669555440546127\n"
     ]
    }
   ],
   "source": [
    "# epsilon_final, epsilon_start, epsilon_decay = 0.01, 1.0, 500\n",
    "# [epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay) for frame_idx in range(500)]\n",
    "test_dqn(gym.make(\"CartPole-v1\"), USS_test_agent, reward_function=USS_test_r_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([634, 4])\n"
     ]
    }
   ],
   "source": [
    "def get_state_shape(env):\n",
    "    return 1 if len(env.observation_space.shape) == 0 else env.observation_space.shape[0]\n",
    "def get_state_size(env):\n",
    "    return env.observation_space.n if len(env.observation_space.shape) == 0 else env.observation_space.shape[0]\n",
    "UPS_agents = [DQNAgent(get_state_size(env), env.action_space.n) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool, GCNConv, GATConv\n",
    "\n",
    "class GraphLevelGCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(GraphLevelGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 16)\n",
    "        self.linear = torch.nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        # edge_weights = data.edge_attr\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Aggregate node features to graph-level features\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Make a binary classification prediction\n",
    "        x = self.linear(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class GATGraphLevelBinary(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(GATGraphLevelBinary, self).__init__()\n",
    "        self.conv1 = GATConv(num_node_features, 8, heads=8, dropout=0.6)\n",
    "        # Increase the number of output features from the first GAT layer\n",
    "        self.conv2 = GATConv(8 * 8, 16, heads=1, concat=False, dropout=0.6)\n",
    "        # Additional GAT layer for richer node representations\n",
    "        self.linear = torch.nn.Linear(16, 1)\n",
    "        # Final linear layer to produce a graph-level output\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)  # Aggregate node features to graph-level\n",
    "        x = self.linear(x)\n",
    "        return torch.sigmoid(x)  # Sigmoid activation function for binary classification\n",
    "\n",
    "# Training loop\n",
    "USS_data = [nn_to_data(agent.model) for agent in USS_agents]\n",
    "URS_data = [nn_to_data(agent.model) for agent in URS_agents]\n",
    "print(URS_data[0].x.shape)\n",
    "UPS_data = [nn_to_data(agent.model) for agent in UPS_agents]\n",
    "assert URS_data[0].x.shape == UPS_data[0].x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Train Loss: 0.611354649066925, Average Test Loss: 0.8701407313346863\n",
      "Epoch 2: Average Train Loss: 0.583457887172699, Average Test Loss: 0.9334022402763367\n",
      "Epoch 3: Average Train Loss: 0.5527416467666626, Average Test Loss: 1.0072805881500244\n",
      "Epoch 4: Average Train Loss: 0.5177010297775269, Average Test Loss: 1.0963069200515747\n",
      "Early stopping at epoch 4\n"
     ]
    }
   ],
   "source": [
    "# Binary classification between two datasets\n",
    "dataset1 = URS_data\n",
    "dataset2 = UPS_data\n",
    "def generate_data(dataset1, dataset2):\n",
    "    indices = np.random.permutation(len(dataset1) + len(dataset2))\n",
    "    data = [dataset1[i] if i < len(dataset1) else dataset2[i - len(dataset1)] for i in indices]\n",
    "    for i in range(len(data)):\n",
    "        data[i].y = 1.0 if indices[i] < len(dataset1) else 0.0 # Binary labels for each node; 1 = URS, 0 = UPS\n",
    "        # Hence roughly speaking, 1 = more coherent, 0 = less coherent\n",
    "\n",
    "    train_data_ratio = 0.8\n",
    "    train_data, test_data = data[:int(train_data_ratio * len(data))], data[int(train_data_ratio * len(data)):]\n",
    "    num_node_features = data[0].x.shape[1] # Number of features for each node\n",
    "    return train_data, test_data, num_node_features\n",
    "\n",
    "train_data, test_data, num_node_features = generate_data(dataset1, dataset2)\n",
    "# Loss and optimizer\n",
    "model = GraphLevelGCN(num_node_features)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "def train_classifier(model, criterion, optimizer, train_data, test_data, epochs = 40, patience = 3, \n",
    "                     epochs_without_improvement = 0, best_loss = float('inf')):\n",
    "    for epoch in range(epochs):\n",
    "        avg_train_loss = 0\n",
    "        for datapt in train_data:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # print(f\"datapt.x shape: {datapt.x.shape}\")  # Should be [num_nodes, num_node_features]\n",
    "            # print(f\"datapt.edge_index shape: {datapt.edge_index.shape}\")  # Should be [2, num_edges]\n",
    "            out = model.forward(datapt)\n",
    "            # print(out.size())\n",
    "            # print(torch.tensor([[datapt.y]]).size())\n",
    "            loss = criterion(out, torch.tensor([[datapt.y]]))  # Adjust shape as necessary\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_train_loss += loss.item()\n",
    "        avg_train_loss /= len(train_data)\n",
    "\n",
    "        avg_test_loss = 0\n",
    "        for datapt in test_data:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model.forward(datapt)\n",
    "                loss = criterion(out, torch.tensor([[datapt.y]]))\n",
    "                avg_test_loss += loss.item()\n",
    "        avg_test_loss /= len(test_data)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Average Train Loss: {avg_train_loss}, Average Test Loss: {avg_test_loss}')\n",
    "        \n",
    "        # Early Stopping\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "train_classifier(model, criterion, optimizer, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.0007]], grad_fn=<SigmoidBackward0>)\n",
      "[tensor([[0.2728]], grad_fn=<SigmoidBackward0>), tensor([[0.9962]], grad_fn=<SigmoidBackward0>), tensor([[1.]], grad_fn=<SigmoidBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "# Test GCN model on a \"more powerful\" NN\n",
    "print(model.forward(dataset1[0]))\n",
    "print(model.forward(dataset2[0]))\n",
    "powerful_models = [nn_to_data(train_dqn(env_name = env_name, episodes = 5 * i).model) \n",
    "                   for i in [1, 3, 10]]\n",
    "print([model.forward(data) for data in powerful_models])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The classifier training process is finicky -- sometimes it overfits, sometimes it underfits -- but sometimes can reach very low loss (< 0.002)\n",
    "- Even weak classifiers classify powerful models (a.k.a. agents with >15 episodes in CartPole) as having P(URS) = 1, corresponding to coherence ~ $\\infty$\n",
    "- P(USS) / P(URS) is still having trouble as a metric; seems extremely difficult to detect differences between USS and URS-generated policies here with current methods\n",
    "    - We will need some kind of \"more advanced\" coherence metric to distinguish more advanced policies; TODO: implement UUS somehow\n",
    "- Adding node weights to every other node to the features passed into the GCN (such that, in CartPole, the data matrix has shape (134, 134) instead of (134, 1)) makes the GCN much worse, probably because of higher dimensionality\n",
    "    - Using attention in the GNN does not help, and in fact actively overfits when using (134, 1) data\n",
    "- Even with sparsity = 0.999, USS is still hard to distinguish\n",
    "- For simpler discrete environments, maybe a Q-table is enough to solve the problem\n",
    "- Takes >= 500 episodes, small epsilon to effectively learn DQN policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now classifying q-table agents\n",
    "NUM_EPS_TRAIN_R = 1000\n",
    "NUM_TRAIN_R_FUNCS = 50\n",
    "NUM_REWARD_CALLS = 0\n",
    "env = gym.make(env_name)\n",
    "def deterministic_random(*args, lb = -1, ub = 1, sparsity = 0.0, continuous = False):\n",
    "    \"\"\"\n",
    "    Create a deterministic random number generator for a given set of arguments.\n",
    "    Used to generate deterministic reward functions for the coherence classifier.\n",
    "    [Edit 4/3/24: adapted to continuous state space]\"\"\"\n",
    "    global NUM_REWARD_CALLS\n",
    "    NUM_REWARD_CALLS += 1\n",
    "    unique_seed = f\"{args}\".encode(\"utf-8\")\n",
    "    random.seed(unique_seed)\n",
    "    return random.uniform(lb, ub) if random.random() > sparsity else random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "URS_r_funcs = [lambda *args: deterministic_random(args) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "URS_agents = [train_qtable(env_name = env_name, episodes=NUM_EPS_TRAIN_R, \n",
    "                           reward_function = r_func) for r_func in URS_r_funcs]\n",
    "USS_r_funcs = [lambda *args: deterministic_random(args, sparsity=0.99) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "USS_agents = [train_qtable(env_name = env_name, episodes=NUM_EPS_TRAIN_R,\n",
    "                            reward_function = r_func) for r_func in USS_r_funcs]\n",
    "UPS_agents = [QTableAgent(get_state_size(env), env.action_space.n) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "\n",
    "# The Q-Table is already one-hot encoded, so we don't need to convert it to a Data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.879610979477867e-10, -8.537837971656553e-10, 1.3453720557822044e-10, -6.91582481382308e-10, 5.834328244211258e-10, 2.689469001408139e-10, -7.747733415111116e-10, 3.5242825351422126e-10, 2.6150563760737437e-10, -8.919200340033589e-10]\n",
      "Episode: 50, Average total reward: 5.595625995452984, Epsilon: 0.62\n",
      "Maximum reward: 8\n",
      "Average reward: 2.7499838254254803\n",
      "Maximum reward: 8\n",
      "Average reward: 0.0067166083132540865\n"
     ]
    }
   ],
   "source": [
    "# Test ground\n",
    "print([USS_r_funcs[0](i) for i in range(10)])\n",
    "test_USS_agent = train_qtable(env_name = env_name, episodes = 50, verbose=True, epsilon_decay = 100, \n",
    "                              lr = 0.01, gamma = 0.9, reward_function = USS_r_funcs[0])\n",
    "test_qtable(gym.make(env_name), test_USS_agent, episodes = 100, reward_function = USS_r_funcs[0])\n",
    "test_UPS_agent = QTableAgent(get_state_size(env), env.action_space.n)\n",
    "test_qtable(gym.make(env_name), test_UPS_agent, episodes = 100, reward_function = USS_r_funcs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNNBinary(nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(FCNNBinary, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_node_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "def qtable_to_feat(qtable: torch.Tensor, label):\n",
    "    # In qtable, rows are states and columns are actions taken in that state\n",
    "    return Data(x = torch.flatten(qtable), y = label) # Naive approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "Epoch 1: Average Train Loss: 45.84827167169324, Average Test Loss: 65.0\n",
      "Epoch 2: Average Train Loss: 46.25, Average Test Loss: 65.0\n",
      "Epoch 3: Average Train Loss: 46.25, Average Test Loss: 65.0\n",
      "Epoch 4: Average Train Loss: 46.25, Average Test Loss: 65.0\n",
      "Early stopping at epoch 4\n"
     ]
    }
   ],
   "source": [
    "UPS_agents = [QTableAgent(get_state_size(env), env.action_space.n) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "\n",
    "for agent in UPS_agents:\n",
    "    for row in agent.q_table:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = np.random.uniform(-1, 1) # set each value to a random number between -1 and 1\n",
    "dataset1 = [qtable_to_feat(torch.tensor(agent.q_table, dtype=torch.float32), 1) for agent in USS_agents]\n",
    "dataset2 = [qtable_to_feat(torch.tensor(agent.q_table, dtype=torch.float32), 0) for agent in URS_agents] # URS = 1, UPS = 0\n",
    "\n",
    "def generate_fcnn_data(dataset1, dataset2):\n",
    "    indices = np.random.permutation(len(dataset1) + len(dataset2))\n",
    "    data = [dataset1[i] if i < len(dataset1) else dataset2[i - len(dataset1)] for i in indices]\n",
    "    for i in range(len(data)):\n",
    "        data[i].y = 1.0 if indices[i] < len(dataset1) else 0.0 # Binary labels for each node; 1 = URS, 0 = UPS\n",
    "        # Hence roughly speaking, 1 = more coherent, 0 = less coherent\n",
    "\n",
    "    train_data_ratio = 0.8\n",
    "    train_data, test_data = data[:int(train_data_ratio * len(data))], data[int(train_data_ratio * len(data)):]\n",
    "    num_node_features = data[0].x.shape[0] # Number of features for each node\n",
    "    return train_data, test_data, num_node_features\n",
    "\n",
    "def train_fcnn_classifier(model, criterion, optimizer, train_data, test_data, epochs = 40, patience = 3, \n",
    "                          epochs_without_improvement = 0, best_loss = float('inf')):\n",
    "    for epoch in range(epochs):\n",
    "        avg_train_loss = 0\n",
    "        for datapt in train_data:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model.forward(datapt)\n",
    "            assert isinstance(out, torch.Tensor), f\"Expected model.forward to return a tensor, but got {out}\"\n",
    "            loss = criterion(out, torch.tensor([datapt.y]))  # Adjust shape as necessary\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_train_loss += loss.item()\n",
    "        avg_train_loss /= len(train_data)\n",
    "\n",
    "        avg_test_loss = 0\n",
    "        for datapt in test_data:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model.forward(datapt)\n",
    "                loss = criterion(out, torch.tensor([datapt.y]))\n",
    "                avg_test_loss += loss.item()\n",
    "        avg_test_loss /= len(test_data)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Average Train Loss: {avg_train_loss}, Average Test Loss: {avg_test_loss}')\n",
    "        \n",
    "        # Early Stopping\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "train_data, test_data, num_node_features = generate_fcnn_data(dataset1, dataset2)\n",
    "print(num_node_features)\n",
    "model = FCNNBinary(num_node_features)\n",
    "criterion = torch.nn.BCELoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "train_fcnn_classifier(model, criterion, optimizer, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_UVS_qagent(rand_qtable):\n",
    "    \"\"\"\n",
    "    Generate a Q-table agent with \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0,   0,   0,  ..., 499, 499, 499],\n",
       "         [100,   0,  20,  ..., 479, 499, 499]]),\n",
       " torch.Size([2, 3000]))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Turn the state and action space of Taxi-v3 into a graph\n",
    "\n",
    "from collections import defaultdict\n",
    "taxi_env = gym.make(\"Taxi-v3\")\n",
    "taxi_env.reset()\n",
    "# Initialize containers for graph data\n",
    "edges = defaultdict(list)\n",
    "edge_attr = defaultdict(list)\n",
    "\n",
    "# A helper function to encode the state into a single number (node index)\n",
    "def state_to_node(taxi_row, taxi_col, pass_loc, dest_idx):\n",
    "    # This encoding assumes specific knowledge about the Taxi-v3 state space size\n",
    "    return taxi_row * 100 + taxi_col * 20 + pass_loc * 4 + dest_idx\n",
    "    # max = 4 * 100 + 4 * 20 + 4 * 4 + 3 = 400 + 80 + 16 + 3 = 499\n",
    "\n",
    "# Iterate through all possible states and actions to construct the graph\n",
    "for taxi_row in range(5):\n",
    "    for taxi_col in range(5):\n",
    "        for pass_loc in range(5):  # 4 locations + 1 for 'in taxi'\n",
    "            for dest_idx in range(4):\n",
    "                current_state = state_to_node(taxi_row, taxi_col, pass_loc, dest_idx)\n",
    "                for action in range(taxi_env.action_space.n):\n",
    "                    # Set the environment to the current state\n",
    "                    taxi_env.unwrapped.s = current_state\n",
    "                    # Take action and observe the next state and reward\n",
    "                    next_state, reward, done, _ = taxi_env.step(action)\n",
    "                    # Add edge from current state to next state\n",
    "                    edges[current_state].append(next_state)\n",
    "                    # Optionally, use rewards as edge attributes\n",
    "                    # edge_attr[(current_state, next_state)].append(reward)\n",
    "                    taxi_env.reset()\n",
    "\n",
    "\n",
    "# Convert edges and edge attributes to tensors\n",
    "edge_index = []\n",
    "for src, dsts in edges.items():\n",
    "    for dst in dsts:\n",
    "        edge_index.append([src, dst])\n",
    "edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "edge_index, edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Train Loss: 0.7374219067802187, Average Test Loss: 0.5225726202130317\n",
      "Epoch 2: Average Train Loss: 0.38468063431232624, Average Test Loss: 0.033242557127960023\n",
      "Epoch 3: Average Train Loss: 0.008957611854202695, Average Test Loss: 0.003896899623214267\n",
      "Epoch 4: Average Train Loss: 0.003252486547717126, Average Test Loss: 0.002658354368759319\n",
      "Epoch 5: Average Train Loss: 0.002364098767429823, Average Test Loss: 0.0020714492442493793\n",
      "Epoch 6: Average Train Loss: 0.0019292355794277682, Average Test Loss: 0.0017695214715786277\n",
      "Epoch 7: Average Train Loss: 0.0016966756207693833, Average Test Loss: 0.0016076479536422995\n",
      "Epoch 8: Average Train Loss: 0.0015676271767006255, Average Test Loss: 0.001522450581251178\n",
      "Epoch 9: Average Train Loss: 0.001495974242243392, Average Test Loss: 0.0014816406735917553\n",
      "Epoch 10: Average Train Loss: 0.0014567843648819689, Average Test Loss: 0.0014669116084405688\n",
      "Epoch 11: Average Train Loss: 0.001435726875297405, Average Test Loss: 0.001469265490595717\n",
      "Epoch 12: Average Train Loss: 0.0014242741953239602, Average Test Loss: 0.0014810538545134478\n",
      "Epoch 13: Average Train Loss: 0.0014169181312809088, Average Test Loss: 0.0014984206354711205\n",
      "Early stopping at epoch 13\n"
     ]
    }
   ],
   "source": [
    "def greedy_policy(q_table):\n",
    "    return torch.tensor(np.argmax(q_table, axis=1).reshape(-1, 1).astype(np.float32))\n",
    "def random_policy(state_dim):\n",
    "    return torch.randint(0, 6, (state_dim, 1)).float()\n",
    "\n",
    "dataset1 = [Data(x = greedy_policy(agent.q_table), edge_index = edge_index, y = 1) for agent in URS_agents]\n",
    "# dataset2 = [Data(x = greedy_policy(agent.q_table), edge_index = edge_index, y = 0) for agent in URS_agents]\n",
    "dataset2 = [Data(x = random_policy(agent.q_table.shape[0]), edge_index = edge_index, y = 0) for agent in UPS_agents]\n",
    "print(dataset1[0].x.shape)\n",
    "\n",
    "train_data, test_data, num_node_features = generate_data(dataset1, dataset2)\n",
    "model = GraphLevelGCN(num_node_features)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "train_classifier(model, criterion, optimizer, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 2., 1., 1., 1., 0., 1., 2., 3., 1., 0., 3., 1., 1., 3., 0., 5., 0.,\n",
      "         3., 3., 0., 1., 3., 3., 0., 0., 2., 3., 3., 1., 0., 3., 0., 0., 3., 0.,\n",
      "         2., 1., 3., 2., 0., 3., 3., 3., 1., 0., 1., 1., 3., 1.]])\n",
      "tensor([[5., 0., 0., 4., 3., 0., 2., 0., 1., 1., 2., 4., 5., 1., 4., 5., 1., 0.,\n",
      "         2., 5., 3., 4., 0., 3., 4., 4., 4., 0., 4., 3., 4., 5., 3., 4., 2., 0.,\n",
      "         1., 4., 2., 5., 4., 0., 4., 2., 5., 0., 1., 0., 3., 4.]])\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.0076]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.transpose(dataset1[0].x, 0, -1)[:, 0:50]) # Example greedy policies\n",
    "print(torch.transpose(dataset2[0].x, 0, -1)[:, 0:50])\n",
    "print(model.forward(dataset1[0]))\n",
    "print(model.forward(dataset2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000, Total reward: -299, Epsilon: 0.14\n",
      "Episode: 2000, Total reward: -112, Epsilon: 0.03\n",
      "Episode: 3000, Total reward: -200, Epsilon: 0.01\n",
      "tensor([[0.9997]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "taxi_model = train_qtable(env_name = \"Taxi-v3\", episodes = 3000, verbose = True, print_every = 1000, \n",
    "                          return_reward = False)\n",
    "taxi_data = Data(x = torch.tensor(greedy_policy(taxi_model.q_table)), edge_index = edge_index)\n",
    "print(model.forward(taxi_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
