{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup:\n",
    "- Starting point: just try to train classifier on RL policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DQN implementation\n",
    "\n",
    "# Define the Q-network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "\n",
    "# Experience Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=1e-2, batch_size=64, gamma=0.99, replay_size=1000):\n",
    "        self.model = DQN(state_dim, action_dim, hidden_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.replay_buffer = ReplayBuffer(replay_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        state = torch.FloatTensor(state)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        if len(state.shape) == 1:\n",
    "            state = state.reshape(-1, 1)\n",
    "        if len(next_state.shape) == 1:\n",
    "            next_state = next_state.reshape(-1, 1)\n",
    "        action = torch.LongTensor(action)\n",
    "        reward = torch.FloatTensor(reward)\n",
    "        done = torch.FloatTensor(done)\n",
    "\n",
    "        q_values = self.model.forward(state)\n",
    "        next_q_values = self.model.forward(next_state)\n",
    "\n",
    "        # state = state.T\n",
    "        # next_state = next_state.T\n",
    "        \n",
    "        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        next_q_value = next_q_values.max(1)[0]\n",
    "        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "        \n",
    "        loss = nn.MSELoss()(q_value, expected_q_value.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.FloatTensor(np.expand_dims(state, 0))\n",
    "            q_value = self.model(state)\n",
    "            action = q_value.max(-1)[1].item()\n",
    "        else:\n",
    "            action = random.randrange(self.action_dim)\n",
    "        return action\n",
    "    \n",
    "class QTableAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-2, gamma=0.99):\n",
    "        self.q_table = np.zeros((state_dim, action_dim))\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        q_value = self.q_table[state, action]\n",
    "        next_q_value = np.max(self.q_table[next_state])\n",
    "        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "        \n",
    "        self.q_table[state, action] += self.lr * (expected_q_value - q_value)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            action = np.argmax(self.q_table[state])\n",
    "        else:\n",
    "            action = random.randrange(self.action_dim)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NON_ZERO_REWARDS = 0\n",
    "def one_hot_state(state, env):\n",
    "    state_arr = np.zeros(env.observation_space.n)\n",
    "    state_arr[state] = 1\n",
    "    return state_arr\n",
    "\n",
    "def train_dqn(env_name=\"CartPole-v1\", episodes=500, epsilon_start=1.0, epsilon_final=0.01, \n",
    "              epsilon_decay=500, reward_function = None, verbose = False, return_reward = False, \n",
    "              print_every=50, **kwargs):\n",
    "    \"\"\"\n",
    "    Train a DQN agent on the specified environment.\n",
    "    \n",
    "    Args:\n",
    "        env_name: str\n",
    "            Name of the environment to train the agent on.\n",
    "        episodes: int\n",
    "            Number of episodes to train the agent for.\n",
    "        epsilon_start: float\n",
    "            Initial epsilon value for epsilon-greedy action selection.\n",
    "        epsilon_final: float\n",
    "            Final epsilon value for epsilon-greedy action selection.\n",
    "        epsilon_decay: float\n",
    "            Decay rate for epsilon.\n",
    "        reward_function: function\n",
    "            Optional reward function to use for training.\n",
    "        verbose: bool\n",
    "            Whether to print training progress.\n",
    "\n",
    "    Returns:\n",
    "        DQNAgent: trained DQN agent. \n",
    "    \"\"\"\n",
    "    global NUM_NON_ZERO_REWARDS\n",
    "    env = gym.make(env_name)\n",
    "    if len(env.observation_space.shape) == 0:\n",
    "        state_dim = env.observation_space.n\n",
    "    else:\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    agent = DQNAgent(state_dim, action_dim, **kwargs)\n",
    "    \n",
    "    epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay)\n",
    "    \n",
    "    rewards = np.zeros(episodes) \n",
    "    is_state_discrete = hasattr(env.observation_space, 'n')\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset() # Reset the environment, reward\n",
    "        if is_state_discrete and state_dim == env.observation_space.n:\n",
    "            state = one_hot_state(state, env)\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            epsilon = epsilon_by_frame(episode)\n",
    "            # One-hot encode the state\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if is_state_discrete and state_dim == env.observation_space.n:\n",
    "                next_state = one_hot_state(next_state, env)\n",
    "\n",
    "            if reward_function: #custom reward function\n",
    "                reward = reward_function(done, state, action, next_state)\n",
    "            NUM_NON_ZERO_REWARDS += 0 if math.isclose(reward, 0) else 1\n",
    "            \n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            agent.update()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            # print(f\"Episode: {episode+1}, Total reward: {episode_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "        rewards[episode] = episode_reward\n",
    "        # Optional: Render the environment to visualize training progress\n",
    "        if verbose and episode % print_every == print_every - 1:\n",
    "        #     render_env(env, agent)\n",
    "            print(f\"Episode: {episode+1}, Average total reward: {np.average(rewards[episode - print_every + 1 : episode])}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    return agent if not return_reward else (agent, rewards)\n",
    "\n",
    "# Optional: Function to render the environment with the current policy\n",
    "def render_env(env, agent):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state, 0)  # Using 0 epsilon for greedy action selection\n",
    "        # print(env.step(action))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qtable(env_name=\"CartPole-v1\", episodes=500, epsilon_start=1.0, epsilon_final=0.01, \n",
    "              epsilon_decay=500, reward_function = None, verbose = False, return_reward = False, \n",
    "              print_every=50, **kwargs):\n",
    "    \"\"\"\n",
    "    Train a Q-table agent on the specified environment.\"\"\"\n",
    "    global NUM_NON_ZERO_REWARDS\n",
    "    env = gym.make(env_name)\n",
    "    if len(env.observation_space.shape) == 0:\n",
    "        state_dim = env.observation_space.n\n",
    "    else:\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    agent = QTableAgent(state_dim, action_dim, **kwargs)\n",
    "\n",
    "    rewards = np.zeros(episodes)\n",
    "    epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay)\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            epsilon = epsilon_by_frame(episode)\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if reward_function:\n",
    "                reward = reward_function(done, state, action, next_state)\n",
    "\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards[episode] = episode_reward\n",
    "        if verbose and episode % print_every == print_every - 1:\n",
    "            print(f\"Episode: {episode+1}, Average total reward: {np.average(rewards[episode - print_every + 1 : episode])}, Epsilon: {epsilon:.2f}\")\n",
    "        \n",
    "    env.close()\n",
    "    return agent if not return_reward else (agent, rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEAR_ZERO = 1e-9\n",
    "def test_dqn(env, agent, episodes=10, reward_function=None, verbose = False):\n",
    "    print(f\"Maximum reward: {env.spec.reward_threshold}\")\n",
    "    average_value = 0\n",
    "    for episode in range(episodes):\n",
    "        # if episode == 0:\n",
    "        #     render_env(env, agent)\n",
    "        state = env.reset()\n",
    "        if len(env.observation_space.shape) == 0:\n",
    "            state = one_hot_state(state, env)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state, 0)  # Using 0 epsilon for greedy action selection\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if len(env.observation_space.shape) == 0:\n",
    "                next_state = one_hot_state(next_state, env)\n",
    "            if reward_function:\n",
    "                reward = reward_function(done, state, action, next_state)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        if verbose:\n",
    "            print(f\"Episode: {episode+1}, Total reward: {episode_reward}\")\n",
    "        average_value += episode_reward\n",
    "    average_value /= episodes\n",
    "    print(f\"Average reward: {average_value}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qtable(env, agent, episodes=10, reward_function=None, verbose = False):\n",
    "    \"\"\"\n",
    "    Test a Q-table agent on the specified environment.\n",
    "    (This is basically test_dqn but without the one-hot encoding.)\n",
    "    \"\"\"\n",
    "    print(f\"Maximum reward: {env.spec.reward_threshold}\")\n",
    "    average_value = 0\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state, 0)  # Using 0 epsilon for greedy action selection\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if reward_function:\n",
    "                reward = reward_function(done, state, action, next_state)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        if verbose:\n",
    "            print(f\"Episode: {episode+1}, Total reward: {episode_reward}\")\n",
    "        average_value += episode_reward\n",
    "    average_value /= episodes\n",
    "    print(f\"Average reward: {average_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, Average total reward: -398.55102040816325, Epsilon: 0.02\n",
      "Episode: 100, Average total reward: -222.81632653061226, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 150, Average total reward: -191.3469387755102, Epsilon: 0.01\n",
      "Episode: 200, Average total reward: -179.83673469387756, Epsilon: 0.01\n",
      "Episode: 250, Average total reward: -163.0, Epsilon: 0.01\n",
      "Episode: 300, Average total reward: -137.6734693877551, Epsilon: 0.01\n",
      "Episode: 350, Average total reward: -128.9795918367347, Epsilon: 0.01\n",
      "Episode: 400, Average total reward: -112.12244897959184, Epsilon: 0.01\n",
      "Episode: 450, Average total reward: -79.36734693877551, Epsilon: 0.01\n",
      "Episode: 500, Average total reward: -80.20408163265306, Epsilon: 0.01\n",
      "Episode: 550, Average total reward: -60.6530612244898, Epsilon: 0.01\n",
      "Episode: 600, Average total reward: -42.734693877551024, Epsilon: 0.01\n",
      "Episode: 650, Average total reward: -44.69387755102041, Epsilon: 0.01\n",
      "Episode: 700, Average total reward: -35.08163265306123, Epsilon: 0.01\n",
      "Episode: 750, Average total reward: -29.285714285714285, Epsilon: 0.01\n",
      "Episode: 800, Average total reward: -22.081632653061224, Epsilon: 0.01\n",
      "Episode: 850, Average total reward: -12.020408163265307, Epsilon: 0.01\n",
      "Episode: 900, Average total reward: -15.36734693877551, Epsilon: 0.01\n",
      "Episode: 950, Average total reward: -9.591836734693878, Epsilon: 0.01\n",
      "Episode: 1000, Average total reward: -9.183673469387756, Epsilon: 0.01\n",
      "Episode: 1050, Average total reward: -15.040816326530612, Epsilon: 0.01\n",
      "Episode: 1100, Average total reward: -5.653061224489796, Epsilon: 0.01\n",
      "Episode: 1150, Average total reward: -4.551020408163265, Epsilon: 0.01\n",
      "Episode: 1200, Average total reward: 0.24489795918367346, Epsilon: 0.01\n",
      "Episode: 1250, Average total reward: 0.9387755102040817, Epsilon: 0.01\n",
      "Episode: 1300, Average total reward: 1.1224489795918366, Epsilon: 0.01\n",
      "Episode: 1350, Average total reward: 5.285714285714286, Epsilon: 0.01\n",
      "Episode: 1400, Average total reward: 5.816326530612245, Epsilon: 0.01\n",
      "Episode: 1450, Average total reward: 4.816326530612245, Epsilon: 0.01\n",
      "Episode: 1500, Average total reward: 2.9591836734693877, Epsilon: 0.01\n",
      "Episode: 1550, Average total reward: 5.346938775510204, Epsilon: 0.01\n",
      "Episode: 1600, Average total reward: 5.469387755102041, Epsilon: 0.01\n",
      "Episode: 1650, Average total reward: 6.775510204081633, Epsilon: 0.01\n",
      "Episode: 1700, Average total reward: 4.551020408163265, Epsilon: 0.01\n",
      "Episode: 1750, Average total reward: 6.285714285714286, Epsilon: 0.01\n",
      "Episode: 1800, Average total reward: 6.204081632653061, Epsilon: 0.01\n",
      "Episode: 1850, Average total reward: 6.6938775510204085, Epsilon: 0.01\n",
      "Episode: 1900, Average total reward: 7.816326530612245, Epsilon: 0.01\n",
      "Episode: 1950, Average total reward: 4.469387755102041, Epsilon: 0.01\n",
      "Episode: 2000, Average total reward: 6.775510204081633, Epsilon: 0.01\n",
      "Episode: 2050, Average total reward: 5.775510204081633, Epsilon: 0.01\n",
      "Episode: 2100, Average total reward: 6.816326530612245, Epsilon: 0.01\n",
      "Episode: 2150, Average total reward: 6.530612244897959, Epsilon: 0.01\n",
      "Episode: 2200, Average total reward: 8.714285714285714, Epsilon: 0.01\n",
      "Episode: 2250, Average total reward: 7.653061224489796, Epsilon: 0.01\n",
      "Episode: 2300, Average total reward: 6.959183673469388, Epsilon: 0.01\n",
      "Episode: 2350, Average total reward: 7.714285714285714, Epsilon: 0.01\n",
      "Episode: 2400, Average total reward: 5.0, Epsilon: 0.01\n",
      "Episode: 2450, Average total reward: 7.489795918367347, Epsilon: 0.01\n",
      "Episode: 2500, Average total reward: 7.122448979591836, Epsilon: 0.01\n",
      "Episode: 2550, Average total reward: 6.938775510204081, Epsilon: 0.01\n",
      "Episode: 2600, Average total reward: 8.204081632653061, Epsilon: 0.01\n",
      "Episode: 2650, Average total reward: 6.979591836734694, Epsilon: 0.01\n",
      "Episode: 2700, Average total reward: 6.877551020408164, Epsilon: 0.01\n",
      "Episode: 2750, Average total reward: 5.673469387755102, Epsilon: 0.01\n",
      "Episode: 2800, Average total reward: 7.918367346938775, Epsilon: 0.01\n",
      "Episode: 2850, Average total reward: 6.877551020408164, Epsilon: 0.01\n",
      "Episode: 2900, Average total reward: 6.428571428571429, Epsilon: 0.01\n",
      "Episode: 2950, Average total reward: 7.755102040816326, Epsilon: 0.01\n",
      "Episode: 3000, Average total reward: 7.469387755102041, Epsilon: 0.01\n",
      "Episode: 3050, Average total reward: 7.26530612244898, Epsilon: 0.01\n",
      "Episode: 3100, Average total reward: 7.387755102040816, Epsilon: 0.01\n",
      "Episode: 3150, Average total reward: 7.63265306122449, Epsilon: 0.01\n",
      "Episode: 3200, Average total reward: 6.673469387755102, Epsilon: 0.01\n",
      "Episode: 3250, Average total reward: 6.183673469387755, Epsilon: 0.01\n",
      "Episode: 3300, Average total reward: 7.3061224489795915, Epsilon: 0.01\n",
      "Episode: 3350, Average total reward: 6.877551020408164, Epsilon: 0.01\n",
      "Episode: 3400, Average total reward: 6.938775510204081, Epsilon: 0.01\n",
      "Episode: 3450, Average total reward: 7.204081632653061, Epsilon: 0.01\n",
      "Episode: 3500, Average total reward: 6.836734693877551, Epsilon: 0.01\n",
      "Episode: 3550, Average total reward: 6.571428571428571, Epsilon: 0.01\n",
      "Episode: 3600, Average total reward: 8.326530612244898, Epsilon: 0.01\n",
      "Episode: 3650, Average total reward: 7.020408163265306, Epsilon: 0.01\n",
      "Episode: 3700, Average total reward: 7.326530612244898, Epsilon: 0.01\n",
      "Episode: 3750, Average total reward: 7.795918367346939, Epsilon: 0.01\n",
      "Episode: 3800, Average total reward: 8.204081632653061, Epsilon: 0.01\n",
      "Episode: 3850, Average total reward: 7.3061224489795915, Epsilon: 0.01\n",
      "Episode: 3900, Average total reward: 7.285714285714286, Epsilon: 0.01\n",
      "Episode: 3950, Average total reward: 7.918367346938775, Epsilon: 0.01\n",
      "Episode: 4000, Average total reward: 7.346938775510204, Epsilon: 0.01\n",
      "Episode: 4050, Average total reward: 6.938775510204081, Epsilon: 0.01\n",
      "Episode: 4100, Average total reward: 7.530612244897959, Epsilon: 0.01\n",
      "Episode: 4150, Average total reward: 7.877551020408164, Epsilon: 0.01\n",
      "Episode: 4200, Average total reward: 7.183673469387755, Epsilon: 0.01\n",
      "Episode: 4250, Average total reward: 7.510204081632653, Epsilon: 0.01\n",
      "Episode: 4300, Average total reward: 7.428571428571429, Epsilon: 0.01\n",
      "Episode: 4350, Average total reward: 8.10204081632653, Epsilon: 0.01\n",
      "Episode: 4400, Average total reward: 7.36734693877551, Epsilon: 0.01\n",
      "Episode: 4450, Average total reward: 6.571428571428571, Epsilon: 0.01\n",
      "Episode: 4500, Average total reward: 7.081632653061225, Epsilon: 0.01\n",
      "Episode: 4550, Average total reward: 6.918367346938775, Epsilon: 0.01\n",
      "Episode: 4600, Average total reward: 7.571428571428571, Epsilon: 0.01\n",
      "Episode: 4650, Average total reward: 8.183673469387756, Epsilon: 0.01\n",
      "Episode: 4700, Average total reward: 7.591836734693878, Epsilon: 0.01\n",
      "Episode: 4750, Average total reward: 7.469387755102041, Epsilon: 0.01\n",
      "Episode: 4800, Average total reward: 7.73469387755102, Epsilon: 0.01\n",
      "Episode: 4850, Average total reward: 7.836734693877551, Epsilon: 0.01\n",
      "Episode: 4900, Average total reward: 7.510204081632653, Epsilon: 0.01\n",
      "Episode: 4950, Average total reward: 7.142857142857143, Epsilon: 0.01\n",
      "Episode: 5000, Average total reward: 7.448979591836735, Epsilon: 0.01\n",
      "Episode: 5050, Average total reward: 7.326530612244898, Epsilon: 0.01\n",
      "Episode: 5100, Average total reward: 6.448979591836735, Epsilon: 0.01\n",
      "Episode: 5150, Average total reward: 6.673469387755102, Epsilon: 0.01\n",
      "Episode: 5200, Average total reward: 8.448979591836734, Epsilon: 0.01\n",
      "Episode: 5250, Average total reward: 7.714285714285714, Epsilon: 0.01\n",
      "Episode: 5300, Average total reward: 7.775510204081633, Epsilon: 0.01\n",
      "Episode: 5350, Average total reward: 7.081632653061225, Epsilon: 0.01\n",
      "Episode: 5400, Average total reward: 7.469387755102041, Epsilon: 0.01\n",
      "Episode: 5450, Average total reward: 7.714285714285714, Epsilon: 0.01\n",
      "Episode: 5500, Average total reward: 6.653061224489796, Epsilon: 0.01\n",
      "Episode: 5550, Average total reward: 7.673469387755102, Epsilon: 0.01\n",
      "Episode: 5600, Average total reward: 8.122448979591837, Epsilon: 0.01\n",
      "Episode: 5650, Average total reward: 7.979591836734694, Epsilon: 0.01\n",
      "Episode: 5700, Average total reward: 7.775510204081633, Epsilon: 0.01\n",
      "Episode: 5750, Average total reward: 7.836734693877551, Epsilon: 0.01\n",
      "Episode: 5800, Average total reward: 7.489795918367347, Epsilon: 0.01\n",
      "Episode: 5850, Average total reward: 7.142857142857143, Epsilon: 0.01\n",
      "Episode: 5900, Average total reward: 6.795918367346939, Epsilon: 0.01\n",
      "Episode: 5950, Average total reward: 7.755102040816326, Epsilon: 0.01\n",
      "Episode: 6000, Average total reward: 6.857142857142857, Epsilon: 0.01\n",
      "Episode: 6050, Average total reward: 7.224489795918367, Epsilon: 0.01\n",
      "Episode: 6100, Average total reward: 7.959183673469388, Epsilon: 0.01\n",
      "Episode: 6150, Average total reward: 7.224489795918367, Epsilon: 0.01\n",
      "Episode: 6200, Average total reward: 7.673469387755102, Epsilon: 0.01\n",
      "Episode: 6250, Average total reward: 7.489795918367347, Epsilon: 0.01\n",
      "Episode: 6300, Average total reward: 7.836734693877551, Epsilon: 0.01\n",
      "Episode: 6350, Average total reward: 6.63265306122449, Epsilon: 0.01\n",
      "Episode: 6400, Average total reward: 7.081632653061225, Epsilon: 0.01\n",
      "Episode: 6450, Average total reward: 7.6938775510204085, Epsilon: 0.01\n",
      "Episode: 6500, Average total reward: 6.591836734693878, Epsilon: 0.01\n",
      "Episode: 6550, Average total reward: 6.979591836734694, Epsilon: 0.01\n",
      "Episode: 6600, Average total reward: 7.26530612244898, Epsilon: 0.01\n",
      "Episode: 6650, Average total reward: 7.469387755102041, Epsilon: 0.01\n",
      "Episode: 6700, Average total reward: 7.3061224489795915, Epsilon: 0.01\n",
      "Episode: 6750, Average total reward: 7.6938775510204085, Epsilon: 0.01\n",
      "Episode: 6800, Average total reward: 7.775510204081633, Epsilon: 0.01\n",
      "Episode: 6850, Average total reward: 8.061224489795919, Epsilon: 0.01\n",
      "Episode: 6900, Average total reward: 8.46938775510204, Epsilon: 0.01\n",
      "Episode: 6950, Average total reward: 6.836734693877551, Epsilon: 0.01\n",
      "Episode: 7000, Average total reward: 6.959183673469388, Epsilon: 0.01\n",
      "Episode: 7050, Average total reward: 7.918367346938775, Epsilon: 0.01\n",
      "Episode: 7100, Average total reward: 8.46938775510204, Epsilon: 0.01\n",
      "Episode: 7150, Average total reward: 6.6938775510204085, Epsilon: 0.01\n",
      "Episode: 7200, Average total reward: 7.36734693877551, Epsilon: 0.01\n",
      "Episode: 7250, Average total reward: 7.510204081632653, Epsilon: 0.01\n",
      "Episode: 7300, Average total reward: 6.326530612244898, Epsilon: 0.01\n",
      "Episode: 7350, Average total reward: 7.510204081632653, Epsilon: 0.01\n",
      "Episode: 7400, Average total reward: 7.73469387755102, Epsilon: 0.01\n",
      "Episode: 7450, Average total reward: 7.530612244897959, Epsilon: 0.01\n",
      "Episode: 7500, Average total reward: 7.020408163265306, Epsilon: 0.01\n",
      "Episode: 7550, Average total reward: 7.571428571428571, Epsilon: 0.01\n",
      "Episode: 7600, Average total reward: 6.877551020408164, Epsilon: 0.01\n",
      "Episode: 7650, Average total reward: 7.530612244897959, Epsilon: 0.01\n",
      "Episode: 7700, Average total reward: 6.938775510204081, Epsilon: 0.01\n",
      "Episode: 7750, Average total reward: 7.571428571428571, Epsilon: 0.01\n",
      "Episode: 7800, Average total reward: 7.469387755102041, Epsilon: 0.01\n",
      "Episode: 7850, Average total reward: 7.387755102040816, Epsilon: 0.01\n",
      "Episode: 7900, Average total reward: 8.204081632653061, Epsilon: 0.01\n",
      "Episode: 7950, Average total reward: 7.387755102040816, Epsilon: 0.01\n",
      "Episode: 8000, Average total reward: 7.489795918367347, Epsilon: 0.01\n",
      "Episode: 8050, Average total reward: 7.469387755102041, Epsilon: 0.01\n",
      "Episode: 8100, Average total reward: 7.3061224489795915, Epsilon: 0.01\n",
      "Episode: 8150, Average total reward: 6.244897959183674, Epsilon: 0.01\n",
      "Episode: 8200, Average total reward: 6.938775510204081, Epsilon: 0.01\n",
      "Episode: 8250, Average total reward: 8.16326530612245, Epsilon: 0.01\n",
      "Episode: 8300, Average total reward: 8.020408163265307, Epsilon: 0.01\n",
      "Episode: 8350, Average total reward: 7.795918367346939, Epsilon: 0.01\n",
      "Episode: 8400, Average total reward: 8.122448979591837, Epsilon: 0.01\n",
      "Episode: 8450, Average total reward: 7.408163265306122, Epsilon: 0.01\n",
      "Episode: 8500, Average total reward: 7.346938775510204, Epsilon: 0.01\n",
      "Episode: 8550, Average total reward: 7.326530612244898, Epsilon: 0.01\n",
      "Episode: 8600, Average total reward: 7.081632653061225, Epsilon: 0.01\n",
      "Episode: 8650, Average total reward: 7.714285714285714, Epsilon: 0.01\n",
      "Episode: 8700, Average total reward: 7.3061224489795915, Epsilon: 0.01\n",
      "Episode: 8750, Average total reward: 6.918367346938775, Epsilon: 0.01\n",
      "Episode: 8800, Average total reward: 7.36734693877551, Epsilon: 0.01\n",
      "Episode: 8850, Average total reward: 8.224489795918368, Epsilon: 0.01\n",
      "Episode: 8900, Average total reward: 6.836734693877551, Epsilon: 0.01\n",
      "Episode: 8950, Average total reward: 6.26530612244898, Epsilon: 0.01\n",
      "Episode: 9000, Average total reward: 7.6938775510204085, Epsilon: 0.01\n",
      "Episode: 9050, Average total reward: 7.857142857142857, Epsilon: 0.01\n",
      "Episode: 9100, Average total reward: 7.326530612244898, Epsilon: 0.01\n",
      "Episode: 9150, Average total reward: 7.387755102040816, Epsilon: 0.01\n",
      "Episode: 9200, Average total reward: 6.979591836734694, Epsilon: 0.01\n",
      "Episode: 9250, Average total reward: 7.6938775510204085, Epsilon: 0.01\n",
      "Episode: 9300, Average total reward: 8.36734693877551, Epsilon: 0.01\n",
      "Episode: 9350, Average total reward: 7.795918367346939, Epsilon: 0.01\n",
      "Episode: 9400, Average total reward: 8.122448979591837, Epsilon: 0.01\n",
      "Episode: 9450, Average total reward: 7.408163265306122, Epsilon: 0.01\n",
      "Episode: 9500, Average total reward: 7.122448979591836, Epsilon: 0.01\n",
      "Episode: 9550, Average total reward: 7.428571428571429, Epsilon: 0.01\n",
      "Episode: 9600, Average total reward: 7.551020408163265, Epsilon: 0.01\n",
      "Episode: 9650, Average total reward: 7.510204081632653, Epsilon: 0.01\n",
      "Episode: 9700, Average total reward: 7.122448979591836, Epsilon: 0.01\n",
      "Episode: 9750, Average total reward: 7.73469387755102, Epsilon: 0.01\n",
      "Episode: 9800, Average total reward: 7.653061224489796, Epsilon: 0.01\n",
      "Episode: 9850, Average total reward: 7.551020408163265, Epsilon: 0.01\n",
      "Episode: 9900, Average total reward: 6.836734693877551, Epsilon: 0.01\n",
      "Episode: 9950, Average total reward: 7.387755102040816, Epsilon: 0.01\n",
      "Episode: 10000, Average total reward: 7.285714285714286, Epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "env_name = \"Taxi-v3\"\n",
    "agent, rewards = train_qtable(env_name = env_name, episodes = 10000, verbose = True, return_reward = True,\n",
    "                           epsilon_decay=10, lr=0.1, gamma=0.9)\n",
    "# rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum reward: 8\n",
      "Average reward: 8.12\n"
     ]
    }
   ],
   "source": [
    "test_qtable(gym.make(env_name), agent, episodes = 100)\n",
    "# agent.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, Average total reward: 18.102040816326532, Epsilon: 0.91\n",
      "Episode: 100, Average total reward: 22.224489795918366, Epsilon: 0.82\n",
      "Episode: 150, Average total reward: 20.612244897959183, Epsilon: 0.74\n",
      "Episode: 200, Average total reward: 17.408163265306122, Epsilon: 0.67\n",
      "Episode: 250, Average total reward: 16.53061224489796, Epsilon: 0.61\n",
      "Episode: 300, Average total reward: 16.163265306122447, Epsilon: 0.55\n",
      "Episode: 350, Average total reward: 14.755102040816327, Epsilon: 0.50\n",
      "Episode: 400, Average total reward: 12.755102040816327, Epsilon: 0.46\n",
      "Episode: 450, Average total reward: 13.0, Epsilon: 0.41\n",
      "Episode: 500, Average total reward: 12.0, Epsilon: 0.37\n",
      "Episode: 550, Average total reward: 13.63265306122449, Epsilon: 0.34\n",
      "Episode: 600, Average total reward: 14.142857142857142, Epsilon: 0.31\n",
      "Episode: 650, Average total reward: 15.428571428571429, Epsilon: 0.28\n",
      "Episode: 700, Average total reward: 18.26530612244898, Epsilon: 0.25\n",
      "Episode: 750, Average total reward: 45.673469387755105, Epsilon: 0.23\n",
      "Episode: 800, Average total reward: 26.816326530612244, Epsilon: 0.21\n",
      "Episode: 850, Average total reward: 23.428571428571427, Epsilon: 0.19\n",
      "Episode: 900, Average total reward: 21.408163265306122, Epsilon: 0.17\n",
      "Episode: 950, Average total reward: 18.816326530612244, Epsilon: 0.16\n",
      "Episode: 1000, Average total reward: 18.102040816326532, Epsilon: 0.14\n"
     ]
    }
   ],
   "source": [
    "### Coherence classifier\n",
    "\n",
    "#agent.model.get_weights()\n",
    "\n",
    "# Define a simple GCN model\n",
    "from torch_geometric.data import Data\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super(GCN, self).__init__()\n",
    "        # Define the GCN layers\n",
    "        self.conv1 = GCNConv(data.num_node_features, 4)  # Input features to hidden\n",
    "        self.conv2 = GCNConv(4, 2)  # Hidden to output features\n",
    "        self.data = data\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # Pass data through the first GCN layer, then apply ReLU\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        # Pass data through the second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def nn_to_data(model: nn.Module) -> Data:\n",
    "    edges = []\n",
    "\n",
    "    # Counter for global neuron index\n",
    "    idx = 0\n",
    "\n",
    "    # Iterate over each layer in the network\n",
    "    base = next(model.children())\n",
    "    if isinstance(base, nn.Sequential):\n",
    "        layers = list(base.children())\n",
    "        layers2 = list(base.children())\n",
    "    else:\n",
    "        layers = list(model.children()) # iterator over the layers of the model\n",
    "        layers2 = list(model.children())\n",
    "    \n",
    "    num_nodes = layers2[0].weight.shape[1] + sum([layer.weight.shape[0] for layer in layers2 if isinstance(layer, nn.Linear)])\n",
    "    num_node_features = num_nodes\n",
    "    node_features = torch.zeros(num_nodes, num_node_features)\n",
    "    # shape = (num_nodes, num_node_features), where the node features are the bias of each node\n",
    "    # and the weights of the edges to each node (zero if there is no edge)\n",
    "\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # Update edges based on the weight matrix\n",
    "            input_dim = layer.weight.shape[1]\n",
    "            output_dim = layer.weight.shape[0]\n",
    "            for i in range(input_dim):  # Input neurons (e.g. 4)\n",
    "                for j in range(output_dim):  # Output neurons (e.g. 64)\n",
    "                    edges.append((idx + i, idx + input_dim + j))\n",
    "            \n",
    "            # Update node features (e.g., biases)\n",
    "            biases = torch.tensor(layer.bias.detach().numpy())\n",
    "            edge_weights = torch.tensor(layer.weight.detach().numpy().T)\n",
    "            node_features[idx + input_dim:idx + input_dim + output_dim, 0] = biases\n",
    "            node_features[idx:idx + input_dim, 1+idx:1+idx+output_dim] = edge_weights\n",
    "            node_features[idx + input_dim:idx + input_dim + output_dim, 1+idx:1+idx+input_dim] = edge_weights.T\n",
    "            \n",
    "            # Update the global neuron index\n",
    "            idx += input_dim\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    num_nonzero = [np.count_nonzero(node_features[i]) for i in range(node_features.shape[0])]\n",
    "    # print(num_nonzero)\n",
    "    row_mean, row_median, row_var = torch.mean(node_features[:, 1:], dim=1), torch.median(node_features[:, 1:], dim=1)[0], torch.var(node_features[:, 1:], dim=1)\n",
    "    x = torch.stack([node_features[:, 0], row_mean, row_median, row_var]).T\n",
    "    # print(x.shape)\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "agent = train_dqn(env_name = \"CartPole-v1\", episodes = 1000, verbose = True, return_reward = False)\n",
    "data = nn_to_data(agent.model)\n",
    "gcn = GCN(data)\n",
    "# data.x.shape, data.edge_index.shape\n",
    "# print(data.x)\n",
    "\n",
    "#Debug\n",
    "out_of_bounds = data.edge_index >= data.x.shape[0]\n",
    "if out_of_bounds.any():\n",
    "    print(\"Out-of-bounds indices found at locations:\")\n",
    "    print(data.edge_index[:, out_of_bounds.any(dim=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reward function calls: 58310\n",
      "Number of non-zero rewards: 58310\n"
     ]
    }
   ],
   "source": [
    "# Dataset generation\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "NEAR_ZERO = 1e-9\n",
    "NUM_REWARD_CALLS = 0\n",
    "NUM_NON_ZERO_REWARDS = 0\n",
    "def deterministic_random(*args, lb = -1, ub = 1, sparsity = 0.0, continuous = False):\n",
    "    \"\"\"\n",
    "    Create a deterministic random number generator for a given set of arguments.\n",
    "    Used to generate deterministic reward functions for the coherence classifier.\n",
    "    [Edit 4/3/24: adapted to continuous state space]\"\"\"\n",
    "    global NUM_REWARD_CALLS\n",
    "    NUM_REWARD_CALLS += 1\n",
    "    unique_seed = f\"{args}\".encode(\"utf-8\")\n",
    "    random.seed(unique_seed)\n",
    "    return random.uniform(lb, ub) if random.random() > sparsity else random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "\n",
    "NUM_TRAIN_R_FUNCS = 50\n",
    "NUM_EPS_TRAIN_R = 50\n",
    "URS_r_funcs = [lambda *args: deterministic_random(args) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "URS_agents = [train_dqn(env_name = env_name, \n",
    "                        episodes=NUM_EPS_TRAIN_R, reward_function=r_func) for r_func in URS_r_funcs]\n",
    "USS_r_funcs = [lambda *args: deterministic_random(args, sparsity=0.99) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "print(f\"Number of reward function calls: {NUM_REWARD_CALLS}\")\n",
    "print(f\"Number of non-zero rewards: {NUM_NON_ZERO_REWARDS}\")\n",
    "USS_agents = [train_dqn(env_name = env_name, \n",
    "                        episodes=NUM_EPS_TRAIN_R, reward_function=r_func) for r_func in USS_r_funcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6014137224608205,\n",
       " 5.734868810378968e-10,\n",
       " 0.18947200717913826,\n",
       " -0.11464719428521586,\n",
       " 1.9375194864306798e-11,\n",
       " 3.1131227593489704e-10,\n",
       " -7.023178277046693e-10,\n",
       " 2.965355797951794e-10,\n",
       " 0.41831271768541045,\n",
       " -3.207247699354683e-10]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if deterministic_random is deterministic and has the correct sparsity\n",
    "assert deterministic_random(1, 2, 3, 4) == deterministic_random(1, 2, 3, 4)\n",
    "assert not deterministic_random(1, 2, 3, 4) == deterministic_random(1, 2, 3, 6)\n",
    "[deterministic_random(1, 2, 3, i, sparsity = 0.5) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, Average total reward: -0.7109034800800098, Epsilon: 0.91\n",
      "Episode: 100, Average total reward: -0.17504186308399963, Epsilon: 0.82\n",
      "Episode: 150, Average total reward: -0.82087640624123, Epsilon: 0.74\n",
      "Episode: 200, Average total reward: -0.3467656369794918, Epsilon: 0.67\n",
      "Episode: 250, Average total reward: -0.3202516562303173, Epsilon: 0.61\n",
      "Episode: 300, Average total reward: -1.2871440511690573, Epsilon: 0.55\n",
      "Episode: 350, Average total reward: 0.14199784219845837, Epsilon: 0.50\n",
      "Episode: 400, Average total reward: 0.37669043104779626, Epsilon: 0.46\n",
      "Episode: 450, Average total reward: -0.5010131068566521, Epsilon: 0.41\n",
      "Episode: 500, Average total reward: -0.5665183593912818, Epsilon: 0.37\n"
     ]
    }
   ],
   "source": [
    "# Test when do USS agents have non-zero rewards\n",
    "env_name = \"CartPole-v1\"\n",
    "USS_test_r_func = lambda *args: deterministic_random(args, sparsity=0.0)\n",
    "assert USS_test_r_func(42) == USS_test_r_func(42)\n",
    "USS_test_agent = train_dqn(env_name = env_name, episodes=500, reward_function=USS_test_r_func, \n",
    "                           verbose = True)\n",
    "# Epsilon measuring how much the agent is exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum reward: 475.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: -0.5560146586302399\n"
     ]
    }
   ],
   "source": [
    "# epsilon_final, epsilon_start, epsilon_decay = 0.01, 1.0, 500\n",
    "# [epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay) for frame_idx in range(500)]\n",
    "test_dqn(gym.make(env_name), USS_test_agent, reward_function=USS_test_r_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([134, 4])\n"
     ]
    }
   ],
   "source": [
    "def get_state_shape(env):\n",
    "    return 1 if len(env.observation_space.shape) == 0 else env.observation_space.shape[0]\n",
    "def get_state_size(env):\n",
    "    return env.observation_space.n if len(env.observation_space.shape) == 0 else env.observation_space.shape[0]\n",
    "UPS_agents = [DQNAgent(get_state_size(env), env.action_space.n) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool, GCNConv, GATConv\n",
    "\n",
    "class GraphLevelGCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(GraphLevelGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 16)\n",
    "        self.linear = torch.nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        # edge_weights = data.edge_attr\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Aggregate node features to graph-level features\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Make a binary classification prediction\n",
    "        x = self.linear(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class GATGraphLevelBinary(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(GATGraphLevelBinary, self).__init__()\n",
    "        self.conv1 = GATConv(num_node_features, 8, heads=8, dropout=0.6)\n",
    "        # Increase the number of output features from the first GAT layer\n",
    "        self.conv2 = GATConv(8 * 8, 16, heads=1, concat=False, dropout=0.6)\n",
    "        # Additional GAT layer for richer node representations\n",
    "        self.linear = torch.nn.Linear(16, 1)\n",
    "        # Final linear layer to produce a graph-level output\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)  # Aggregate node features to graph-level\n",
    "        x = self.linear(x)\n",
    "        return torch.sigmoid(x)  # Sigmoid activation function for binary classification\n",
    "\n",
    "# Training loop\n",
    "USS_data = [nn_to_data(agent.model) for agent in USS_agents]\n",
    "URS_data = [nn_to_data(agent.model) for agent in URS_agents]\n",
    "print(URS_data[0].x.shape)\n",
    "UPS_data = [nn_to_data(agent.model) for agent in UPS_agents]\n",
    "assert URS_data[0].x.shape == UPS_data[0].x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Train Loss: 0.96087555669219, Average Test Loss: 1.4202829338610172\n",
      "Epoch 2: Average Train Loss: 0.7309860734501854, Average Test Loss: 0.843775562942028\n",
      "Epoch 3: Average Train Loss: 0.7203299042768776, Average Test Loss: 0.7586588948965073\n",
      "Epoch 4: Average Train Loss: 0.7053088508546352, Average Test Loss: 0.7255915313959121\n",
      "Epoch 5: Average Train Loss: 0.6966455757617951, Average Test Loss: 0.7026014655828476\n",
      "Epoch 6: Average Train Loss: 0.7097975861281156, Average Test Loss: 0.7265136957168579\n",
      "Epoch 7: Average Train Loss: 0.6949755385518074, Average Test Loss: 0.7046276897192001\n",
      "Epoch 8: Average Train Loss: 0.694453526288271, Average Test Loss: 0.7069998249411583\n",
      "Early stopping at epoch 8\n"
     ]
    }
   ],
   "source": [
    "# Binary classification between two datasets\n",
    "dataset1 = USS_data\n",
    "dataset2 = URS_data\n",
    "def generate_data(dataset1, dataset2):\n",
    "    indices = np.random.permutation(len(dataset1) + len(dataset2))\n",
    "    data = [dataset1[i] if i < len(dataset1) else dataset2[i - len(dataset1)] for i in indices]\n",
    "    for i in range(len(data)):\n",
    "        data[i].y = 1.0 if indices[i] < len(dataset1) else 0.0 # Binary labels for each node; 1 = URS, 0 = UPS\n",
    "        # Hence roughly speaking, 1 = more coherent, 0 = less coherent\n",
    "\n",
    "    train_data_ratio = 0.8\n",
    "    train_data, test_data = data[:int(train_data_ratio * len(data))], data[int(train_data_ratio * len(data)):]\n",
    "    num_node_features = data[0].x.shape[1] # Number of features for each node\n",
    "    return train_data, test_data, num_node_features\n",
    "\n",
    "train_data, test_data, num_node_features = generate_data(dataset1, dataset2)\n",
    "# Loss and optimizer\n",
    "model = GraphLevelGCN(num_node_features)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "def train_classifier(model, criterion, optimizer, train_data, test_data, epochs = 40, patience = 3, \n",
    "                     epochs_without_improvement = 0, best_loss = float('inf')):\n",
    "    for epoch in range(epochs):\n",
    "        avg_train_loss = 0\n",
    "        for datapt in train_data:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # print(f\"datapt.x shape: {datapt.x.shape}\")  # Should be [num_nodes, num_node_features]\n",
    "            # print(f\"datapt.edge_index shape: {datapt.edge_index.shape}\")  # Should be [2, num_edges]\n",
    "            out = model.forward(datapt)\n",
    "            # print(out.size())\n",
    "            # print(torch.tensor([[datapt.y]]).size())\n",
    "            loss = criterion(out, torch.tensor([[datapt.y]]))  # Adjust shape as necessary\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_train_loss += loss.item()\n",
    "        avg_train_loss /= len(train_data)\n",
    "\n",
    "        avg_test_loss = 0\n",
    "        for datapt in test_data:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model.forward(datapt)\n",
    "                loss = criterion(out, torch.tensor([[datapt.y]]))\n",
    "                avg_test_loss += loss.item()\n",
    "        avg_test_loss /= len(test_data)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Average Train Loss: {avg_train_loss}, Average Test Loss: {avg_test_loss}')\n",
    "        \n",
    "        # Early Stopping\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "train_classifier(model, criterion, optimizer, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3876]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.3860]], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.3863]], grad_fn=<SigmoidBackward0>), tensor([[0.5091]], grad_fn=<SigmoidBackward0>), tensor([[0.5598]], grad_fn=<SigmoidBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "# Test GCN model on a \"more powerful\" NN\n",
    "print(model.forward(dataset1[0]))\n",
    "print(model.forward(dataset2[0]))\n",
    "powerful_models = [nn_to_data(train_dqn(env_name = env_name, episodes = 5 * i).model) \n",
    "                   for i in [1, 3, 10]]\n",
    "print([model.forward(data) for data in powerful_models])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The classifier training process is finicky -- sometimes it overfits, sometimes it underfits -- but sometimes can reach very low loss (< 0.002)\n",
    "- Even weak classifiers classify powerful models (a.k.a. agents with >15 episodes in CartPole) as having P(URS) = 1, corresponding to coherence ~ $\\infty$\n",
    "- P(USS) / P(URS) is still having trouble as a metric; seems extremely difficult to detect differences between USS and URS-generated policies here with current methods\n",
    "    - We will need some kind of \"more advanced\" coherence metric to distinguish more advanced policies; TODO: implement UUS somehow\n",
    "- Adding node weights to every other node to the features passed into the GCN (such that, in CartPole, the data matrix has shape (134, 134) instead of (134, 1)) makes the GCN much worse, probably because of higher dimensionality\n",
    "    - Using attention in the GNN does not help, and in fact actively overfits when using (134, 1) data\n",
    "- Even with sparsity = 0.999, USS is still hard to distinguish\n",
    "- For simpler discrete environments, maybe a Q-table is enough to solve the problem\n",
    "- Takes >= 500 episodes, small epsilon to effectively learn DQN policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now classifying q-table agents\n",
    "env_name = \"Taxi-v3\"\n",
    "NUM_EPS_TRAIN_R = 1000\n",
    "NUM_TRAIN_R_FUNCS = 50\n",
    "NUM_REWARD_CALLS = 0\n",
    "env = gym.make(env_name)\n",
    "def deterministic_random(*args, lb = -1, ub = 1, sparsity = 0.0, continuous = False):\n",
    "    \"\"\"\n",
    "    Create a deterministic random number generator for a given set of arguments.\n",
    "    Used to generate deterministic reward functions for the coherence classifier.\n",
    "    [Edit 4/3/24: adapted to continuous state space]\"\"\"\n",
    "    global NUM_REWARD_CALLS\n",
    "    NUM_REWARD_CALLS += 1\n",
    "    unique_seed = f\"{args}\".encode(\"utf-8\")\n",
    "    random.seed(unique_seed)\n",
    "    return random.uniform(lb, ub) if random.random() > sparsity else random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "URS_r_funcs = [lambda *args: deterministic_random(args) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "URS_agents = [train_qtable(env_name = env_name, episodes=NUM_EPS_TRAIN_R, \n",
    "                           reward_function = r_func) for r_func in URS_r_funcs]\n",
    "USS_r_funcs = [lambda *args: deterministic_random(args, sparsity=0.99) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "USS_agents = [train_qtable(env_name = env_name, episodes=NUM_EPS_TRAIN_R,\n",
    "                            reward_function = r_func) for r_func in USS_r_funcs]\n",
    "UPS_agents = [QTableAgent(get_state_size(env), env.action_space.n) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "\n",
    "# The Q-Table is already one-hot encoded, so we don't need to convert it to a Data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.879610979477867e-10, -8.537837971656553e-10, 1.3453720557822044e-10, -6.91582481382308e-10, 5.834328244211258e-10, 2.689469001408139e-10, -7.747733415111116e-10, 3.5242825351422126e-10, 2.6150563760737437e-10, -8.919200340033589e-10]\n",
      "Episode: 50, Average total reward: 0.013892658673890956, Epsilon: 0.62\n",
      "Maximum reward: 8\n",
      "Average reward: 1.8141342803036227\n",
      "Maximum reward: 8\n",
      "Average reward: 0.019907783115625098\n"
     ]
    }
   ],
   "source": [
    "# Test ground\n",
    "print([USS_r_funcs[0](i) for i in range(10)])\n",
    "test_USS_agent = train_qtable(env_name = env_name, episodes = 50, verbose=True, epsilon_decay = 100, \n",
    "                              lr = 0.01, gamma = 0.9, reward_function = USS_r_funcs[0])\n",
    "test_qtable(gym.make(env_name), test_USS_agent, episodes = 100, reward_function = USS_r_funcs[0])\n",
    "test_UPS_agent = QTableAgent(get_state_size(env), env.action_space.n)\n",
    "test_qtable(gym.make(env_name), test_UPS_agent, episodes = 100, reward_function = USS_r_funcs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNNBinary(nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(FCNNBinary, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_node_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "def qtable_to_feat(qtable: torch.Tensor, label):\n",
    "    # In qtable, rows are states and columns are actions taken in that state\n",
    "    return Data(x = torch.flatten(qtable), y = label) # Naive approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "Epoch 1: Average Train Loss: 0.11231718239287707, Average Test Loss: 0.00022335199305162818\n",
      "Epoch 2: Average Train Loss: 0.0002989479297568248, Average Test Loss: 8.912703181408952e-05\n",
      "Epoch 3: Average Train Loss: 0.00014629946607331237, Average Test Loss: 4.654434895650121e-05\n",
      "Epoch 4: Average Train Loss: 8.502696133767995e-05, Average Test Loss: 2.7614850406871973e-05\n",
      "Epoch 5: Average Train Loss: 5.480071272152748e-05, Average Test Loss: 1.7909364215942464e-05\n",
      "Epoch 6: Average Train Loss: 3.78367068058298e-05, Average Test Loss: 6.568596279142097e-06\n",
      "Epoch 7: Average Train Loss: 3.982042360476125e-06, Average Test Loss: 5.3644196640334485e-08\n",
      "Epoch 8: Average Train Loss: 2.2202764586864988e-07, Average Test Loss: 2.384186146286993e-08\n",
      "Epoch 9: Average Train Loss: 1.2516986487298708e-07, Average Test Loss: 1.1920930376163597e-08\n",
      "Epoch 10: Average Train Loss: 8.344655171299564e-08, Average Test Loss: 5.9604651880817984e-09\n",
      "Epoch 11: Average Train Loss: 6.109478771776366e-08, Average Test Loss: 5.9604651880817984e-09\n",
      "Epoch 12: Average Train Loss: 4.7683733228609523e-08, Average Test Loss: 5.9604651880817984e-09\n",
      "Epoch 13: Average Train Loss: 3.874303011741631e-08, Average Test Loss: 5.9604651880817984e-09\n",
      "Early stopping at epoch 13\n"
     ]
    }
   ],
   "source": [
    "UPS_agents = [QTableAgent(get_state_size(env), env.action_space.n) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "\n",
    "for agent in UPS_agents:\n",
    "    for row in agent.q_table:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = np.random.uniform(-1, 1) # set each value to a random number between -1 and 1\n",
    "dataset1 = [qtable_to_feat(torch.tensor(agent.q_table, dtype=torch.float32), 1) for agent in USS_agents]\n",
    "dataset2 = [qtable_to_feat(torch.tensor(agent.q_table, dtype=torch.float32), 0) for agent in URS_agents] # URS = 1, UPS = 0\n",
    "\n",
    "def generate_fcnn_data(dataset1, dataset2):\n",
    "    indices = np.random.permutation(len(dataset1) + len(dataset2))\n",
    "    data = [dataset1[i] if i < len(dataset1) else dataset2[i - len(dataset1)] for i in indices]\n",
    "    for i in range(len(data)):\n",
    "        data[i].y = 1.0 if indices[i] < len(dataset1) else 0.0 # Binary labels for each node; 1 = URS, 0 = UPS\n",
    "        # Hence roughly speaking, 1 = more coherent, 0 = less coherent\n",
    "\n",
    "    train_data_ratio = 0.8\n",
    "    train_data, test_data = data[:int(train_data_ratio * len(data))], data[int(train_data_ratio * len(data)):]\n",
    "    num_node_features = data[0].x.shape[0] # Number of features for each node\n",
    "    return train_data, test_data, num_node_features\n",
    "\n",
    "def train_fcnn_classifier(model, criterion, optimizer, train_data, test_data, epochs = 40, patience = 3, \n",
    "                          epochs_without_improvement = 0, best_loss = float('inf')):\n",
    "    for epoch in range(epochs):\n",
    "        avg_train_loss = 0\n",
    "        for datapt in train_data:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model.forward(datapt)\n",
    "            assert isinstance(out, torch.Tensor), f\"Expected model.forward to return a tensor, but got {out}\"\n",
    "            loss = criterion(out, torch.tensor([datapt.y]))  # Adjust shape as necessary\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_train_loss += loss.item()\n",
    "        avg_train_loss /= len(train_data)\n",
    "\n",
    "        avg_test_loss = 0\n",
    "        for datapt in test_data:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model.forward(datapt)\n",
    "                loss = criterion(out, torch.tensor([datapt.y]))\n",
    "                avg_test_loss += loss.item()\n",
    "        avg_test_loss /= len(test_data)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Average Train Loss: {avg_train_loss}, Average Test Loss: {avg_test_loss}')\n",
    "        \n",
    "        # Early Stopping\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "train_data, test_data, num_node_features = generate_fcnn_data(dataset1, dataset2)\n",
    "print(num_node_features)\n",
    "model = FCNNBinary(num_node_features)\n",
    "criterion = torch.nn.BCELoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "train_fcnn_classifier(model, criterion, optimizer, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_UQS_qagent(rand_qtable, gamma, env: gym.Env, episodes = 500):\n",
    "    \"\"\"\n",
    "    Train a Q-table agent based on a reward function uniformly sampled from the set of \n",
    "    possible reward functions compatible with the given random Q-table.\"\"\"\n",
    "    # Generate the reward function using the Bellman equation\n",
    "    r_table = np.zeros(rand_qtable.shape)\n",
    "    for s in range(rand_qtable.shape[0]):\n",
    "        for a in range(rand_qtable.shape[1]):\n",
    "            env.reset()\n",
    "            env.unwrapped.s = s\n",
    "            ns = env.step(a)[0]\n",
    "            r_table[s, a] = rand_qtable[s, a] - gamma * np.max(rand_qtable[ns]) #assuming greedy policy\n",
    "    \n",
    "    # Train the agent\n",
    "    r_func = lambda s, a, *args: r_table[s, a]\n",
    "    return train_qtable(env_name = env.spec.id, episodes = episodes, reward_function = r_func)\n",
    "\n",
    "UQS_agents = [generate_UQS_qagent(agent.q_table, 0.9, env, episodes = NUM_EPS_TRAIN_R) for agent in UPS_agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_UVS_qagent(rand_values, gamma, env: gym.Env, episodes = 500, lb = -1, ub = 1):\n",
    "    \"\"\"\n",
    "    Train a Q-table agent based on a reward function uniformly sampled from the set of \n",
    "    possible reward functions compatible with the given values for each state.\n",
    "    Assumes a uniform distribution between [lb, ub].\"\"\"\n",
    "    r_table = np.zeros((len(rand_values), env.action_space.n))\n",
    "    for s in range(len(rand_values)):\n",
    "        next_states = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            env.reset()\n",
    "            env.unwrapped.s = s\n",
    "            next_states[a] = env.step(a)[0]\n",
    "        #v(s) = max_a(R(s, a) + gamma * v(s'))\n",
    "        reward_ub = rand_values[s] - np.array([gamma * rand_values[int(ns)] for ns in next_states])\n",
    "        taut_probs = np.zeros(env.action_space.n)\n",
    "        for i in range(env.action_space.n):\n",
    "            all_except_i = np.delete(np.arange(env.action_space.n), i)\n",
    "            taut_probs[i] = np.prod((reward_ub[all_except_i] + 1) / 2)\n",
    "            # probability that all other rewards at action j are less than reward_ub[j]\n",
    "        taut_probs /= np.sum(taut_probs)\n",
    "        taut = np.random.choice(env.action_space.n, p = taut_probs) \n",
    "        #index of the action where the reward is equal to the maximum\n",
    "\n",
    "        rewards = np.full(env.action_space.n, float('inf'))\n",
    "        while np.any(rewards >= reward_ub): #while any of the rewards are greater than the upper bound\n",
    "            rewards = np.random.uniform(-1, 1, env.action_space.n)\n",
    "        rewards[taut] = reward_ub[taut]\n",
    "        r_table[s] = rewards\n",
    "    \n",
    "    r_func = lambda s, a, *args: r_table[s, a]\n",
    "    return train_qtable(env_name = env.spec.id, episodes = episodes, reward_function = r_func)\n",
    "\n",
    "# UVS_agents = [generate_UVS_qagent(np.random.uniform(-1, 1, env.unwrapped.s), 0.9, env, episodes = NUM_EPS_TRAIN_R) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "# this currently takes way too long so it has been commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19997789,  0.8480003 , -0.94945213,  0.36469594, -0.21314675,\n",
       "         0.29373737],\n",
       "       [-0.51630777, -0.73252197, -0.82781936,  0.27989021, -0.46934794,\n",
       "        -0.20870209],\n",
       "       [ 0.03481552,  0.09088575, -0.27765734, -0.20246238, -0.03002716,\n",
       "        -0.28835059],\n",
       "       ...,\n",
       "       [-0.21234347, -0.22230378,  0.44615135, -0.80687533, -0.02927372,\n",
       "        -0.87625879],\n",
       "       [ 0.93125485,  0.60710758, -0.53284528, -0.6932406 , -0.1986679 ,\n",
       "        -0.55611788],\n",
       "       [ 0.64275359, -0.31388159, -0.50294809,  0.34698775,  0.06680938,\n",
       "         0.12857975]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UQS_agents[0].q_table - UPS_agents[0].q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only attaching reward to terminal states (kind of like UUS? but with the inductive biases)\n",
    "\n",
    "def det_rand_terminal(done: bool, *args, lb = -1, ub = 1, sparsity = 0.0):\n",
    "    \"\"\"\n",
    "    Create a deterministic random number generator for a given set of arguments.\n",
    "    Used to generate deterministic reward functions for the coherence classifier. \"\"\"\n",
    "    global NUM_REWARD_CALLS\n",
    "    NUM_REWARD_CALLS += 1\n",
    "    if not done:\n",
    "        return random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "    unique_seed = f\"{args}\".encode(\"utf-8\")\n",
    "    random.seed(unique_seed)\n",
    "    return random.uniform(lb, ub) if random.random() > sparsity else random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "\n",
    "UUS_agents = [train_qtable(env_name = env_name, episodes = NUM_EPS_TRAIN_R, \n",
    "                           reward_function = lambda *args: det_rand_terminal(*args)) for _ in range(NUM_TRAIN_R_FUNCS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0,   0,   0,  ..., 499, 499, 499],\n",
       "         [100,   0,  20,  ..., 479, 499, 499]]),\n",
       " torch.Size([2, 3000]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Turn the state and action space of Taxi-v3 into a graph\n",
    "\n",
    "from collections import defaultdict\n",
    "taxi_env = gym.make(\"Taxi-v3\")\n",
    "taxi_env.reset()\n",
    "# Initialize containers for graph data\n",
    "edges = defaultdict(list)\n",
    "edge_attr = defaultdict(list)\n",
    "\n",
    "# A helper function to encode the state into a single number (node index)\n",
    "def state_to_node(taxi_row, taxi_col, pass_loc, dest_idx):\n",
    "    # This encoding assumes specific knowledge about the Taxi-v3 state space size\n",
    "    return taxi_row * 100 + taxi_col * 20 + pass_loc * 4 + dest_idx\n",
    "    # max = 4 * 100 + 4 * 20 + 4 * 4 + 3 = 400 + 80 + 16 + 3 = 499\n",
    "\n",
    "# Iterate through all possible states and actions to construct the graph\n",
    "for taxi_row in range(5):\n",
    "    for taxi_col in range(5):\n",
    "        for pass_loc in range(5):  # 4 locations + 1 for 'in taxi'\n",
    "            for dest_idx in range(4):\n",
    "                current_state = state_to_node(taxi_row, taxi_col, pass_loc, dest_idx)\n",
    "                for action in range(taxi_env.action_space.n):\n",
    "                    # Set the environment to the current state\n",
    "                    taxi_env.unwrapped.s = current_state\n",
    "                    # Take action and observe the next state and reward\n",
    "                    next_state, reward, done, _ = taxi_env.step(action)\n",
    "                    # Add edge from current state to next state\n",
    "                    edges[current_state].append(next_state)\n",
    "                    # Optionally, use rewards as edge attributes\n",
    "                    # edge_attr[(current_state, next_state)].append(reward)\n",
    "                    taxi_env.reset()\n",
    "\n",
    "\n",
    "# Convert edges and edge attributes to tensors\n",
    "edge_index = []\n",
    "for src, dsts in edges.items():\n",
    "    for dst in dsts:\n",
    "        edge_index.append([src, dst])\n",
    "edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "edge_index, edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 6])\n",
      "Epoch 1: Average Train Loss: 0.06216179677363045, Average Test Loss: 0.00010359175885241712\n",
      "Epoch 2: Average Train Loss: 0.0004164183939451505, Average Test Loss: 0.0006073502612707671\n",
      "Epoch 3: Average Train Loss: 0.0007531430309427378, Average Test Loss: 0.0007604949561937247\n",
      "Epoch 4: Average Train Loss: 0.0008634724525109049, Average Test Loss: 0.0006281233057961799\n",
      "Early stopping at epoch 4\n"
     ]
    }
   ],
   "source": [
    "def greedy_policy(q_table):\n",
    "    return torch.tensor(np.argmax(q_table, axis=1).reshape(-1, 1).astype(np.float32))\n",
    "def random_policy(state_dim):\n",
    "    return torch.randint(0, 6, (state_dim, 1)).float()\n",
    "def prep_qtable(q_table):\n",
    "    return torch.tensor(q_table, dtype=torch.float32)\n",
    "\n",
    "dataset1 = [Data(x = prep_qtable(agent.q_table), edge_index = edge_index, y = 1) for agent in USS_agents]\n",
    "dataset2 = [Data(x = prep_qtable(agent.q_table), edge_index = edge_index, y = 0) for agent in URS_agents]\n",
    "#dataset2 = [Data(x = random_policy(agent.q_table.shape[0]), edge_index = edge_index, y = 0) for agent in UPS_agents]\n",
    "# ^ random_policy = UPS sampling\n",
    "print(dataset1[0].x.shape)\n",
    "\n",
    "train_data, test_data, num_node_features = generate_data(dataset1, dataset2)\n",
    "model = GraphLevelGCN(num_node_features)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)\n",
    "train_classifier(model, criterion, optimizer, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -8.5504e-12,  0.0000e+00, -4.1088e-02,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -6.6273e-13,  5.3457e-12,  6.9077e-11],\n",
      "        [ 0.0000e+00, -3.9739e-13,  2.7565e-09,  7.3998e-10,  7.7897e-11,\n",
      "          0.0000e+00,  3.4917e-12,  6.0604e-09,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -7.2252e-12,  0.0000e+00,  0.0000e+00,  3.4676e-09,\n",
      "          0.0000e+00, -2.4605e-11,  0.0000e+00,  6.7238e-11,  5.3296e-11],\n",
      "        [ 0.0000e+00, -7.2347e-12,  8.4538e-12,  0.0000e+00,  8.8427e-10,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  6.9332e-12,  7.6284e-12],\n",
      "        [ 0.0000e+00, -7.9438e-12,  0.0000e+00,  0.0000e+00,  3.3650e-10,\n",
      "          0.0000e+00,  0.0000e+00,  1.0530e-11, -5.9638e-12,  1.0271e-11],\n",
      "        [ 0.0000e+00,  0.0000e+00, -2.7835e-10,  0.0000e+00,  1.0513e-09,\n",
      "          0.0000e+00,  0.0000e+00,  3.0895e-09, -3.5969e-12,  5.3653e-12]])\n",
      "tensor([[ 0.0000e+00, -8.6664e-03, -2.8030e-03, -2.4628e-02,  0.0000e+00,\n",
      "          0.0000e+00,  3.0579e+00,  2.1873e-03, -4.5405e-03,  4.9855e-02],\n",
      "        [ 0.0000e+00, -3.9739e-04,  6.1937e+00,  7.9838e-03,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  1.9243e+00, -7.3727e-03,  7.0700e-03],\n",
      "        [ 0.0000e+00, -1.4378e-02, -1.9427e-03,  3.0402e-01,  9.4381e-02,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  6.2298e-03,  0.0000e+00],\n",
      "        [ 0.0000e+00, -7.2347e-03,  6.1454e-02,  0.0000e+00,  4.1653e-01,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  6.0963e-03,  0.0000e+00, -3.9459e-01,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -3.0557e-03,  0.0000e+00],\n",
      "        [ 0.0000e+00,  4.7327e-01, -2.8052e-01,  0.0000e+00,  2.1606e+00,\n",
      "          0.0000e+00,  0.0000e+00,  2.1431e-01,  0.0000e+00,  0.0000e+00]])\n",
      "tensor([[0.9989]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.0004]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.transpose(dataset1[0].x, 0, -1)[0:10, 0:10]) # Example greedy policies\n",
    "print(torch.transpose(dataset2[0].x, 0, -1)[0:10, 0:10])\n",
    "print(model.forward(dataset1[0]))\n",
    "print(model.forward(dataset2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000, Average total reward: -423.2692692692693, Epsilon: 0.14\n",
      "Episode: 2000, Average total reward: -174.55655655655656, Epsilon: 0.03\n",
      "Episode: 3000, Average total reward: -108.48448448448448, Epsilon: 0.01\n",
      "tensor([[0.9863]], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_18028\\1055913281.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  taxi_data = Data(x = torch.tensor(prep_qtable(taxi_model.q_table)), edge_index = edge_index)\n"
     ]
    }
   ],
   "source": [
    "taxi_model = train_qtable(env_name = \"Taxi-v3\", episodes = 3000, verbose = True, print_every = 1000, \n",
    "                          return_reward = False)\n",
    "taxi_data = Data(x = torch.tensor(prep_qtable(taxi_model.q_table)), edge_index = edge_index)\n",
    "print(model.forward(taxi_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [-2.5970681 , -2.58678979, -2.59171511, -2.59413361, -2.59139141,\n",
       "        -3.18260675],\n",
       "       [-1.62748602, -1.63384201, -1.64086321, -1.63292275, -1.6048012 ,\n",
       "        -3.79311952],\n",
       "       ...,\n",
       "       [-0.8030423 , -0.74959066, -0.79656961, -0.80100224, -1.95104146,\n",
       "        -1.15839785],\n",
       "       [-2.05603415, -2.05609945, -2.04796067, -2.05448177, -2.06334143,\n",
       "        -2.58135788],\n",
       "       [-0.05834885, -0.05008236, -0.03969999,  0.7222398 , -0.297306  ,\n",
       "        -0.1       ]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_model.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find optimal policy by MCTS and test coherence classifier on it\n",
    "class Node:\n",
    "    def __init__(self, state, parent=None, action = None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action = action # Action taken to reach this state\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.value = 0\n",
    "\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def update(self, reward):\n",
    "        self.visits += 1\n",
    "        self.value += reward\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) == env.action_space.n\n",
    "\n",
    "    def best_child(self, c_param=1.4):\n",
    "        choices_weights = [(child.value / child.visits) + c_param * np.sqrt((2 * np.log(self.visits) / child.visits)) for child in self.children]\n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "\n",
    "def selection(node):\n",
    "    while not node.is_fully_expanded():\n",
    "        if not node.children:\n",
    "            return expansion(node)\n",
    "        else:\n",
    "            node = node.best_child()\n",
    "    return node\n",
    "\n",
    "def expansion(node):\n",
    "    tried_children = [child.state for child in node.children]\n",
    "    new_state = None\n",
    "    # Attempt to find an untried action/state\n",
    "    for _ in range(env.action_space.n):\n",
    "        action = env.action_space.sample()\n",
    "        env.reset()  # Reset to the current node's state\n",
    "        env.env.s = node.state  # This uses internal knowledge about gym's environment state setting\n",
    "        next_state, _, _, _ = env.step(action)\n",
    "        if next_state not in tried_children:\n",
    "            new_state = next_state\n",
    "            break\n",
    "    if new_state is not None:\n",
    "        child_node = Node(new_state, parent=node, action=action)\n",
    "        node.add_child(child_node)\n",
    "        return child_node\n",
    "    return node  # This case handles leaf nodes\n",
    "\n",
    "def simulation(node):\n",
    "    current_state = node.state\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    for _ in range(100):  # Limit the number of steps to prevent infinite loop\n",
    "        if done:\n",
    "            break\n",
    "        action = env.action_space.sample()\n",
    "        env.reset()  # Reset to the current simulation node's state\n",
    "        env.env.s = current_state\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        current_state = next_state\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "def backpropagation(node, reward):\n",
    "    while node is not None:\n",
    "        node.update(reward)\n",
    "        node = node.parent\n",
    "\n",
    "def mcts(root, iterations=1000):\n",
    "    for _ in range(iterations):\n",
    "        leaf = selection(root)\n",
    "        reward = simulation(leaf)\n",
    "        backpropagation(leaf, reward)\n",
    "\n",
    "# Initialize the Gym environment\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# MCTS\n",
    "initial_state = env.reset()\n",
    "root_node = Node(initial_state)\n",
    "mcts(root_node, iterations=1000)\n",
    "\n",
    "# After running MCTS, your root_node will have an approximated policy.\n",
    "# You can select actions by traversing the tree from the root using best_child()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward from the MCTS policy: -772.74\n"
     ]
    }
   ],
   "source": [
    "### Test MCTS\n",
    "\n",
    "def choose_action(node):\n",
    "    # Choose the child with the highest visit count\n",
    "    if node.children:\n",
    "        return max(node.children, key=lambda child: child.visits).action\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def simulate_episode_from_root(env, root_node):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    current_node = root_node\n",
    "    env.reset()\n",
    "    env.env.s = current_node.state\n",
    "    \n",
    "    while not done and current_node is not None:\n",
    "        action = choose_action(current_node)\n",
    "        if action is None:\n",
    "            # No more information in the tree; choose random action\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)  # Execute the chosen action\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Move to the next node in the tree, if it exists\n",
    "        next_node = None\n",
    "        for child in current_node.children:\n",
    "            if child.action == action:\n",
    "                next_node = child\n",
    "                break\n",
    "        current_node = next_node\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# Test the policy derived from the MCTS root node\n",
    "env = gym.make('Taxi-v3')\n",
    "average_reward = np.mean([simulate_episode_from_root(env, root_node) for _ in range(100)])\n",
    "print(f\"Average Reward from the MCTS policy: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GNNs over the environment work really well, even on USS/URS and UUS/URS (identifying \"sparsity\")\n",
    "    - Real Taxi agents are graded strongly towards USS and UUS end\n",
    "    - Looking at the q-tables, there are some noticible differences (e.g. USS q-tables tend to have lower magnitude)\n",
    "    - GNNs don't work yet on USS/URS when only the policy is passed in\n",
    "    - Started working once I put the fix in of changing the reward function of the environment *for each state*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
