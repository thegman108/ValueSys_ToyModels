{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: measure how correlated certain features of the policy $\\pi_0$ are to the value $P(URS | \\pi=\\pi_0)$, where $URS$ indicates that the policy was optimized for a random reward function $R \\in U[-1,1]^{|T|}$ (where $|T|$ is the number of transitions with non-zero probability). For simplicity's sake, we assume that it was either optimized for some $R$ or generated uniformly randomly from the set of all policies, with a 50% chance of each scenario. We also assume that the reward is generated i.i.d. via $R(s, a, s') \\sim N(0, 1)$.\n",
    "\n",
    "We can also analyze $P(USS | \\pi = \\pi_0)$ where $USS$ consists of sampling a sparsity factor $k \\in [1, |T|]$, then zeroing out $k$ values from a randomly sampled $R$ as before.\n",
    "\n",
    "***For outside observers: see the bottom two cells for how the graphs in our report were generated***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox as mdpt, numpy as np\n",
    "import mdptoolbox.example\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a bunch of MDPs with different parameters, sparsity\n",
    "from functools import partial\n",
    "\n",
    "NUM_MDPs = 100\n",
    "NUM_STATES = 10\n",
    "NUM_ACTIONS = 4\n",
    "\n",
    "def get_transition_matrix(num_states, num_actions, generator = np.random.dirichlet, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns a determinstic transition matrix for a given number of states and actions\n",
    "    \n",
    "    Returns:\n",
    "        P: (num_actions, num_states, num_states) array, where P[a, s, s'] is the probability of \n",
    "        transitioning from state s to state s' given action a\n",
    "    \"\"\"\n",
    "    P = np.zeros((num_actions, num_states, num_states)) # (A, S, S') shape\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            P[a, s, :] = generator(np.ones(num_states))\n",
    "    return P\n",
    "\n",
    "NEAR_ZERO = 0.0001\n",
    "def get_reward_matrix(transitions, sparsity = 0.0, generator = partial(np.random.uniform, -1, 1), **kwargs):\n",
    "    \"\"\"\n",
    "    Returns a reward matrix for a given number of states and actions\n",
    "    [3/16/24 edit: made sparse rewards near-zero to avoid ties, simulate uniformly sampling\n",
    "    the optimal policy]\n",
    "    \"\"\"\n",
    "    num_pos_transitions = np.count_nonzero(transitions)\n",
    "    num_sparse_rewards = max(1, int(sparsity * num_pos_transitions))\n",
    "    rewards = np.array([(np.random.uniform(-1.0 * NEAR_ZERO, NEAR_ZERO) \n",
    "                         if i < num_sparse_rewards else generator()) for i in range(num_pos_transitions)])\n",
    "    np.random.shuffle(rewards) # create a random permutation of the rewards\n",
    "    # num_pos_transitions number of rewards, with num_sparse_rewards number of zeros\n",
    "    out = np.zeros(transitions.shape)\n",
    "    i = 0\n",
    "    for a, s, s_prime in np.argwhere(transitions):\n",
    "        out[a, s, s_prime] = rewards[i]\n",
    "        i += 1\n",
    "    # assert (np.abs(out) < np.full(out.shape, NEAR_ZERO)).sum() == num_pos_transitions - num_sparse_rewards\n",
    "    return out\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "EPSILON = 0.01 # roughly indicates the \"skill level\" of the agent\n",
    "MAX_ITER = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests(num_mdps = NUM_MDPs, sparsity_levels: np.ndarray = None, mdp_generator = mdpt.mdp.PolicyIterationModified, P_generator = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate a bunch of MDPs with different sparsity levels, and return the sparsity levels and the MDPs\n",
    "\n",
    "    Args:\n",
    "        sparsity_levels: a list of sparsity levels to generate MDPs with\n",
    "    Returns:\n",
    "        sparsity_levels: the sparsity levels used to generate the MDPs, in the same order as the MDPs\n",
    "        MDPS: an array of MDPs\n",
    "    \"\"\"\n",
    "    (max_iter, epsilon) = (kwargs['max_iter'], kwargs['epsilon']) if 'max_iter' in kwargs and 'epsilon' in kwargs else (MAX_ITER, EPSILON)\n",
    "    sparsity_levels = sparsity_levels if sparsity_levels is not None else np.arange(num_mdps) / num_mdps\n",
    "    sparsity_copy = sparsity_levels.copy() # defensive copy\n",
    "    np.random.shuffle(sparsity_copy)\n",
    "    transitions = np.array([get_transition_matrix(NUM_STATES, NUM_ACTIONS, **kwargs) if P_generator is None else P_generator(NUM_STATES, NUM_ACTIONS, **kwargs) for i in range(num_mdps)])\n",
    "    \n",
    "    MDPS = np.array([mdp_generator(\n",
    "        transitions[i], \n",
    "        get_reward_matrix(transitions[i], sparsity_copy[i], **kwargs), \n",
    "        DISCOUNT, max_iter = max_iter) \n",
    "        for i in range(num_mdps)\n",
    "    ])\n",
    "    for mdp in MDPS:\n",
    "        if mdp_generator == mdpt.mdp.ValueIteration:\n",
    "            mdp.epsilon = epsilon\n",
    "    return sparsity_copy, MDPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a transition function with various settings for properties (e.g. deterministic, sparse, fixed) and train a classifier to predict P(URS | $\\pi = \\pi_0$) and P(USS | $\\pi = \\pi_0$) (baseline probability = 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a bunch of MDPs (with baseline/zero sparsity), solve some of them, \n",
    "# generate random policy for others\n",
    "\n",
    "def transition_function_sparse_loops(states, actions, fixed = False, **kwargs):\n",
    "    \"\"\"\n",
    "    Sparse transition function with guaranteed loops\n",
    "    TODO: possibly implement terminal states\n",
    "    \"\"\"\n",
    "    # print(fixed)\n",
    "    rng = np.random.default_rng(seed = 0) if fixed else None\n",
    "    transitions = np.zeros((actions, states, states))\n",
    "    for state in range(states):\n",
    "        self_loop = np.random.randint(0, actions) if not fixed else rng.integers(0, actions)\n",
    "        for action in range(actions):\n",
    "            if action == self_loop:\n",
    "                for next_state in range(states):\n",
    "                    transitions[action, state, next_state] = 1 if next_state == state else 0\n",
    "            else: # sparse randomness\n",
    "                transitions[action, state, :] = np.zeros(states)\n",
    "                transitions[action, state, np.random.randint(states) if not fixed else rng.integers(0, states)] = 1\n",
    "    return transitions\n",
    "\n",
    "NUM_MDPs = 10000\n",
    "fixed = False\n",
    "#print(np.random.uniform(1.0/NUM_ACTIONS/NUM_STATES**2, 1, NUM_MDPs))\n",
    "sparsity_levels = np.random.uniform(1.0/NUM_ACTIONS/NUM_STATES**2, 1.0, NUM_MDPs)\n",
    "#sparsity_levels = np.zeros(NUM_MDPs)\n",
    "# URS would be np.zeros(NUM_MDPs)\n",
    "\n",
    "random_pol_indices = np.random.choice(NUM_MDPs, NUM_MDPs // 2, replace = False) # The indices of the MDPs with random policies\n",
    "random_pol_set = set(random_pol_indices)\n",
    "# i not in random_pol_set = random_or_rr[i] == 1, sparsity_levels[i] > 0\n",
    "MDPS = generate_tests(NUM_MDPs, sparsity_levels = sparsity_levels,\n",
    "                      P_generator = transition_function_sparse_loops, fixed = fixed)[1]\n",
    "# print(np.ndim(MDPS[0].R))\n",
    "# Problem with _bounditer in ValueIteration happening when upper uniform bound is too high/sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(random_pol_indices)\n",
    "for i in range(NUM_MDPs): # 50% RR, 50% random\n",
    "    MDPS[i].run()\n",
    "for i in random_pol_indices:\n",
    "    MDPS[i].policy = np.random.randint(NUM_ACTIONS, size = NUM_STATES)\n",
    "policies = np.array([mdp.policy for mdp in MDPS])\n",
    "# print(policies.shape)\n",
    "random_or_rr = np.array([0 if i in random_pol_set else 1 for i in range(NUM_MDPs)])\n",
    "# 0 if random, 1 if generated from RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([MDPS[1].P[j] == MDPS[0].P[j] for j in range(NUM_ACTIONS)])\n",
    "assert not fixed or np.all([np.all([MDPS[i].P[j] == MDPS[0].P[j] for j in range(NUM_ACTIONS)]) for i in range(NUM_MDPs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 0 1 1 0 1 3 3 2]\n",
      " [0 1 0 3 3 2 3 3 1 0]\n",
      " [0 1 2 2 2 3 3 1 0 2]\n",
      " [2 0 2 1 1 1 1 3 3 0]\n",
      " [2 0 2 1 0 2 0 1 1 1]\n",
      " [2 0 2 0 1 1 0 0 1 3]\n",
      " [2 2 1 1 2 3 1 3 3 2]\n",
      " [3 3 0 3 2 0 2 0 2 3]\n",
      " [3 1 3 1 1 1 2 1 3 2]\n",
      " [3 2 0 0 0 1 1 3 2 3]] [1 1 1 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(policies[0:10], random_or_rr[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear Regression\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "\n",
    "def regression(X, y, test_size = 0.2, regression = LinearRegression):\n",
    "    \"\"\"\n",
    "    Trains a linear regression model on the given data, and returns the model and test data\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    model = regression().fit(X_train, y_train)\n",
    "    return model, model.predict_proba(X_test), y_test\n",
    "\n",
    "def neural_network(X, y, test_size = 0.2, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Trains a neural network on the given data, and returns the model and the mean squared error\n",
    "    \"\"\"\n",
    "    def build_model():\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(64, activation = 'relu', input_shape = [X.shape[1]]),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(64, activation = 'relu'),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(1, activation = 'sigmoid')\n",
    "        ])\n",
    "        return model\n",
    "    model = build_model()\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['mae'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    model.fit(X_train, y_train, epochs = 100, validation_split = 0.2, verbose = 1, \n",
    "              callbacks = [keras.callbacks.EarlyStopping(patience = 3)])\n",
    "    return model, model.predict(X_test), y_test\n",
    "\n",
    "def find_loop_dist_and_length(transitions, policy: np.ndarray, initial_state):\n",
    "    \"\"\"\n",
    "    Computes the distance to the loop and the length of the loop for a given policy and initial state\n",
    "    \"\"\"\n",
    "    visited_states = {}  # Using a dict for quicker lookups\n",
    "    current_state = initial_state\n",
    "    step = 0  # Track the number of steps taken to find the loop length directly\n",
    "\n",
    "    while current_state not in visited_states:\n",
    "        visited_states[current_state] = step\n",
    "        # Simulate a transition\n",
    "        current_state = np.random.choice(np.arange(len(policy)), 1, \n",
    "                                         p = transitions[policy[current_state]][current_state]).item()\n",
    "        step += 1\n",
    "    \n",
    "    #distance to loop = visited_states[current_state]; loop length = step - visited_states[current_state]\n",
    "    return visited_states[current_state], step - visited_states[current_state]\n",
    "\n",
    "def takes_self_loop(transitions, policy: np.ndarray, initial_state):\n",
    "    \"\"\" \n",
    "    Returns 1 if the policy takes a self loop, 0 otherwise\n",
    "    \"\"\"\n",
    "    return int(transitions[policy[initial_state]][initial_state][initial_state] > 0.5)\n",
    "\n",
    "def num_out_arrows(transitions, policy: np.ndarray, initial_state):\n",
    "    \"\"\"\n",
    "    Returns the sum of outgoing arrows for each state that the policy visits from the \n",
    "    initial state before reaching a loop\n",
    "    \"\"\"\n",
    "    visited_states = {}  # Using a dict for quicker lookups\n",
    "    current_state = initial_state\n",
    "    step = 0  # Track the number of steps taken to find the loop length directly\n",
    "    out_arrows = 0\n",
    "    while current_state not in visited_states:\n",
    "        visited_states[current_state] = step\n",
    "        # Simulate a transition\n",
    "        current_state = np.random.choice(np.arange(len(policy)), 1, \n",
    "                                         p = transitions[policy[current_state]][current_state]).item()\n",
    "        step += 1\n",
    "        out_arrows += np.count_nonzero(transitions[policy[current_state]][current_state])\n",
    "    return out_arrows\n",
    "\n",
    "### Generate features\n",
    "encoder = OneHotEncoder(categories = 'auto', sparse_output = False, drop = 'first')\n",
    "# Drop first to avoid multicollinearity, large coefficients\n",
    "# encoder.fit(np.arange(NUM_ACTIONS))\n",
    "# print(encoder.categories_)\n",
    "\n",
    "### Train the model\n",
    "policies_encoded = encoder.fit_transform(policies)\n",
    "features = np.array([np.concatenate((np.array(MDPS[i].P).flatten(), policies_encoded[i]), axis = 0)\n",
    "                      for i in range(NUM_MDPs)])\n",
    "# features = encoder.fit_transform(policies)\n",
    "loop_lengths = np.array([[find_loop_dist_and_length(MDPS[i].P, policies[i], policies[i][j])[x] for j in range(NUM_STATES) for x in range(2)] \n",
    "                         for i in range(NUM_MDPs)])\n",
    "self_loops = np.array([[takes_self_loop(MDPS[i].P, policies[i], j) for j in range(NUM_STATES)] for i in range(NUM_MDPs)])\n",
    "# features = np.concatenate((features, encoder.fit_transform(loop_lengths)), axis = 1)\n",
    "out_arrows = np.array([[num_out_arrows(MDPS[i].P, policies[i], j) for j in range(NUM_STATES)] for i in range(NUM_MDPs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-entropy loss: 0.4046394451155128\n",
      "Accuracy: 0.815\n",
      "Baseline log loss: 0.6931471805599454\n",
      "Model coefficients, intercept: [[ 0.45806306  0.35026912  0.30968588  0.17161846  0.20360599  0.42195516\n",
      "  -0.15391289 -0.48674678  0.44036281  0.40304854  0.36422364  0.22522363\n",
      "  -0.15216993  0.25806502 -0.3755877   0.10283701  0.38722109  0.39241475\n",
      "   0.25847082  0.06441381 -0.15982995 -0.07519855 -0.09316132  0.334299\n",
      "   0.5390171   0.43942827  0.44309575  0.0911745   0.06970824 -0.11171158\n",
      "   0.0249155  -0.45012175  0.36759033  0.27136415  0.3328607   0.13397817\n",
      "  -0.00341763  0.218185   -0.26786912  0.31485089  0.30028153  0.1485144\n",
      "   0.17994449  0.07748825 -0.05490018  0.1299357   0.5163291   0.03202936\n",
      "   0.36524741  0.42719615  0.20531985  0.16211559  0.17319969  0.2976421\n",
      "   0.07201912 -0.44638787  0.30806629  0.47898937  0.24493465  0.17671838\n",
      "   0.24947828  0.07577393 -0.26760273  0.11498681  0.36469353  0.46714944\n",
      "   0.37342881  0.06720513  0.10322448 -0.08227148 -0.49605943  0.47523945\n",
      "   0.38250297  0.32463389  0.16244853  0.14876874  0.28106628  0.19228106\n",
      "  -0.09528757  0.         -1.26140141 -1.25286642 -1.09549305 -1.03236944\n",
      "  -1.3544059  -1.31648606 -1.24321545 -1.36820839 -1.25947334 -1.38327729]] [0.09961597]\n",
      "Sample outputs: [(array([0.45206666, 0.54793334]), 1), (array([0.14033442, 0.85966558]), 1), (array([0.9863765, 0.0136235]), 0), (array([0.19480888, 0.80519112]), 1), (array([0.75498917, 0.24501083]), 1), (array([0.93348889, 0.06651111]), 0), (array([0.11020594, 0.88979406]), 1), (array([0.36489025, 0.63510975]), 0), (array([0.94444108, 0.05555892]), 0), (array([0.99704311, 0.00295689]), 0)]\n"
     ]
    }
   ],
   "source": [
    "# features = np.concatenate((encoder.fit_transform(policies), encoder.fit_transform(loop_lengths),\n",
    "#                            ), axis = 1)\n",
    "# print(loop_lengths[0:10])\n",
    "features = np.concatenate((encoder.fit_transform(out_arrows), self_loops), axis = 1) # for interpretability\n",
    "model, y_pred, y_test = regression(features, random_or_rr, regression = partial(LogisticRegression, max_iter = 1000))\n",
    "print(\"Average cross-entropy loss:\", log_loss(y_test, y_pred, normalize = True))\n",
    "print(\"Accuracy:\", np.mean([np.round(y_pred[i][0]) != y_test[i] for i in range(len(y_pred))])) \n",
    "\n",
    "# if round(y_pred[0]) is 0, then model thinks 1 is more likely; if 1, then 0 is more likely\n",
    "# print(y_pred)\n",
    "print(\"Baseline log loss:\", log_loss(y_test, np.full(y_pred.shape, 0.5), normalize = True))\n",
    "print(\"Model coefficients, intercept:\", model.coef_, model.intercept_)\n",
    "print(\"Sample outputs:\", [(y_pred[i], y_test[i]) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: [2 1 3 0 1 2 2 0 1 2] Probability: [0.02536816 0.97463184] Actual: 1\n",
      "Policy: [2 0 3 0 0 2 0 2 1 3] Probability: [0.02430077 0.97569923] Actual: 1\n",
      "Policy: [1 1 1 1 2 0 0 2 1 1] Probability: [0.02356227 0.97643773] Actual: 1\n",
      "Policy: [3 3 2 0 3 2 1 1 3 2] Probability: [0.01908744 0.98091256] Actual: 1\n",
      "Policy: [3 2 3 1 2 0 1 0 2 1] Probability: [0.01894779 0.98105221] Actual: 1\n",
      "Policy: [2 2 2 1 3 2 0 3 0 3] Probability: [9.99890989e-01 1.09011360e-04] Actual: 0\n",
      "Policy: [1 1 1 2 2 2 3 3 3 0] Probability: [9.99592482e-01 4.07518251e-04] Actual: 0\n",
      "Policy: [3 1 0 1 0 2 2 2 0 0] Probability: [9.99547436e-01 4.52563815e-04] Actual: 0\n",
      "Policy: [2 2 3 1 1 0 2 1 1 1] Probability: [9.99542292e-01 4.57707702e-04] Actual: 0\n",
      "Policy: [2 3 1 2 1 0 0 3 3 0] Probability: [9.99539888e-01 4.60111705e-04] Actual: 0\n"
     ]
    }
   ],
   "source": [
    "### Grab the five policies with the highest and lowest probabilities of being random\n",
    "import networkx as nx\n",
    "\n",
    "if fixed:\n",
    "    # Generate a graph of the first MDP\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(NUM_STATES):\n",
    "        G.add_node(i)\n",
    "    enumerated_edges = {}\n",
    "    for i in range(NUM_ACTIONS):\n",
    "        enumerated_edges[i] = []\n",
    "        for j in range(NUM_STATES):\n",
    "            for k in range(NUM_STATES):\n",
    "                if MDPS[0].P[i][j, k] == 1:\n",
    "                    G.add_edge(j, k, action = i)\n",
    "                    enumerated_edges[i].append((j, k))\n",
    "    edge_labels = {(u, v): f\"{d['action']}\" for u, v, d in G.edges(data=True)}\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=20)  # k: Optimal distance between nodes. Increase/decrease to spread nodes out\n",
    "    nx.draw(G, pos = pos, with_labels = True)\n",
    "    nx.draw_networkx_edge_labels(G, pos = pos, edge_labels = edge_labels)\n",
    "    \n",
    "    for i in range(NUM_ACTIONS):\n",
    "        print(f\"Action {i} transitions:\", enumerated_edges[i])\n",
    "\n",
    "highest_probs = np.argsort(y_pred[:, 1])[-5:]\n",
    "lowest_probs = np.argsort(y_pred[:, 1])[:5]\n",
    "#print(\"Highest probabilities:\", [(y_pred[i], y_test[i]) for i in highest_probs])\n",
    "for i in np.concatenate((highest_probs, lowest_probs)):\n",
    "    print(\"Policy:\", policies[i], \"Probability:\", y_pred[i], \"Actual:\", y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On a random deterministic MDP(s), it doesn't seem like URS is identifiable, which is perhaps to be expected as every policy is optimal for some (normalized) reward function\n",
    "    - This also matches our results when looking at the distribution of optimal policies for \"cloud\"-y MDPs\n",
    "- As a control, with MDPs with loops, passing in just the policy (which shouldn't give much information without the related MDP) gives 0.54 accuracy\n",
    "    - Why not 0.50 exactly?\n",
    "    - Similar results with neural network\n",
    "    - This holds true when we use the label predictions for regression (model.predict), as well as the probability prediction (model.predict_proba)\n",
    "- Passing in the policy with the transition function (flattened) gives 0.53 accuracy, even with the NN\n",
    "    - Would likely need a graph neural network to train deep enough \"circuits\" to use the data of the whole transition function effectively\n",
    "- Distance to loop correlates somewhat well with P(URS) (~0.66-0.68 accuracy, 0.61-0.63 log loss on a diverse dataset of sparse transition functions), length of loop not as well (~0.56 accuracy, 0.687 log loss)\n",
    "    - Putting them together doesn't give improvement (~0.67-0.71 accuracy, 0.57-0.62 log loss)\n",
    "    - Intuitively, the length of the loop an optimal policy takes is its “goal complexity”; distance to loop = “agency” \n",
    "- Setting $k \\in U[1, N/2]$ gives:\n",
    "    - 0.56 accuracy, 0.688 log loss with length of loop; 0.66-0.672 accuracy, 0.61 log loss with distance to loop\n",
    "    - Setting the upper bound of $k$ too high results in some weird MDP package errors, I suspect because sparsity is too high\n",
    "    - This matches the distribution results we found in reward_function.ipynb, as sparsity didn't seem to \"matter\" until around ~0.9 given (S, A) = (10, 4)\n",
    "- $k \\in U[1, N]$ gives similar results\n",
    "    - (Note that this was run with PolicyIterationModified instead of ValueIteration with the same settings, which I don't expect to change any of the results, but I might be wrong)\n",
    "    - 0.72-0.74 accuracy with policy, distance to loop, and length of loop\n",
    "- Calculating whether the policy enters a self loop or not for each state $s$ gives 0.80 accuracy, 0.43 log loss!\n",
    "    - With policy and distance to loop included, ~0.82-0.84 accuracy, 0.38-0.40 log loss\n",
    "    - Similar results with neural network\n",
    "    - The logistic coefficients are all negative, lending evidence to the claim that a policy that takes more self-loops is *less* likely to be sampled via URS or USS\n",
    "        - 0.9999 chance of being from UPS if the policy always takes self-loops; 0.95 chance of being from URS/USS if it never takes self-loops\n",
    "- Calculating out-arrows also gives ~0.80 accuracy, 0.44 log loss\n",
    "    - Combining with self-loops doesn't give much\n",
    "    - Coefficients are positive --> a policy that reaches more out-arrows is more likely to be sampled via URS or USS\n",
    "- P(USS) / P(URS) is really difficult (basically 0.5)\n",
    "- Graph results (see plt plot below) fits for P(USS) given uniform k, P(URS)\n",
    "- Apparently when there is only one reward, passing in policy is *more* predictive than LL or O,S (.77 acc, .45 LL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize the model's performance\n",
    "### This is the cell that generated the plots in our reports\n",
    "\n",
    "num_runs = 90\n",
    "accuracies = np.zeros((num_runs // 3, 3))\n",
    "log_losses = np.zeros((num_runs // 3, 3))\n",
    "\n",
    "NUM_MDPs = 10000\n",
    "fixed = False\n",
    "#print(np.random.uniform(1.0/NUM_ACTIONS/NUM_STATES**2, 1, NUM_MDPs))\n",
    "sparsity_levels = np.random.uniform(1.0/NUM_ACTIONS/NUM_STATES**2, 1.0, NUM_MDPs)\n",
    "# sparsity_levels = np.random.uniform(1.0 - 1.0/NUM_ACTIONS/NUM_STATES**2, 1.0, NUM_MDPs) #high sparsity\n",
    "# sparsity_levels = np.zeros(NUM_MDPs) #no sparsity\n",
    "\n",
    "random_pol_indices = np.random.choice(NUM_MDPs, NUM_MDPs // 2, replace = False) \n",
    "# The indices of the MDPs with random policies (or random dense rewards)\n",
    "random_pol_set = set(random_pol_indices)\n",
    "# i not in random_pol_set = random_or_rr[i] == 1, sparsity_levels[i] > 0\n",
    "\n",
    "# to measure P(USS) / P(URS), \n",
    "# let sparsity_levels = [sparsity_levels[i] if i in random_pol_set else 0 for i in range(NUM_MDPs)]\n",
    "MDPS = generate_tests(NUM_MDPs, sparsity_levels = [sparsity_levels[i] if i in random_pol_set else 0 for i in range(NUM_MDPs)],\n",
    "                    P_generator = transition_function_sparse_loops, fixed = fixed)[1]\n",
    "\n",
    "# print(random_pol_indices)\n",
    "for i in range(NUM_MDPs): # 50% RR, 50% random\n",
    "    MDPS[i].run()\n",
    "# for i in random_pol_indices: #comment out when measuring P(USS) / P(URS)\n",
    "#     MDPS[i].policy = np.random.randint(NUM_ACTIONS, size = NUM_STATES)\n",
    "policies = np.array([mdp.policy for mdp in MDPS])\n",
    "# print(policies.shape)\n",
    "random_or_rr = np.array([0 if i in random_pol_set else 1 for i in range(NUM_MDPs)])\n",
    "# 0 if random, 1 if generated from RR\n",
    "\n",
    "encoder = OneHotEncoder(categories = 'auto', sparse_output = False, drop = 'first')\n",
    "\n",
    "# features = encoder.fit_transform(policies)\n",
    "loop_lengths = np.array([[find_loop_dist_and_length(MDPS[i].P, policies[i], policies[i][j])[x] for j in range(NUM_STATES)\n",
    "                           for x in range(2)] for i in range(NUM_MDPs)])\n",
    "self_loops = np.array([[takes_self_loop(MDPS[i].P, policies[i], j) for j in range(NUM_STATES)] for i in range(NUM_MDPs)])\n",
    "# features = np.concatenate((features, encoder.fit_transform(loop_lengths)), axis = 1)\n",
    "out_arrows = np.array([[num_out_arrows(MDPS[i].P, policies[i], j) for j in range(NUM_STATES)] for i in range(NUM_MDPs)])\n",
    "\n",
    "for n in range(num_runs):\n",
    "    ### Train the model\n",
    "    policies_encoded = encoder.fit_transform(policies)\n",
    "    features = np.array([np.concatenate((np.array(MDPS[i].P).flatten(), policies_encoded[i]), axis = 0)\n",
    "                        for i in range(NUM_MDPs)])\n",
    "\n",
    "    if n % 3 == 2:\n",
    "        features = np.concatenate((encoder.fit_transform(out_arrows), self_loops), axis = 1) # for interpretability\n",
    "    elif n % 3 == 1:\n",
    "        features = encoder.fit_transform(loop_lengths)\n",
    "    model, y_pred, y_test = regression(features, random_or_rr, regression=partial(LogisticRegression, max_iter=MAX_ITER))\n",
    "    accuracy = np.mean([np.round(y_pred[i][0]) != y_test[i] for i in range(len(y_pred))])\n",
    "    log_loss_value = log_loss(y_test, y_pred, normalize=True)\n",
    "    accuracies[n // 3][n % 3] = accuracy\n",
    "    log_losses[n // 3][n % 3] = log_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(np.array([x for pair in zip(accuracies.T, log_losses.T) for x in pair]).T, labels=\n",
    "            ['Accuracy (P)', 'Log Loss (P)', 'Accuracy (LL)', 'Log Loss (LL)', 'Accuracy (O,S)', 'Log Loss (O,S)'])\n",
    "plt.title('Regression Performances with Different Features')\n",
    "plt.ylabel('Value')\n",
    "plt.ylim(0.3, 0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "M = get_reward_matrix(transition_function_sparse_loops(10, 4), 1.0 - 1.0/NUM_ACTIONS/NUM_STATES**2/2)\n",
    "print(sum([abs(m) > NEAR_ZERO for m in M.flatten()]))\n",
    "print(np.count_nonzero(M))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
