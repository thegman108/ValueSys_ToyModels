{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: measure how correlated certain features of the policy $\\pi_0$ are to the value $P(URS | \\pi=\\pi_0)$, where $URS$ indicates that the policy was optimized for a random reward function $R \\in U[-1,1]^{|T|}$ (where $|T|$ is the number of transitions with non-zero probability). For simplicity's sake, we assume that it was either optimized for some $R$ or generated uniformly randomly from the set of all policies, with a 50% chance of each scenario. We also assume that the reward is generated i.i.d. via $R(s, a, s') \\sim N(0, 1)$.\n",
    "\n",
    "We can also analyze $P(USS | \\pi = \\pi_0)$ where $USS$ consists of sampling a sparsity factor $k \\in [1, |T|]$, then zeroing out $k$ values from a randomly sampled $R$ as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox as mdpt, numpy as np\n",
    "import mdptoolbox.example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a bunch of MDPs with different parameters, sparsity\n",
    "from functools import partial\n",
    "\n",
    "NUM_MDPs = 100\n",
    "NUM_STATES = 10\n",
    "NUM_ACTIONS = 4\n",
    "\n",
    "def get_transition_matrix(num_states, num_actions, generator = np.random.dirichlet, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns a determinstic transition matrix for a given number of states and actions\n",
    "    \n",
    "    Returns:\n",
    "        P: (num_actions, num_states, num_states) array, where P[a, s, s'] is the probability of \n",
    "        transitioning from state s to state s' given action a\n",
    "    \"\"\"\n",
    "    P = np.zeros((num_actions, num_states, num_states)) # (A, S, S') shape\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            P[a, s, :] = generator(np.ones(num_states))\n",
    "    return P\n",
    "\n",
    "def get_reward_matrix(transitions, sparsity = 0.0, generator = partial(np.random.uniform, -1, 1), **kwargs):\n",
    "    \"\"\"\n",
    "    Returns a reward matrix for a given number of states and actions\n",
    "    \"\"\"\n",
    "    num_pos_transitions = np.count_nonzero(transitions)\n",
    "    num_sparse_rewards = max(1, int(sparsity * num_pos_transitions))\n",
    "    rewards = np.array([(0 if i < num_sparse_rewards else generator()) for i in range(num_pos_transitions)])\n",
    "    np.random.shuffle(rewards) # create a random permutation of the rewards\n",
    "    # num_pos_transitions number of rewards, with num_sparse_rewards number of zeros\n",
    "    out = np.zeros(transitions.shape)\n",
    "    i = 0\n",
    "    for a, s, s_prime in np.argwhere(transitions):\n",
    "        out[a, s, s_prime] = rewards[i]\n",
    "        i += 1\n",
    "    assert np.count_nonzero(out) == num_pos_transitions - num_sparse_rewards\n",
    "    return out\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "EPSILON = 0.01 # roughly indicates the \"skill level\" of the agent\n",
    "MAX_ITER = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests(num_mdps = NUM_MDPs, sparsity_levels: np.ndarray = None, mdp_generator = mdpt.mdp.PolicyIterationModified, P_generator = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate a bunch of MDPs with different sparsity levels, and return the sparsity levels and the MDPs\n",
    "\n",
    "    Args:\n",
    "        sparsity_levels: a list of sparsity levels to generate MDPs with\n",
    "    Returns:\n",
    "        sparsity_levels: the sparsity levels used to generate the MDPs, in the same order as the MDPs\n",
    "        MDPS: an array of MDPs\n",
    "    \"\"\"\n",
    "    (max_iter, epsilon) = (kwargs['max_iter'], kwargs['epsilon']) if 'max_iter' in kwargs and 'epsilon' in kwargs else (MAX_ITER, EPSILON)\n",
    "    sparsity_levels = sparsity_levels if sparsity_levels is not None else np.arange(num_mdps) / num_mdps\n",
    "    sparsity_copy = sparsity_levels.copy() # defensive copy\n",
    "    np.random.shuffle(sparsity_copy)\n",
    "    transitions = np.array([get_transition_matrix(NUM_STATES, NUM_ACTIONS, **kwargs) if P_generator is None else P_generator(NUM_STATES, NUM_ACTIONS, **kwargs) for i in range(num_mdps)])\n",
    "    MDPS = np.array([mdp_generator(\n",
    "        transitions[i], \n",
    "        get_reward_matrix(transitions[i], sparsity_copy[i], **kwargs), \n",
    "        DISCOUNT, max_iter = max_iter) \n",
    "        for i in range(num_mdps)\n",
    "    ])\n",
    "    for mdp in MDPS:\n",
    "        if mdp_generator == mdpt.mdp.ValueIteration:\n",
    "            mdp.epsilon = epsilon\n",
    "    return sparsity_copy, MDPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a transition function with various settings for properties (e.g. deterministic, sparse, fixed) and train a classifier to predict URS | $\\pi = \\pi_0$ (baseline probability = 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a bunch of MDPs (with baseline/zero sparsity), solve some of them, \n",
    "# generate random policy for others\n",
    "\n",
    "def transition_function_sparse_loops(states, actions, fixed = False, **kwargs):\n",
    "    \"\"\"\n",
    "    Sparse transition function with guaranteed loops\n",
    "    TODO: possibly implement terminal states\n",
    "    \"\"\"\n",
    "    # print(fixed)\n",
    "    rng = np.random.default_rng(seed = 0) if fixed else None\n",
    "    transitions = np.zeros((actions, states, states))\n",
    "    for state in range(states):\n",
    "        for action in range(actions):\n",
    "            if action == 0:\n",
    "                for next_state in range(states):\n",
    "                    transitions[action, state, next_state] = 1 if next_state == state else 0\n",
    "            else: # sparse randomness\n",
    "                transitions[action, state, :] = np.zeros(states)\n",
    "                transitions[action, state, np.random.randint(states) if not fixed else rng.integers(0, states)] = 1\n",
    "    return transitions\n",
    "\n",
    "NUM_MDPs = 10000\n",
    "fixed = False\n",
    "#print(np.random.uniform(1.0/NUM_ACTIONS/NUM_STATES**2, 1, NUM_MDPs))\n",
    "sparsity_levels = np.random.uniform(1.0/NUM_ACTIONS/NUM_STATES**2, 1, NUM_MDPs)\n",
    "# URS would be np.zeros(NUM_MDPs)\n",
    "\n",
    "MDPS = generate_tests(NUM_MDPs, sparsity_levels = sparsity_levels,\n",
    "                      P_generator = transition_function_sparse_loops, fixed = fixed)[1]\n",
    "# Problem with _bounditer in ValueIteration happening when upper uniform bound is too high/sparse\n",
    "random_pol_indices = np.random.choice(NUM_MDPs, int(NUM_MDPs / 2), replace = False) # The indices of the MDPs with random policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(random_pol_indices)\n",
    "for i in range(NUM_MDPs): # 50% RR, 50% random\n",
    "    MDPS[i].run()\n",
    "for i in random_pol_indices:\n",
    "    MDPS[i].policy = np.random.randint(NUM_ACTIONS, size = NUM_STATES)\n",
    "policies = np.array([mdp.policy for mdp in MDPS])\n",
    "# print(policies.shape)\n",
    "random_pol_set = set(random_pol_indices)\n",
    "random_or_rr = np.array([0 if i in random_pol_set else 1 for i in range(NUM_MDPs)])\n",
    "# 0 if random, 1 if generated from RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([MDPS[1].P[j] == MDPS[0].P[j] for j in range(NUM_ACTIONS)])\n",
    "assert not fixed or np.all([np.all([MDPS[i].P[j] == MDPS[0].P[j] for j in range(NUM_ACTIONS)]) for i in range(NUM_MDPs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 0 3 1 0 1 0]\n",
      " [2 2 3 0 1 0 1 3 2 2]\n",
      " [3 1 2 0 0 3 1 2 2 3]\n",
      " [2 2 3 1 1 1 2 3 1 2]\n",
      " [1 2 2 3 1 0 1 1 1 1]\n",
      " [0 1 3 0 0 0 3 3 0 1]\n",
      " [0 2 0 3 1 0 3 1 3 0]\n",
      " [2 2 1 3 3 3 1 3 3 3]\n",
      " [3 3 2 3 1 2 1 2 2 3]\n",
      " [1 2 3 1 3 2 2 0 0 2]] [0 0 0 0 1 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(policies[0:10], random_or_rr[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear Regression\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "\n",
    "def regression(X, y, test_size = 0.2, regression = LinearRegression):\n",
    "    \"\"\"\n",
    "    Trains a linear regression model on the given data, and returns the model and the mean squared error\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    model = regression().fit(X_train, y_train)\n",
    "    return model, model.predict_proba(X_test), y_test\n",
    "\n",
    "def neural_network(X, y, test_size = 0.2, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Trains a neural network on the given data, and returns the model and the mean squared error\n",
    "    \"\"\"\n",
    "    def build_model():\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(64, activation = 'relu', input_shape = [X.shape[1]]),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(64, activation = 'relu'),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(1, activation = 'sigmoid')\n",
    "        ])\n",
    "        return model\n",
    "    model = build_model()\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['mae'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    model.fit(X_train, y_train, epochs = 100, validation_split = 0.2, verbose = 1, \n",
    "              callbacks = [keras.callbacks.EarlyStopping(patience = 3)])\n",
    "    return model, model.predict(X_test), y_test\n",
    "\n",
    "def find_loop_dist_and_length(transitions, policy: np.ndarray, initial_state):\n",
    "    visited_states = {}  # Using a dict for quicker lookups\n",
    "    current_state = initial_state\n",
    "    step = 0  # Track the number of steps taken to find the loop length directly\n",
    "\n",
    "    while current_state not in visited_states:\n",
    "        visited_states[current_state] = step\n",
    "        # Simulate a transition\n",
    "        current_state = np.random.choice(np.arange(len(policy)), 1, \n",
    "                                         p = transitions[policy[current_state]][current_state]).item()\n",
    "        step += 1\n",
    "    \n",
    "    #distance to loop = visited_states[current_state]; loop length = step - visited_states[current_state]\n",
    "    return visited_states[current_state], step - visited_states[current_state]\n",
    "\n",
    "\n",
    "### Generate features\n",
    "encoder = OneHotEncoder(categories = 'auto', sparse_output = False, drop = 'first')\n",
    "# Drop first to avoid multicollinearity, large coefficients\n",
    "# encoder.fit(np.arange(NUM_ACTIONS))\n",
    "# print(encoder.categories_)\n",
    "\n",
    "### Train the model\n",
    "# features = np.array([np.concatenate((np.array(MDPS[i].P).flatten(), policies_encoded[i]), axis = 0)\n",
    "#                      for i in range(NUM_MDPs)])\n",
    "features = encoder.fit_transform(policies)\n",
    "loop_lengths = np.array([[find_loop_dist_and_length(MDPS[i].P, policies[i], policies[i][j])[0] for j in range(NUM_STATES)] \n",
    "                         for i in range(NUM_MDPs)])\n",
    "features = encoder.fit_transform(loop_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-entropy loss: 0.6326985483625261\n",
      "Accuracy: 0.6645\n",
      "Baseline log loss: 0.6931471805599454\n",
      "Model coefficients, intercept: [[0.18003894 0.03842198 0.02386525 0.04267284 0.0498999  0.09509716\n",
      "  0.05531086 0.12606996 0.05786498 0.09934552]] [-0.9748501]\n",
      "Sample outputs: [(array([0.45669389, 0.54330611]), 1), (array([0.34906547, 0.65093453]), 1), (array([0.3190951, 0.6809049]), 0), (array([0.72608517, 0.27391483]), 0), (array([0.47355157, 0.52644843]), 0), (array([0.56282145, 0.43717855]), 1), (array([0.25057232, 0.74942768]), 1), (array([0.64209953, 0.35790047]), 1), (array([0.37293366, 0.62706634]), 0), (array([0.55463121, 0.44536879]), 1)]\n"
     ]
    }
   ],
   "source": [
    "features = loop_lengths # for interpretability\n",
    "model, y_pred, y_test = regression(features, random_or_rr, regression = partial(LogisticRegression, max_iter = 1000))\n",
    "print(\"Average cross-entropy loss:\", log_loss(y_test, y_pred, normalize = True))\n",
    "print(\"Accuracy:\", np.mean([np.round(y_pred[i][0]) != y_test[i] for i in range(len(y_pred))])) \n",
    "\n",
    "# if round(y_pred[0]) is 0, then model thinks 1 is more likely; if 1, then 0 is more likely\n",
    "# print(y_pred)\n",
    "print(\"Baseline log loss:\", log_loss(y_test, np.full(y_pred.shape, 0.5), normalize = True))\n",
    "print(\"Model coefficients, intercept:\", model.coef_, model.intercept_)\n",
    "print(\"Sample outputs:\", [(y_pred[i], y_test[i]) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: [3 2 0 2 0 2 3 2 0 3] Probability: [0.07292828 0.92707172] Actual: 1\n",
      "Policy: [0 0 0 1 0 0 0 0 0 0] Probability: [0.05545048 0.94454952] Actual: 0\n",
      "Policy: [2 2 0 2 2 3 1 2 0 0] Probability: [0.05441745 0.94558255] Actual: 0\n",
      "Policy: [2 3 3 0 1 1 3 2 1 1] Probability: [0.04486485 0.95513515] Actual: 1\n",
      "Policy: [1 1 3 2 2 2 1 2 1 1] Probability: [0.04056318 0.95943682] Actual: 0\n",
      "Policy: [3 1 1 3 1 3 1 3 3 1] Probability: [0.72608517 0.27391483] Actual: 0\n",
      "Policy: [2 2 0 2 3 2 1 3 0 1] Probability: [0.72608517 0.27391483] Actual: 0\n",
      "Policy: [2 2 2 0 2 1 0 3 3 0] Probability: [0.72608517 0.27391483] Actual: 0\n",
      "Policy: [0 1 2 2 3 2 1 3 3 0] Probability: [0.72608517 0.27391483] Actual: 0\n",
      "Policy: [3 2 2 1 1 1 2 1 1 0] Probability: [0.72608517 0.27391483] Actual: 1\n"
     ]
    }
   ],
   "source": [
    "### Grab the five policies with the highest and lowest probabilities of being random\n",
    "import networkx as nx\n",
    "\n",
    "if fixed:\n",
    "    # Generate a graph of the first MDP\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(NUM_STATES):\n",
    "        G.add_node(i)\n",
    "    enumerated_edges = {}\n",
    "    for i in range(NUM_ACTIONS):\n",
    "        enumerated_edges[i] = []\n",
    "        for j in range(NUM_STATES):\n",
    "            for k in range(NUM_STATES):\n",
    "                if MDPS[0].P[i][j, k] == 1:\n",
    "                    G.add_edge(j, k, action = i)\n",
    "                    enumerated_edges[i].append((j, k))\n",
    "    edge_labels = {(u, v): f\"{d['action']}\" for u, v, d in G.edges(data=True)}\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=20)  # k: Optimal distance between nodes. Increase/decrease to spread nodes out\n",
    "    nx.draw(G, pos = pos, with_labels = True)\n",
    "    nx.draw_networkx_edge_labels(G, pos = pos, edge_labels = edge_labels)\n",
    "    \n",
    "    for i in range(NUM_ACTIONS):\n",
    "        print(f\"Action {i} transitions:\", enumerated_edges[i])\n",
    "\n",
    "highest_probs = np.argsort(y_pred[:, 1])[-5:]\n",
    "lowest_probs = np.argsort(y_pred[:, 1])[:5]\n",
    "#print(\"Highest probabilities:\", [(y_pred[i], y_test[i]) for i in highest_probs])\n",
    "for i in np.concatenate((highest_probs, lowest_probs)):\n",
    "    print(\"Policy:\", policies[i], \"Probability:\", y_pred[i], \"Actual:\", y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On a random deterministic MDP(s), it doesn't seem like URS is identifiable, which is perhaps to be expected as every policy is optimal for some (normalized) reward function\n",
    "    - This also matches our results when looking at the distribution of optimal policies for \"cloud\"-y MDPs\n",
    "- Apparently my neural networks aren't predicting very well\n",
    "- With MDPs with loops, logistic regression achieves ~0.74-0.76 accuracy (0.59 - 0.56 log loss); neural network does slightly better than random?\n",
    "    - Although the NN's accuracy is basically 0.5\n",
    "    - This holds true when we use the label predictions for regression (model.predict), as well as the probability prediction (model.predict_proba)\n",
    "- Distance to loop correlates somewhat well with P(URS) (~0.66-0.68 accuracy, 0.61-0.63 log loss on a diverse dataset of sparse transition functions), length of loop not as well (~0.56 accuracy, 0.687 log loss)\n",
    "    - Putting them together doesn't give improvement (~0.67 accuracy, 0.62 log loss)\n",
    "    - Intuitively, the length of the loop an optimal policy takes is its “goal complexity”; distance to loop = “agency” \n",
    "- Setting $k \\in U[1, N/2]$ gives ~0.72-0.74 accuracy, ~0.54 log loss\n",
    "    - 0.56 accuracy, 0.688 log loss with length of loop; 0.66-0.672 accuracy, 0.61 log loss with distance to loop\n",
    "    - Setting the upper bound of $k$ too high results in some weird MDP package errors, I suspect because sparsity is too high\n",
    "    - This matches the distribution results we found in reward_function.ipynb, as sparsity didn't seem to \"matter\" until around ~0.9 given (S, A) = (10, 4)\n",
    "- $k \\in U[1, N]$ gives ~0.74-0.76 accuracy, 0.58-0.59 log loss\n",
    "    - (Note that this was run with PolicyIterationModified instead of ValueIteration with the same settings, which I don't expect to change any of the results, but I might be wrong)\n",
    "- TODO: run these tests multiple times to make box plots in the writeup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
