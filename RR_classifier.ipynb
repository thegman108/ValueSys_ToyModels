{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: measure how correlated certain features of the policy $\\pi_0$ are to the value $P(URS | \\pi=\\pi_0)$, where $URS$ indicates that the policy was optimized for a random reward function $R \\in U[-1,1]^{|T|}$ (where $|T|$ is the number of transitions with non-zero probability). For simplicity's sake, we assume that it was either optimized for some $R$ or generated uniformly randomly from the set of all policies, with a 50% chance of each scenario. We also assume that the reward is generated i.i.d. via $R(s, a, s') \\sim N(0, 1)$.\n",
    "\n",
    "We can also analyze $P(USS | \\pi = \\pi_0)$ where $USS$ consists of sampling a sparsity factor $k \\in [1, |T|]$, then zeroing out $k$ values from a randomly sampled $R$ as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox as mdpt, numpy as np\n",
    "import mdptoolbox.example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a bunch of MDPs with different parameters, sparsity\n",
    "from functools import partial\n",
    "\n",
    "NUM_MDPs = 100\n",
    "NUM_STATES = 10\n",
    "NUM_ACTIONS = 4\n",
    "\n",
    "def get_transition_matrix(num_states, num_actions, generator = np.random.dirichlet, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns a determinstic transition matrix for a given number of states and actions\n",
    "    \n",
    "    Returns:\n",
    "        P: (num_actions, num_states, num_states) array, where P[a, s, s'] is the probability of \n",
    "        transitioning from state s to state s' given action a\n",
    "    \"\"\"\n",
    "    P = np.zeros((num_actions, num_states, num_states)) # (A, S, S') shape\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            P[a, s, :] = generator(np.ones(num_states))\n",
    "    return P\n",
    "\n",
    "def get_reward_matrix(transitions, sparsity = 0.0, generator = partial(np.random.uniform, -1, 1), **kwargs):\n",
    "    \"\"\"\n",
    "    Returns a reward matrix for a given number of states and actions\n",
    "    \"\"\"\n",
    "    num_pos_transitions = np.count_nonzero(transitions)\n",
    "    num_sparse_rewards = max(1, int(sparsity * num_pos_transitions))\n",
    "    rewards = np.array([(0 if i < num_sparse_rewards else generator()) for i in range(num_pos_transitions)])\n",
    "    np.random.shuffle(rewards) # create a random permutation of the rewards\n",
    "    # num_pos_transitions number of rewards, with num_sparse_rewards number of zeros\n",
    "    out = np.zeros(transitions.shape)\n",
    "    i = 0\n",
    "    for a, s, s_prime in np.argwhere(transitions):\n",
    "        out[a, s, s_prime] = rewards[i]\n",
    "        i += 1\n",
    "    assert np.count_nonzero(out) == num_pos_transitions - num_sparse_rewards\n",
    "    return out\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "EPSILON = 0.01 # roughly indicates the \"skill level\" of the agent\n",
    "MAX_ITER = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tests(num_mdps = NUM_MDPs, sparsity_levels: np.ndarray = None, mdp_generator = mdpt.mdp.PolicyIterationModified, P_generator = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate a bunch of MDPs with different sparsity levels, and return the sparsity levels and the MDPs\n",
    "\n",
    "    Args:\n",
    "        sparsity_levels: a list of sparsity levels to generate MDPs with\n",
    "    Returns:\n",
    "        sparsity_levels: the sparsity levels used to generate the MDPs, in the same order as the MDPs\n",
    "        MDPS: an array of MDPs\n",
    "    \"\"\"\n",
    "    (max_iter, epsilon) = (kwargs['max_iter'], kwargs['epsilon']) if 'max_iter' in kwargs and 'epsilon' in kwargs else (MAX_ITER, EPSILON)\n",
    "    sparsity_levels = sparsity_levels if sparsity_levels is not None else np.arange(num_mdps) / num_mdps\n",
    "    sparsity_copy = sparsity_levels.copy() # defensive copy\n",
    "    np.random.shuffle(sparsity_copy)\n",
    "    transitions = np.array([get_transition_matrix(NUM_STATES, NUM_ACTIONS, **kwargs) if P_generator is None else P_generator(NUM_STATES, NUM_ACTIONS, **kwargs) for i in range(num_mdps)])\n",
    "    \n",
    "    MDPS = np.array([mdp_generator(\n",
    "        transitions[i], \n",
    "        get_reward_matrix(transitions[i], sparsity_copy[i], **kwargs), \n",
    "        DISCOUNT, max_iter = max_iter) \n",
    "        for i in range(num_mdps)\n",
    "    ])\n",
    "    for mdp in MDPS:\n",
    "        if mdp_generator == mdpt.mdp.ValueIteration:\n",
    "            mdp.epsilon = epsilon\n",
    "    return sparsity_copy, MDPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a transition function with various settings for properties (e.g. deterministic, sparse, fixed) and train a classifier to predict URS | $\\pi = \\pi_0$ (baseline probability = 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a bunch of MDPs (with baseline/zero sparsity), solve some of them, \n",
    "# generate random policy for others\n",
    "\n",
    "def transition_function_sparse_loops(states, actions, fixed = False, **kwargs):\n",
    "    \"\"\"\n",
    "    Sparse transition function with guaranteed loops\n",
    "    TODO: possibly implement terminal states\n",
    "    \"\"\"\n",
    "    # print(fixed)\n",
    "    rng = np.random.default_rng(seed = 0) if fixed else None\n",
    "    transitions = np.zeros((actions, states, states))\n",
    "    for state in range(states):\n",
    "        self_loop = np.random.randint(0, actions) if not fixed else rng.integers(0, actions)\n",
    "        for action in range(actions):\n",
    "            if action == self_loop:\n",
    "                for next_state in range(states):\n",
    "                    transitions[action, state, next_state] = 1 if next_state == state else 0\n",
    "            else: # sparse randomness\n",
    "                transitions[action, state, :] = np.zeros(states)\n",
    "                transitions[action, state, np.random.randint(states) if not fixed else rng.integers(0, states)] = 1\n",
    "    return transitions\n",
    "\n",
    "NUM_MDPs = 10000\n",
    "fixed = False\n",
    "#print(np.random.uniform(1.0/NUM_ACTIONS/NUM_STATES**2, 1, NUM_MDPs))\n",
    "sparsity_levels = np.random.uniform(1.0/NUM_ACTIONS/NUM_STATES**2, 1.0, NUM_MDPs)\n",
    "#sparsity_levels = np.zeros(NUM_MDPs)\n",
    "# URS would be np.zeros(NUM_MDPs)\n",
    "\n",
    "random_pol_indices = np.random.choice(NUM_MDPs, int(NUM_MDPs / 2), replace = False) # The indices of the MDPs with random policies\n",
    "random_pol_set = set(random_pol_indices)\n",
    "# i not in random_pol_set = random_or_rr[i] == 1, sparsity_levels[i] > 0\n",
    "MDPS = generate_tests(NUM_MDPs, sparsity_levels = sparsity_levels,\n",
    "                      P_generator = transition_function_sparse_loops, fixed = fixed)[1]\n",
    "# print(np.ndim(MDPS[0].R))\n",
    "# Problem with _bounditer in ValueIteration happening when upper uniform bound is too high/sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(random_pol_indices)\n",
    "for i in range(NUM_MDPs): # 50% RR, 50% random\n",
    "    MDPS[i].run()\n",
    "for i in random_pol_indices:\n",
    "    MDPS[i].policy = np.random.randint(NUM_ACTIONS, size = NUM_STATES)\n",
    "policies = np.array([mdp.policy for mdp in MDPS])\n",
    "# print(policies.shape)\n",
    "random_or_rr = np.array([0 if i in random_pol_set else 1 for i in range(NUM_MDPs)])\n",
    "# 0 if random, 1 if generated from RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([MDPS[1].P[j] == MDPS[0].P[j] for j in range(NUM_ACTIONS)])\n",
    "assert not fixed or np.all([np.all([MDPS[i].P[j] == MDPS[0].P[j] for j in range(NUM_ACTIONS)]) for i in range(NUM_MDPs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 3 0 0 2 1 3 3]\n",
      " [1 0 1 0 0 0 1 0 2 2]\n",
      " [1 0 1 3 1 2 2 2 0 0]\n",
      " [1 0 2 2 1 0 2 2 0 1]\n",
      " [0 2 2 2 2 0 0 2 1 3]\n",
      " [0 0 3 0 0 1 1 0 0 1]\n",
      " [1 1 0 1 2 0 2 1 0 0]\n",
      " [2 2 3 3 2 3 3 2 0 3]\n",
      " [1 0 1 0 3 1 0 3 3 2]\n",
      " [0 0 2 0 3 0 2 2 2 1]] [0 1 1 1 1 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(policies[0:10], random_or_rr[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear Regression\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "\n",
    "def regression(X, y, test_size = 0.2, regression = LinearRegression):\n",
    "    \"\"\"\n",
    "    Trains a linear regression model on the given data, and returns the model and test data\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    model = regression().fit(X_train, y_train)\n",
    "    return model, model.predict_proba(X_test), y_test\n",
    "\n",
    "def neural_network(X, y, test_size = 0.2, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Trains a neural network on the given data, and returns the model and the mean squared error\n",
    "    \"\"\"\n",
    "    def build_model():\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(64, activation = 'relu', input_shape = [X.shape[1]]),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(64, activation = 'relu'),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            keras.layers.Dense(1, activation = 'sigmoid')\n",
    "        ])\n",
    "        return model\n",
    "    model = build_model()\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['mae'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
    "    model.fit(X_train, y_train, epochs = 100, validation_split = 0.2, verbose = 1, \n",
    "              callbacks = [keras.callbacks.EarlyStopping(patience = 3)])\n",
    "    return model, model.predict(X_test), y_test\n",
    "\n",
    "def find_loop_dist_and_length(transitions, policy: np.ndarray, initial_state):\n",
    "    \"\"\"\n",
    "    Computes the distance to the loop and the length of the loop for a given policy and initial state\n",
    "    \"\"\"\n",
    "    visited_states = {}  # Using a dict for quicker lookups\n",
    "    current_state = initial_state\n",
    "    step = 0  # Track the number of steps taken to find the loop length directly\n",
    "\n",
    "    while current_state not in visited_states:\n",
    "        visited_states[current_state] = step\n",
    "        # Simulate a transition\n",
    "        current_state = np.random.choice(np.arange(len(policy)), 1, \n",
    "                                         p = transitions[policy[current_state]][current_state]).item()\n",
    "        step += 1\n",
    "    \n",
    "    #distance to loop = visited_states[current_state]; loop length = step - visited_states[current_state]\n",
    "    return visited_states[current_state], step - visited_states[current_state]\n",
    "\n",
    "def takes_self_loop(transitions, policy: np.ndarray, initial_state):\n",
    "    \"\"\" \n",
    "    Returns 1 if the policy takes a self loop, 0 otherwise\n",
    "    \"\"\"\n",
    "    return int(transitions[policy[initial_state]][initial_state][initial_state] > 0.5)\n",
    "\n",
    "def num_out_arrows(transitions, policy: np.ndarray, initial_state):\n",
    "    \"\"\"\n",
    "    Returns the sum of outgoing arrows for each state that the policy visits from the \n",
    "    initial state before reaching a loop\n",
    "    \"\"\"\n",
    "    visited_states = {}  # Using a dict for quicker lookups\n",
    "    current_state = initial_state\n",
    "    step = 0  # Track the number of steps taken to find the loop length directly\n",
    "    out_arrows = 0\n",
    "    while current_state not in visited_states:\n",
    "        visited_states[current_state] = step\n",
    "        # Simulate a transition\n",
    "        current_state = np.random.choice(np.arange(len(policy)), 1, \n",
    "                                         p = transitions[policy[current_state]][current_state]).item()\n",
    "        step += 1\n",
    "        out_arrows += np.count_nonzero(transitions[policy[current_state]][current_state])\n",
    "    return out_arrows\n",
    "\n",
    "### Generate features\n",
    "encoder = OneHotEncoder(categories = 'auto', sparse_output = False, drop = 'first')\n",
    "# Drop first to avoid multicollinearity, large coefficients\n",
    "# encoder.fit(np.arange(NUM_ACTIONS))\n",
    "# print(encoder.categories_)\n",
    "\n",
    "### Train the model\n",
    "policies_encoded = encoder.fit_transform(policies)\n",
    "features = np.array([np.concatenate((np.array(MDPS[i].P).flatten(), policies_encoded[i]), axis = 0)\n",
    "                      for i in range(NUM_MDPs)])\n",
    "# features = encoder.fit_transform(policies)\n",
    "loop_lengths = np.array([[find_loop_dist_and_length(MDPS[i].P, policies[i], policies[i][j])[x] for j in range(NUM_STATES) for x in range(2)] \n",
    "                         for i in range(NUM_MDPs)])\n",
    "self_loops = np.array([[takes_self_loop(MDPS[i].P, policies[i], j) for j in range(NUM_STATES)] for i in range(NUM_MDPs)])\n",
    "# features = np.concatenate((features, encoder.fit_transform(loop_lengths)), axis = 1)\n",
    "out_arrows = np.array([[num_out_arrows(MDPS[i].P, policies[i], j) for j in range(NUM_STATES)] for i in range(NUM_MDPs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-entropy loss: 0.42391483087956155\n",
      "Accuracy: 0.813\n",
      "Baseline log loss: 0.6931471805599454\n",
      "Model coefficients, intercept: [[ 3.03961485e-01  2.97586531e-01  2.62080652e-01  1.35112469e-01\n",
      "   2.14286429e-01  4.62192760e-03  7.32581390e-02  1.55434988e-01\n",
      "   3.15008390e-01  3.13906358e-01  2.73764650e-01  6.05475176e-02\n",
      "  -7.35769878e-02  8.42021436e-01 -6.85430430e-02 -4.90698597e-01\n",
      "   3.68483430e-01  3.36184269e-01  2.96177462e-01  1.98113208e-01\n",
      "   5.29665370e-02 -1.05475547e-03  3.50603293e-01 -5.68515001e-01\n",
      "   1.89644730e-01  4.91098889e-01  3.12858176e-01  3.21420159e-01\n",
      "   1.99877438e-01 -9.23429681e-02 -8.75744068e-02 -1.70162928e-01\n",
      "   2.55013872e-01  2.75473327e-01  2.87785985e-01  4.52123427e-05\n",
      "   1.38495322e-01 -6.94435467e-02 -8.36470427e-02  1.20722312e+00\n",
      "  -2.29671941e-01  4.04336060e-01  5.59831173e-01  2.35414714e-01\n",
      "  -4.35013185e-03  4.12247573e-02 -1.11240667e-01  3.31585249e-01\n",
      "  -2.14478228e-01  2.78405911e-01  2.70014228e-01  9.44175486e-02\n",
      "   4.44343090e-02 -9.78158506e-02 -3.67303334e-02  6.20139573e-01\n",
      "   1.86269628e-01  5.22680607e-01  3.94193332e-01  2.81333714e-01\n",
      "   2.04133503e-01  7.84582250e-02  2.59041082e-01 -2.36459230e-01\n",
      "  -6.40013446e-02 -3.97953907e-01  2.39375631e-01  3.46128042e-01\n",
      "   1.59536439e-01 -6.14582964e-02  1.18031759e-01  4.86141929e-03\n",
      "   5.07666826e-01  1.55723771e-01  4.77228200e-01  3.55237556e-01\n",
      "   2.81691942e-01  1.91909254e-01  3.15128981e-01 -3.36883772e-01\n",
      "   4.86980443e-01 -6.10517861e-01  0.00000000e+00 -1.43702184e+00\n",
      "  -1.16310894e+00 -1.21328239e+00 -1.22086745e+00 -1.51693966e+00\n",
      "  -1.23300214e+00 -1.34981423e+00 -1.03210520e+00 -1.46054481e+00\n",
      "  -1.15145396e+00]] [0.38653712]\n",
      "Sample outputs: [(array([0.2284465, 0.7715535]), 0), (array([0.17947348, 0.82052652]), 1), (array([0.0664876, 0.9335124]), 1), (array([0.98108146, 0.01891854]), 0), (array([0.11203101, 0.88796899]), 1), (array([0.94425484, 0.05574516]), 0), (array([0.06487996, 0.93512004]), 1), (array([0.07491568, 0.92508432]), 1), (array([0.45354785, 0.54645215]), 0), (array([0.10256053, 0.89743947]), 1)]\n"
     ]
    }
   ],
   "source": [
    "# features = np.concatenate((encoder.fit_transform(policies), encoder.fit_transform(loop_lengths),\n",
    "#                            ), axis = 1)\n",
    "# print(loop_lengths[0:10])\n",
    "features = np.concatenate((encoder.fit_transform(out_arrows), self_loops), axis = 1) # for interpretability\n",
    "model, y_pred, y_test = regression(features, random_or_rr, regression = partial(LogisticRegression, max_iter = 1000))\n",
    "print(\"Average cross-entropy loss:\", log_loss(y_test, y_pred, normalize = True))\n",
    "print(\"Accuracy:\", np.mean([np.round(y_pred[i][0]) != y_test[i] for i in range(len(y_pred))])) \n",
    "\n",
    "# if round(y_pred[0]) is 0, then model thinks 1 is more likely; if 1, then 0 is more likely\n",
    "# print(y_pred)\n",
    "print(\"Baseline log loss:\", log_loss(y_test, np.full(y_pred.shape, 0.5), normalize = True))\n",
    "print(\"Model coefficients, intercept:\", model.coef_, model.intercept_)\n",
    "print(\"Sample outputs:\", [(y_pred[i], y_test[i]) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: [0 0 2 2 2 0 3 2 0 3] Probability: [0.14126505 0.85873495] Actual: 1\n",
      "Policy: [1 0 2 3 2 3 2 0 1 1] Probability: [0.13567779 0.86432221] Actual: 1\n",
      "Policy: [2 0 0 0 1 3 1 0 2 0] Probability: [0.13082551 0.86917449] Actual: 1\n",
      "Policy: [0 0 3 3 2 1 2 2 0 2] Probability: [0.13049808 0.86950192] Actual: 1\n",
      "Policy: [3 3 3 1 0 1 1 1 3 2] Probability: [0.08836382 0.91163618] Actual: 1\n",
      "Policy: [1 0 2 2 2 3 0 0 1 2] Probability: [0.8658411 0.1341589] Actual: 1\n",
      "Policy: [1 0 3 2 3 2 1 2 1 1] Probability: [0.86565443 0.13434557] Actual: 1\n",
      "Policy: [3 1 2 1 0 2 3 1 1 1] Probability: [0.84712528 0.15287472] Actual: 1\n",
      "Policy: [0 1 1 2 3 2 2 3 3 0] Probability: [0.8466504 0.1533496] Actual: 1\n",
      "Policy: [0 2 0 1 1 1 3 0 3 3] Probability: [0.83482429 0.16517571] Actual: 0\n"
     ]
    }
   ],
   "source": [
    "### Grab the five policies with the highest and lowest probabilities of being random\n",
    "import networkx as nx\n",
    "\n",
    "if fixed:\n",
    "    # Generate a graph of the first MDP\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(NUM_STATES):\n",
    "        G.add_node(i)\n",
    "    enumerated_edges = {}\n",
    "    for i in range(NUM_ACTIONS):\n",
    "        enumerated_edges[i] = []\n",
    "        for j in range(NUM_STATES):\n",
    "            for k in range(NUM_STATES):\n",
    "                if MDPS[0].P[i][j, k] == 1:\n",
    "                    G.add_edge(j, k, action = i)\n",
    "                    enumerated_edges[i].append((j, k))\n",
    "    edge_labels = {(u, v): f\"{d['action']}\" for u, v, d in G.edges(data=True)}\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=20)  # k: Optimal distance between nodes. Increase/decrease to spread nodes out\n",
    "    nx.draw(G, pos = pos, with_labels = True)\n",
    "    nx.draw_networkx_edge_labels(G, pos = pos, edge_labels = edge_labels)\n",
    "    \n",
    "    for i in range(NUM_ACTIONS):\n",
    "        print(f\"Action {i} transitions:\", enumerated_edges[i])\n",
    "\n",
    "highest_probs = np.argsort(y_pred[:, 1])[-5:]\n",
    "lowest_probs = np.argsort(y_pred[:, 1])[:5]\n",
    "#print(\"Highest probabilities:\", [(y_pred[i], y_test[i]) for i in highest_probs])\n",
    "for i in np.concatenate((highest_probs, lowest_probs)):\n",
    "    print(\"Policy:\", policies[i], \"Probability:\", y_pred[i], \"Actual:\", y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On a random deterministic MDP(s), it doesn't seem like URS is identifiable, which is perhaps to be expected as every policy is optimal for some (normalized) reward function\n",
    "    - This also matches our results when looking at the distribution of optimal policies for \"cloud\"-y MDPs\n",
    "- As a control, with MDPs with loops, passing in just the policy (which shouldn't give much information without the related MDP) gives 0.54 accuracy\n",
    "    - Why not 0.50 exactly?\n",
    "    - Similar results with neural network\n",
    "    - This holds true when we use the label predictions for regression (model.predict), as well as the probability prediction (model.predict_proba)\n",
    "- Passing in the policy with the transition function (flattened) gives 0.53 accuracy, even with the NN\n",
    "    - Would likely need a graph neural network to train deep enough \"circuits\" to use the data of the whole transition function effectively\n",
    "- Distance to loop correlates somewhat well with P(URS) (~0.66-0.68 accuracy, 0.61-0.63 log loss on a diverse dataset of sparse transition functions), length of loop not as well (~0.56 accuracy, 0.687 log loss)\n",
    "    - Putting them together doesn't give improvement (~0.67-0.71 accuracy, 0.57-0.62 log loss)\n",
    "    - Intuitively, the length of the loop an optimal policy takes is its “goal complexity”; distance to loop = “agency” \n",
    "- Setting $k \\in U[1, N/2]$ gives:\n",
    "    - 0.56 accuracy, 0.688 log loss with length of loop; 0.66-0.672 accuracy, 0.61 log loss with distance to loop\n",
    "    - Setting the upper bound of $k$ too high results in some weird MDP package errors, I suspect because sparsity is too high\n",
    "    - This matches the distribution results we found in reward_function.ipynb, as sparsity didn't seem to \"matter\" until around ~0.9 given (S, A) = (10, 4)\n",
    "- $k \\in U[1, N]$ gives similar results\n",
    "    - (Note that this was run with PolicyIterationModified instead of ValueIteration with the same settings, which I don't expect to change any of the results, but I might be wrong)\n",
    "    - 0.72-0.74 accuracy with policy, distance to loop, and length of loop\n",
    "- Calculating whether the policy enters a self loop or not for each state $s$ gives 0.80 accuracy, 0.43 log loss!\n",
    "    - With policy and distance to loop included, ~0.82-0.84 accuracy, 0.38-0.40 log loss\n",
    "    - Similar results with neural network\n",
    "    - Surprisingly, the logistic coefficients are all negative, meaning that a policy that takes more self-loops is *less* likely to be sampled via URS or USS\n",
    "        - 0.9999 chance of being from UPS if the policy always takes self-loops; 0.95 chance of being from URS/USS if it never takes self-loops\n",
    "- Calculating out-arrows also gives ~0.80 accuracy, 0.44 log loss\n",
    "    - Combining with self-loops doesn't give much\n",
    "- TODO: run these tests multiple times to make box plots in the writeup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
