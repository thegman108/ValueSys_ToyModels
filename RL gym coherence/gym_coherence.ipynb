{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup:\n",
    "- Starting point: just try to train classifier on RL policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DQN implementation\n",
    "\n",
    "# Define the Q-network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.state_dict()\n",
    "\n",
    "# Experience Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=1e-2, batch_size=64, gamma=0.99, replay_size=1000):\n",
    "        self.model = DQN(state_dim, action_dim, hidden_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.replay_buffer = ReplayBuffer(replay_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        state = torch.FloatTensor(state)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        if len(state.shape) == 1:\n",
    "            state = state.reshape(-1, 1)\n",
    "        if len(next_state.shape) == 1:\n",
    "            next_state = next_state.reshape(-1, 1)\n",
    "        action = torch.LongTensor(action)\n",
    "        reward = torch.FloatTensor(reward)\n",
    "        done = torch.FloatTensor(done)\n",
    "\n",
    "        q_values = self.model.forward(state)\n",
    "        next_q_values = self.model.forward(next_state)\n",
    "\n",
    "        # state = state.T\n",
    "        # next_state = next_state.T\n",
    "        \n",
    "        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        next_q_value = next_q_values.max(1)[0]\n",
    "        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "        \n",
    "        loss = nn.MSELoss()(q_value, expected_q_value.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.FloatTensor(np.expand_dims(state, 0))\n",
    "            q_value = self.model(state)\n",
    "            action = q_value.max(-1)[1].item()\n",
    "        else:\n",
    "            action = random.randrange(self.action_dim)\n",
    "        return action\n",
    "    \n",
    "class QTableAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-2, gamma=0.99):\n",
    "        self.q_table = np.zeros((state_dim, action_dim))\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        q_value = self.q_table[state, action]\n",
    "        next_q_value = np.max(self.q_table[next_state])\n",
    "        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n",
    "        \n",
    "        self.q_table[state, action] += self.lr * (expected_q_value - q_value)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            action = np.argmax(self.q_table[state])\n",
    "        else:\n",
    "            action = random.randrange(self.action_dim)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train_dqn\n",
    "NUM_NON_ZERO_REWARDS = 0\n",
    "def one_hot_state(state, env):\n",
    "    state_arr = np.zeros(env.observation_space.n)\n",
    "    state_arr[state] = 1\n",
    "    return state_arr\n",
    "\n",
    "def train_dqn(env_name=\"CartPole-v1\", episodes=500, epsilon_start=1.0, epsilon_final=0.01, \n",
    "              epsilon_decay=500, reward_function = None, verbose = False, return_reward = False, \n",
    "              print_every=50, **kwargs):\n",
    "    \"\"\"\n",
    "    Train a DQN agent on the specified environment.\n",
    "    \n",
    "    Args:\n",
    "        env_name: str\n",
    "            Name of the environment to train the agent on.\n",
    "        episodes: int\n",
    "            Number of episodes to train the agent for.\n",
    "        epsilon_start: float\n",
    "            Initial epsilon value for epsilon-greedy action selection.\n",
    "        epsilon_final: float\n",
    "            Final epsilon value for epsilon-greedy action selection.\n",
    "        epsilon_decay: float\n",
    "            Decay rate for epsilon.\n",
    "        reward_function: function\n",
    "            Optional reward function to use for training.\n",
    "        verbose: bool\n",
    "            Whether to print training progress.\n",
    "\n",
    "    Returns:\n",
    "        DQNAgent: trained DQN agent. \n",
    "    \"\"\"\n",
    "    global NUM_NON_ZERO_REWARDS\n",
    "    env = gym.make(env_name)\n",
    "    if len(env.observation_space.shape) == 0:\n",
    "        state_dim = env.observation_space.n\n",
    "    else:\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    agent = DQNAgent(state_dim, action_dim, **kwargs)\n",
    "    \n",
    "    epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay)\n",
    "    \n",
    "    rewards = np.zeros(episodes) \n",
    "    is_state_discrete = hasattr(env.observation_space, 'n')\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset() # Reset the environment, reward\n",
    "        if is_state_discrete and state_dim == env.observation_space.n:\n",
    "            state = one_hot_state(state, env)\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            epsilon = epsilon_by_frame(episode)\n",
    "            # One-hot encode the state\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if is_state_discrete and state_dim == env.observation_space.n:\n",
    "                next_state = one_hot_state(next_state, env)\n",
    "\n",
    "            if reward_function: #custom reward function\n",
    "                reward = reward_function(done, state, action, next_state)\n",
    "            NUM_NON_ZERO_REWARDS += 0 if math.isclose(reward, 0) else 1\n",
    "            \n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            agent.update()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            # print(f\"Episode: {episode+1}, Total reward: {episode_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "        rewards[episode] = episode_reward\n",
    "        # Optional: Render the environment to visualize training progress\n",
    "        if verbose and episode % print_every == print_every - 1:\n",
    "        #     render_env(env, agent)\n",
    "            print(f\"Episode: {episode+1}, Average total reward: {np.average(rewards[episode - print_every + 1 : episode])}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    return agent if not return_reward else (agent, rewards)\n",
    "\n",
    "# Optional: Function to render the environment with the current policy\n",
    "def render_env(env, agent):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state, 0)  # Using 0 epsilon for greedy action selection\n",
    "        # print(env.step(action))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qtable(env_name=\"CartPole-v1\", episodes=500, epsilon_start=1.0, epsilon_final=0.01, \n",
    "              epsilon_decay=500, reward_function = None, verbose = False, return_reward = False, \n",
    "              print_every=50, **kwargs):\n",
    "    \"\"\"\n",
    "    Train a Q-table agent on the specified environment.\"\"\"\n",
    "    global NUM_NON_ZERO_REWARDS\n",
    "    env = gym.make(env_name)\n",
    "    if len(env.observation_space.shape) == 0:\n",
    "        state_dim = env.observation_space.n\n",
    "    else:\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    agent = QTableAgent(state_dim, action_dim, **kwargs)\n",
    "\n",
    "    rewards = np.zeros(episodes)\n",
    "    epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay)\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            epsilon = epsilon_by_frame(episode)\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if reward_function:\n",
    "                reward = reward_function(done, state, action, next_state)\n",
    "\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards[episode] = episode_reward\n",
    "        if verbose and episode % print_every == print_every - 1:\n",
    "            print(f\"Episode: {episode+1}, Average total reward: {np.average(rewards[episode - print_every + 1 : episode])}, Epsilon: {epsilon:.2f}\")\n",
    "        \n",
    "    env.close()\n",
    "    return agent if not return_reward else (agent, rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test_dqn\n",
    "NEAR_ZERO = 1e-9\n",
    "def test_dqn(env, agent, episodes=10, reward_function=None, verbose = False):\n",
    "    print(f\"Maximum reward: {env.spec.reward_threshold}\")\n",
    "    average_value = 0\n",
    "    for episode in range(episodes):\n",
    "        # if episode == 0:\n",
    "        #     render_env(env, agent)\n",
    "        state = env.reset()\n",
    "        if len(env.observation_space.shape) == 0:\n",
    "            state = one_hot_state(state, env)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state, 0)  # Using 0 epsilon for greedy action selection\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if len(env.observation_space.shape) == 0:\n",
    "                next_state = one_hot_state(next_state, env)\n",
    "            if reward_function:\n",
    "                reward = reward_function(done, state, action, next_state)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        if verbose:\n",
    "            print(f\"Episode: {episode+1}, Total reward: {episode_reward}\")\n",
    "        average_value += episode_reward\n",
    "    average_value /= episodes\n",
    "    print(f\"Average reward: {average_value}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qtable(env, agent, episodes=10, reward_function=None, verbose = False):\n",
    "    \"\"\"\n",
    "    Test a Q-table agent on the specified environment.\n",
    "    (This is basically test_dqn but without the one-hot encoding.)\n",
    "    \"\"\"\n",
    "    print(f\"Maximum reward: {env.spec.reward_threshold}\")\n",
    "    average_value = 0\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state, 0)  # Using 0 epsilon for greedy action selection\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if reward_function:\n",
    "                reward = reward_function(done, state, action, next_state)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        if verbose:\n",
    "            print(f\"Episode: {episode+1}, Total reward: {episode_reward}\")\n",
    "        average_value += episode_reward\n",
    "    average_value /= episodes\n",
    "    print(f\"Average reward: {average_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000, Average total reward: -147.58058058058057, Epsilon: 0.01\n",
      "Episode: 2000, Average total reward: -59.01601601601602, Epsilon: 0.01\n",
      "Episode: 3000, Average total reward: -26.87887887887888, Epsilon: 0.01\n",
      "Episode: 4000, Average total reward: -18.01801801801802, Epsilon: 0.01\n",
      "Episode: 5000, Average total reward: -15.036036036036036, Epsilon: 0.01\n",
      "Episode: 6000, Average total reward: -13.521521521521521, Epsilon: 0.01\n",
      "Episode: 7000, Average total reward: -12.774774774774775, Epsilon: 0.01\n",
      "Episode: 8000, Average total reward: -12.16916916916917, Epsilon: 0.01\n",
      "Episode: 9000, Average total reward: -11.883883883883884, Epsilon: 0.01\n",
      "Episode: 10000, Average total reward: -11.901901901901901, Epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "env_name = \"Taxi-v3\"\n",
    "agent, rewards = train_qtable(env_name = env_name, episodes = 10000, verbose = True, return_reward = True,\n",
    "                           epsilon_decay=10, lr=0.1, gamma=0.9, print_every=1000, \n",
    "                           reward_function = lambda done, *args: 1 if done else -1)  \n",
    "# rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum reward: 8\n",
      "Average reward: -76.4\n"
     ]
    }
   ],
   "source": [
    "test_qtable(gym.make(env_name), agent, episodes = 100)\n",
    "# agent.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, Average total reward: 25.20408163265306, Epsilon: 0.91\n",
      "Episode: 100, Average total reward: 30.918367346938776, Epsilon: 0.82\n",
      "Episode: 150, Average total reward: 36.244897959183675, Epsilon: 0.74\n",
      "Episode: 200, Average total reward: 51.53061224489796, Epsilon: 0.67\n",
      "Episode: 250, Average total reward: 65.10204081632654, Epsilon: 0.61\n",
      "Episode: 300, Average total reward: 69.73469387755102, Epsilon: 0.55\n",
      "Episode: 350, Average total reward: 99.59183673469387, Epsilon: 0.50\n",
      "Episode: 400, Average total reward: 167.24489795918367, Epsilon: 0.46\n",
      "Episode: 450, Average total reward: 158.81632653061226, Epsilon: 0.41\n",
      "Episode: 500, Average total reward: 99.95918367346938, Epsilon: 0.37\n",
      "Episode: 550, Average total reward: 167.53061224489795, Epsilon: 0.34\n",
      "Episode: 600, Average total reward: 105.61224489795919, Epsilon: 0.31\n",
      "Episode: 650, Average total reward: 169.55102040816325, Epsilon: 0.28\n",
      "Episode: 700, Average total reward: 80.6734693877551, Epsilon: 0.25\n",
      "Episode: 750, Average total reward: 54.775510204081634, Epsilon: 0.23\n",
      "Episode: 800, Average total reward: 123.81632653061224, Epsilon: 0.21\n",
      "Episode: 850, Average total reward: 55.734693877551024, Epsilon: 0.19\n",
      "Episode: 900, Average total reward: 99.95918367346938, Epsilon: 0.17\n",
      "Episode: 950, Average total reward: 127.48979591836735, Epsilon: 0.16\n",
      "Episode: 1000, Average total reward: 211.08163265306123, Epsilon: 0.14\n"
     ]
    }
   ],
   "source": [
    "### NN to GCN Data conversion\n",
    "\n",
    "#agent.model.get_weights()\n",
    "\n",
    "# Define a simple GCN model\n",
    "from torch_geometric.data import Data\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super(GCN, self).__init__()\n",
    "        # Define the GCN layers\n",
    "        self.conv1 = GCNConv(data.num_node_features, 4)  # Input features to hidden\n",
    "        self.conv2 = GCNConv(4, 2)  # Hidden to output features\n",
    "        self.data = data\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # Pass data through the first GCN layer, then apply ReLU\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        # Pass data through the second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def nn_to_data(model: nn.Module) -> Data:\n",
    "    edges = []\n",
    "\n",
    "    # Counter for global neuron index\n",
    "    idx = 0\n",
    "\n",
    "    # Iterate over each layer in the network\n",
    "    base = next(model.children())\n",
    "    if isinstance(base, nn.Sequential):\n",
    "        layers = list(base.children())\n",
    "        layers2 = list(base.children())\n",
    "    else:\n",
    "        layers = list(model.children()) # iterator over the layers of the model\n",
    "        layers2 = list(model.children())\n",
    "    \n",
    "    num_nodes = layers2[0].weight.shape[1] + sum([layer.weight.shape[0] for layer in layers2 if isinstance(layer, nn.Linear)])\n",
    "    num_node_features = num_nodes\n",
    "    node_features = torch.zeros(num_nodes, num_node_features)\n",
    "    # shape = (num_nodes, num_node_features), where the node features are the bias of each node\n",
    "    # and the weights of the edges to each node (zero if there is no edge)\n",
    "\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # Update edges based on the weight matrix\n",
    "            input_dim = layer.weight.shape[1]\n",
    "            output_dim = layer.weight.shape[0]\n",
    "            for i in range(input_dim):  # Input neurons (e.g. 4)\n",
    "                for j in range(output_dim):  # Output neurons (e.g. 64)\n",
    "                    edges.append((idx + i, idx + input_dim + j))\n",
    "            \n",
    "            # Update node features (e.g., biases)\n",
    "            biases = torch.tensor(layer.bias.detach().numpy())\n",
    "            edge_weights = torch.tensor(layer.weight.detach().numpy().T)\n",
    "            node_features[idx + input_dim:idx + input_dim + output_dim, 0] = biases\n",
    "            node_features[idx:idx + input_dim, 1+idx:1+idx+output_dim] = edge_weights\n",
    "            node_features[idx + input_dim:idx + input_dim + output_dim, 1+idx:1+idx+input_dim] = edge_weights.T\n",
    "            \n",
    "            # Update the global neuron index\n",
    "            idx += input_dim\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    num_nonzero = [np.count_nonzero(node_features[i]) for i in range(node_features.shape[0])]\n",
    "    # print(num_nonzero)\n",
    "    row_mean, row_median, row_var = torch.mean(node_features[:, 1:], dim=1), torch.median(node_features[:, 1:], dim=1)[0], torch.var(node_features[:, 1:], dim=1)\n",
    "    x = torch.stack([node_features[:, 0], row_mean, row_median, row_var]).T\n",
    "    # print(x.shape)\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "agent = train_dqn(env_name = \"CartPole-v1\", episodes = 1000, verbose = True, return_reward = False)\n",
    "data = nn_to_data(agent.model)\n",
    "gcn = GCN(data)\n",
    "# data.x.shape, data.edge_index.shape\n",
    "# print(data.x)\n",
    "\n",
    "#Debug\n",
    "out_of_bounds = data.edge_index >= data.x.shape[0]\n",
    "if out_of_bounds.any():\n",
    "    print(\"Out-of-bounds indices found at locations:\")\n",
    "    print(data.edge_index[:, out_of_bounds.any(dim=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reward function calls: 59020\n",
      "Number of non-zero rewards: 59020\n"
     ]
    }
   ],
   "source": [
    "# Dataset generation\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "NEAR_ZERO = 1e-9\n",
    "NUM_REWARD_CALLS = 0\n",
    "NUM_NON_ZERO_REWARDS = 0\n",
    "def deterministic_random(*args, lb = -1, ub = 1, sparsity = 0.0, continuous = False):\n",
    "    \"\"\"\n",
    "    Create a deterministic random number generator for a given set of arguments.\n",
    "    Used to generate deterministic reward functions for the coherence classifier.\n",
    "    [Edit 4/3/24: adapted to continuous state space]\"\"\"\n",
    "    global NUM_REWARD_CALLS\n",
    "    NUM_REWARD_CALLS += 1\n",
    "    unique_seed = f\"{args}\".encode(\"utf-8\")\n",
    "    random.seed(unique_seed)\n",
    "    return random.uniform(lb, ub) if random.random() > sparsity else random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "\n",
    "NUM_TRAIN_R_FUNCS = 50\n",
    "NUM_EPS_TRAIN_R = 50\n",
    "URS_r_funcs = [lambda *args: deterministic_random(args) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "URS_agents = [train_dqn(env_name = env_name, \n",
    "                        episodes=NUM_EPS_TRAIN_R, reward_function=r_func) for r_func in URS_r_funcs]\n",
    "USS_r_funcs = [lambda *args: deterministic_random(args, sparsity=0.99) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "print(f\"Number of reward function calls: {NUM_REWARD_CALLS}\")\n",
    "print(f\"Number of non-zero rewards: {NUM_NON_ZERO_REWARDS}\")\n",
    "USS_agents = [train_dqn(env_name = env_name, \n",
    "                        episodes=NUM_EPS_TRAIN_R, reward_function=r_func) for r_func in USS_r_funcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6014137224608205,\n",
       " 5.734868810378968e-10,\n",
       " 0.18947200717913826,\n",
       " -0.11464719428521586,\n",
       " 1.9375194864306798e-11,\n",
       " 3.1131227593489704e-10,\n",
       " -7.023178277046693e-10,\n",
       " 2.965355797951794e-10,\n",
       " 0.41831271768541045,\n",
       " -3.207247699354683e-10]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if deterministic_random is deterministic and has the correct sparsity\n",
    "assert deterministic_random(1, 2, 3, 4) == deterministic_random(1, 2, 3, 4)\n",
    "assert not deterministic_random(1, 2, 3, 4) == deterministic_random(1, 2, 3, 6)\n",
    "[deterministic_random(1, 2, 3, i, sparsity = 0.5) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, Average total reward: -0.08030391727204088, Epsilon: 0.91\n",
      "Episode: 100, Average total reward: 1.1898305988496911, Epsilon: 0.82\n",
      "Episode: 150, Average total reward: 0.5990051160714706, Epsilon: 0.74\n",
      "Episode: 200, Average total reward: -0.20225874692314827, Epsilon: 0.67\n",
      "Episode: 250, Average total reward: -0.3040020801607395, Epsilon: 0.61\n",
      "Episode: 300, Average total reward: 0.088487752426798, Epsilon: 0.55\n",
      "Episode: 350, Average total reward: 0.17828783475711782, Epsilon: 0.50\n",
      "Episode: 400, Average total reward: 0.22612443296435566, Epsilon: 0.46\n",
      "Episode: 450, Average total reward: 0.48808380894686876, Epsilon: 0.41\n",
      "Episode: 500, Average total reward: -0.8578852051258352, Epsilon: 0.37\n"
     ]
    }
   ],
   "source": [
    "# Test when do USS agents have non-zero rewards\n",
    "env_name = \"CartPole-v1\"\n",
    "USS_test_r_func = lambda *args: deterministic_random(args, sparsity=0.0)\n",
    "assert USS_test_r_func(42) == USS_test_r_func(42)\n",
    "USS_test_agent = train_dqn(env_name = env_name, episodes=500, reward_function=USS_test_r_func, \n",
    "                           verbose = True)\n",
    "# Epsilon measuring how much the agent is exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum reward: 475.0\n",
      "Average reward: -3.051088483330308\n"
     ]
    }
   ],
   "source": [
    "# epsilon_final, epsilon_start, epsilon_decay = 0.01, 1.0, 500\n",
    "# [epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay) for frame_idx in range(500)]\n",
    "test_dqn(gym.make(env_name), USS_test_agent, reward_function=USS_test_r_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([134, 4])\n"
     ]
    }
   ],
   "source": [
    "### Define and train GCN classifier on NNs\n",
    "\n",
    "def get_state_shape(env):\n",
    "    return 1 if len(env.observation_space.shape) == 0 else env.observation_space.shape[0]\n",
    "def get_state_size(env):\n",
    "    return env.observation_space.n if len(env.observation_space.shape) == 0 else env.observation_space.shape[0]\n",
    "UPS_agents = [DQNAgent(get_state_size(env), env.action_space.n) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool, GCNConv, GATConv\n",
    "\n",
    "class GraphLevelGCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(GraphLevelGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 16)\n",
    "        self.linear = torch.nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        # edge_weights = data.edge_attr\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Aggregate node features to graph-level features\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Make a binary classification prediction\n",
    "        x = self.linear(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class GATGraphLevelBinary(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(GATGraphLevelBinary, self).__init__()\n",
    "        self.conv1 = GATConv(num_node_features, 8, heads=8, dropout=0.6)\n",
    "        # Increase the number of output features from the first GAT layer\n",
    "        self.conv2 = GATConv(8 * 8, 16, heads=1, concat=False, dropout=0.6)\n",
    "        # Additional GAT layer for richer node representations\n",
    "        self.linear = torch.nn.Linear(16, 1)\n",
    "        # Final linear layer to produce a graph-level output\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)  # Aggregate node features to graph-level\n",
    "        x = self.linear(x)\n",
    "        return torch.sigmoid(x)  # Sigmoid activation function for binary classification\n",
    "\n",
    "# Training loop\n",
    "USS_data = [nn_to_data(agent.model) for agent in USS_agents]\n",
    "URS_data = [nn_to_data(agent.model) for agent in URS_agents]\n",
    "print(URS_data[0].x.shape)\n",
    "UPS_data = [nn_to_data(agent.model) for agent in UPS_agents]\n",
    "assert URS_data[0].x.shape == UPS_data[0].x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Train Loss: 0.7368341466411948, Average Test Loss: 0.6454022590070962\n",
      "Epoch 2: Average Train Loss: 0.6828794175758958, Average Test Loss: 0.64594763207715\n",
      "Epoch 3: Average Train Loss: 0.6786422868259251, Average Test Loss: 0.644322702777572\n",
      "Epoch 4: Average Train Loss: 0.6756062641274184, Average Test Loss: 0.642260631674435\n",
      "Epoch 5: Average Train Loss: 0.6731930308509618, Average Test Loss: 0.6402062258508522\n",
      "Epoch 6: Average Train Loss: 0.6713151082862169, Average Test Loss: 0.6383412880008109\n",
      "Epoch 7: Average Train Loss: 0.6697166946483776, Average Test Loss: 0.636723642249126\n",
      "Epoch 8: Average Train Loss: 0.6685101984301582, Average Test Loss: 0.6353228765859967\n",
      "Epoch 9: Average Train Loss: 0.6675114025827498, Average Test Loss: 0.6340729216404725\n",
      "Epoch 10: Average Train Loss: 0.6667067179689183, Average Test Loss: 0.6329930205072742\n",
      "Epoch 11: Average Train Loss: 0.6660635713487864, Average Test Loss: 0.6320858108156244\n",
      "Epoch 12: Average Train Loss: 0.6655761441215873, Average Test Loss: 0.6313317055508378\n",
      "Epoch 13: Average Train Loss: 0.6651853720191866, Average Test Loss: 0.6306671783866478\n",
      "Epoch 14: Average Train Loss: 0.6648770130705088, Average Test Loss: 0.6301685994912987\n",
      "Epoch 15: Average Train Loss: 0.6648278450942599, Average Test Loss: 0.6296799822855974\n",
      "Epoch 16: Average Train Loss: 0.6643956566695124, Average Test Loss: 0.629236677956942\n",
      "Epoch 17: Average Train Loss: 0.6642091403482482, Average Test Loss: 0.6289181915330119\n",
      "Epoch 18: Average Train Loss: 0.664458000974264, Average Test Loss: 0.6285258048701508\n",
      "Epoch 19: Average Train Loss: 0.663896920811385, Average Test Loss: 0.6282752023034845\n",
      "Epoch 20: Average Train Loss: 0.6638673717156053, Average Test Loss: 0.6279347235176829\n",
      "Epoch 21: Average Train Loss: 0.6638949116459116, Average Test Loss: 0.6275913625642715\n",
      "Epoch 22: Average Train Loss: 0.6634719258057885, Average Test Loss: 0.6274175882273993\n",
      "Epoch 23: Average Train Loss: 0.6637801863776985, Average Test Loss: 0.6269785967415373\n",
      "Epoch 24: Average Train Loss: 0.6631140497629531, Average Test Loss: 0.6267457828886108\n",
      "Epoch 25: Average Train Loss: 0.663369671034161, Average Test Loss: 0.6263781979410851\n",
      "Epoch 26: Average Train Loss: 0.6626911607687361, Average Test Loss: 0.626178216640983\n",
      "Epoch 27: Average Train Loss: 0.6626315431261901, Average Test Loss: 0.6258030354219954\n",
      "Epoch 28: Average Train Loss: 0.6621634825482033, Average Test Loss: 0.625712629854388\n",
      "Epoch 29: Average Train Loss: 0.6619205291965045, Average Test Loss: 0.6257904390004114\n",
      "Epoch 30: Average Train Loss: 0.6615647658938542, Average Test Loss: 0.6254456719238078\n",
      "Epoch 31: Average Train Loss: 0.6613106212695129, Average Test Loss: 0.6255695290237782\n",
      "Epoch 32: Average Train Loss: 0.6609500117425341, Average Test Loss: 0.6255327603525075\n",
      "Epoch 33: Average Train Loss: 0.6607024321390782, Average Test Loss: 0.6255708736243832\n",
      "Epoch 34: Average Train Loss: 0.6603214485221542, Average Test Loss: 0.6254528766578005\n",
      "Epoch 35: Average Train Loss: 0.659977982629789, Average Test Loss: 0.6253821839185548\n",
      "Epoch 36: Average Train Loss: 0.6595524444768671, Average Test Loss: 0.6252805284682836\n",
      "Epoch 37: Average Train Loss: 0.6590356033004354, Average Test Loss: 0.6248640857134887\n",
      "Epoch 38: Average Train Loss: 0.6583707161946222, Average Test Loss: 0.6243456366282771\n",
      "Epoch 39: Average Train Loss: 0.6577818100049626, Average Test Loss: 0.6238497938087676\n",
      "Epoch 40: Average Train Loss: 0.6571732534328476, Average Test Loss: 0.6235238098088303\n",
      "Epoch 41: Average Train Loss: 0.6566229807736818, Average Test Loss: 0.6231443891272648\n",
      "Epoch 42: Average Train Loss: 0.6560514058452099, Average Test Loss: 0.6228799654818431\n",
      "Epoch 43: Average Train Loss: 0.6555204674426932, Average Test Loss: 0.6225149516474631\n",
      "Epoch 44: Average Train Loss: 0.6549792591191362, Average Test Loss: 0.6222671264069504\n",
      "Epoch 45: Average Train Loss: 0.6544581934867892, Average Test Loss: 0.6219468374103598\n",
      "Epoch 46: Average Train Loss: 0.6538846532057505, Average Test Loss: 0.6215413620739128\n",
      "Epoch 47: Average Train Loss: 0.6534056871663779, Average Test Loss: 0.6212632370770734\n",
      "Epoch 48: Average Train Loss: 0.6528748531301971, Average Test Loss: 0.6210107843100559\n",
      "Epoch 49: Average Train Loss: 0.652354620047845, Average Test Loss: 0.6208462026799679\n",
      "Epoch 50: Average Train Loss: 0.6518139899941161, Average Test Loss: 0.6204654185763502\n",
      "Epoch 51: Average Train Loss: 0.6512652410427109, Average Test Loss: 0.6202625475198147\n",
      "Epoch 52: Average Train Loss: 0.6507593597634695, Average Test Loss: 0.6201607356400928\n",
      "Epoch 53: Average Train Loss: 0.6503047725011128, Average Test Loss: 0.619949726772029\n",
      "Epoch 54: Average Train Loss: 0.6498290490766522, Average Test Loss: 0.6195979994368827\n",
      "Epoch 55: Average Train Loss: 0.6494167951401323, Average Test Loss: 0.6196152953732963\n",
      "Epoch 56: Average Train Loss: 0.6490475021593738, Average Test Loss: 0.6193798344902461\n",
      "Epoch 57: Average Train Loss: 0.6486054852721281, Average Test Loss: 0.6190768948694313\n",
      "Epoch 58: Average Train Loss: 0.6482542510901113, Average Test Loss: 0.6191863146614196\n",
      "Epoch 59: Average Train Loss: 0.6479090687585994, Average Test Loss: 0.6190340109573299\n",
      "Epoch 60: Average Train Loss: 0.6475260060396977, Average Test Loss: 0.6189941445194563\n",
      "Epoch 61: Average Train Loss: 0.6471200180065353, Average Test Loss: 0.6187879791414161\n",
      "Epoch 62: Average Train Loss: 0.6467610660183709, Average Test Loss: 0.6187853394443664\n",
      "Epoch 63: Average Train Loss: 0.646397168468684, Average Test Loss: 0.6187068786384771\n",
      "Epoch 64: Average Train Loss: 0.6460382035991643, Average Test Loss: 0.6186249436199432\n",
      "Epoch 65: Average Train Loss: 0.6456839315767866, Average Test Loss: 0.6185793405398726\n",
      "Epoch 66: Average Train Loss: 0.6453584330680314, Average Test Loss: 0.6188225231355318\n",
      "Epoch 67: Average Train Loss: 0.6450909724691882, Average Test Loss: 0.6185225777953747\n",
      "Epoch 68: Average Train Loss: 0.6446765658387449, Average Test Loss: 0.6185846000644233\n",
      "Epoch 69: Average Train Loss: 0.6443245266389568, Average Test Loss: 0.6187229906863649\n",
      "Epoch 70: Average Train Loss: 0.644076086016139, Average Test Loss: 0.6190164577241376\n",
      "Epoch 71: Average Train Loss: 0.6438020582951139, Average Test Loss: 0.6186994881602004\n",
      "Epoch 72: Average Train Loss: 0.6433332148531917, Average Test Loss: 0.6186228856175149\n",
      "Early stopping at epoch 72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.6433332148531917, 'test_loss': 0.6186228856175149}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary classification between two datasets\n",
    "dataset1 = USS_data\n",
    "dataset2 = URS_data\n",
    "def generate_data(dataset1, dataset2):\n",
    "    indices = np.random.permutation(len(dataset1) + len(dataset2))\n",
    "    data = [dataset1[i] if i < len(dataset1) else dataset2[i - len(dataset1)] for i in indices]\n",
    "    for i in range(len(data)):\n",
    "        data[i].y = 1.0 if indices[i] < len(dataset1) else 0.0 # Binary labels for each node; 1 = URS, 0 = UPS\n",
    "        # Hence roughly speaking, 1 = more coherent, 0 = less coherent\n",
    "\n",
    "    train_data_ratio = 0.8\n",
    "    train_data, test_data = data[:int(train_data_ratio * len(data))], data[int(train_data_ratio * len(data)):]\n",
    "    num_node_features = data[0].x.shape[1] # Number of features for each node\n",
    "    return train_data, test_data, num_node_features\n",
    "\n",
    "train_data, test_data, num_node_features = generate_data(dataset1, dataset2)\n",
    "# Loss and optimizer\n",
    "model = GraphLevelGCN(num_node_features)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_classifier(model, criterion, optimizer, train_data, test_data, epochs = 40, patience = 3, \n",
    "                     epochs_without_improvement = 0, best_loss = float('inf')):\n",
    "    for epoch in range(epochs):\n",
    "        avg_train_loss = 0\n",
    "        for datapt in train_data:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # print(f\"datapt.x shape: {datapt.x.shape}\")  # Should be [num_nodes, num_node_features]\n",
    "            # print(f\"datapt.edge_index shape: {datapt.edge_index.shape}\")  # Should be [2, num_edges]\n",
    "            out = model.forward(datapt)\n",
    "            # print(out.size())\n",
    "            # print(torch.tensor([[datapt.y]]).size())\n",
    "            loss = criterion(out, torch.tensor([[datapt.y]]))  # Adjust shape as necessary\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_train_loss += loss.item()\n",
    "        avg_train_loss /= len(train_data)\n",
    "\n",
    "        avg_test_loss = 0\n",
    "        for datapt in test_data:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model.forward(datapt)\n",
    "                loss = criterion(out, torch.tensor([[datapt.y]]))\n",
    "                avg_test_loss += loss.item()\n",
    "        avg_test_loss /= len(test_data)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Average Train Loss: {avg_train_loss}, Average Test Loss: {avg_test_loss}')\n",
    "        \n",
    "        # Early Stopping\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "        \n",
    "    metrics = {'train_loss': avg_train_loss, 'test_loss': avg_test_loss}\n",
    "    return metrics\n",
    "\n",
    "train_classifier(model, criterion, optimizer, train_data, test_data, epochs = 100, patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6210]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.0036]], grad_fn=<SigmoidBackward0>)\n",
      "[tensor([[0.1065]], grad_fn=<SigmoidBackward0>), tensor([[0.1559]], grad_fn=<SigmoidBackward0>), tensor([[0.2046]], grad_fn=<SigmoidBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "# Test GCN model on a \"more powerful\" NN\n",
    "print(model.forward(dataset1[0]))\n",
    "print(model.forward(dataset2[0]))\n",
    "powerful_models = [nn_to_data(train_dqn(env_name = env_name, episodes = 5 * i).model) \n",
    "                   for i in [1, 3, 10]]\n",
    "print([model.forward(data) for data in powerful_models])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The classifier training process is finicky -- sometimes it overfits, sometimes it underfits -- but sometimes can reach very low loss (< 0.002)\n",
    "- Even weak classifiers classify powerful models (a.k.a. agents with >15 episodes in CartPole) as having P(URS) = 1, corresponding to coherence ~ $\\infty$\n",
    "- P(USS) / P(URS) is still having trouble as a metric; seems extremely difficult to detect differences between USS and URS-generated policies here with current methods\n",
    "    - We will need some kind of \"more advanced\" coherence metric to distinguish more advanced policies; TODO: implement UUS somehow\n",
    "- Adding node weights to every other node to the features passed into the GCN (such that, in CartPole, the data matrix has shape (134, 134) instead of (134, 1)) makes the GCN much worse, probably because of higher dimensionality\n",
    "    - Using attention in the GNN does not help, and in fact actively overfits when using (134, 1) data\n",
    "- Even with sparsity = 0.999, USS is still hard to distinguish\n",
    "- For simpler discrete environments, maybe a Q-table is enough to solve the problem\n",
    "- Takes >= 500 episodes, small epsilon to effectively learn DQN policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halfway there!\n"
     ]
    }
   ],
   "source": [
    "# Now classifying q-table agents\n",
    "env_name = \"Taxi-v3\"\n",
    "NUM_EPS_TRAIN_R = 1000\n",
    "NUM_TRAIN_R_FUNCS = 50\n",
    "NUM_REWARD_CALLS = 0\n",
    "env = gym.make(env_name)\n",
    "def deterministic_random(*args, lb = -1, ub = 1, sparsity = 0.0, continuous = False):\n",
    "    \"\"\"\n",
    "    Create a deterministic random number generator for a given set of arguments.\n",
    "    Used to generate deterministic reward functions for the coherence classifier.\n",
    "    [Edit 4/3/24: adapted to continuous state space]\"\"\"\n",
    "    global NUM_REWARD_CALLS\n",
    "    NUM_REWARD_CALLS += 1\n",
    "    unique_seed = f\"{args}\".encode(\"utf-8\")\n",
    "    random.seed(unique_seed)\n",
    "    return random.uniform(lb, ub) if random.random() > sparsity else random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "def get_state_shape(env):\n",
    "    return 1 if len(env.observation_space.shape) == 0 else env.observation_space.shape[0]\n",
    "def get_state_size(env):\n",
    "    return env.observation_space.n if len(env.observation_space.shape) == 0 else env.observation_space.shape[0]\n",
    "\n",
    "URS_r_funcs = [lambda *args: deterministic_random(args) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "URS_agents = [train_qtable(env_name = env_name, episodes=NUM_EPS_TRAIN_R, \n",
    "                           reward_function = r_func) for r_func in URS_r_funcs]\n",
    "print(\"Halfway there!\")\n",
    "USS_r_funcs = [lambda *args: deterministic_random(args, sparsity=0.99) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "USS_agents = [train_qtable(env_name = env_name, episodes=NUM_EPS_TRAIN_R,\n",
    "                            reward_function = r_func) for r_func in USS_r_funcs]\n",
    "UPS_agents = [QTableAgent(get_state_size(env), env.action_space.n) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "\n",
    "# The Q-Table is already one-hot encoded, so we don't need to convert it to a Data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.879610979477867e-10, -8.537837971656553e-10, 1.3453720557822044e-10, -6.91582481382308e-10, 5.834328244211258e-10, 2.689469001408139e-10, -7.747733415111116e-10, 3.5242825351422126e-10, 2.6150563760737437e-10, -8.919200340033589e-10]\n",
      "Episode: 50, Average total reward: 0.0053425241032739375, Epsilon: 0.62\n",
      "Maximum reward: 8\n",
      "Average reward: 0.0018690184912870031\n",
      "Maximum reward: 8\n",
      "Average reward: 2.696479094863794\n"
     ]
    }
   ],
   "source": [
    "# Test ground\n",
    "print([USS_r_funcs[0](i) for i in range(10)])\n",
    "test_USS_agent = train_qtable(env_name = env_name, episodes = 50, verbose=True, epsilon_decay = 100, \n",
    "                              lr = 0.01, gamma = 0.9, reward_function = USS_r_funcs[0])\n",
    "test_qtable(gym.make(env_name), test_USS_agent, episodes = 100, reward_function = USS_r_funcs[0])\n",
    "test_UPS_agent = QTableAgent(get_state_size(env), env.action_space.n)\n",
    "test_qtable(gym.make(env_name), test_UPS_agent, episodes = 100, reward_function = USS_r_funcs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNNBinary(nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(FCNNBinary, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_node_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "def qtable_to_feat(qtable: torch.Tensor, label):\n",
    "    # In qtable, rows are states and columns are actions taken in that state\n",
    "    return Data(x = torch.flatten(qtable), y = label) # Naive approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "Epoch 1: Average Train Loss: 0.049766413247157644, Average Test Loss: 2.836417075364506e-29\n",
      "Epoch 2: Average Train Loss: 3.910217640168412e-06, Average Test Loss: 3.8434927429074585e-29\n",
      "Epoch 3: Average Train Loss: 3.613662419610364e-06, Average Test Loss: 5.327220408929323e-29\n",
      "Epoch 4: Average Train Loss: 3.296245868611436e-06, Average Test Loss: 7.391705247613961e-29\n",
      "Early stopping at epoch 4\n"
     ]
    }
   ],
   "source": [
    "UPS_agents = [QTableAgent(get_state_size(env), env.action_space.n) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "for agent in UPS_agents:\n",
    "    for row in agent.q_table:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = np.random.uniform(-1, 1) # set each value to a random number between -1 and 1\n",
    "dataset1 = [qtable_to_feat(torch.tensor(agent.q_table, dtype=torch.float32), 1) for agent in USS_agents]\n",
    "dataset2 = [qtable_to_feat(torch.tensor(agent.q_table, dtype=torch.float32), 0) for agent in URS_agents] # URS = 1, UPS = 0\n",
    "\n",
    "def generate_fcnn_data(dataset1, dataset2):\n",
    "    indices = np.random.permutation(len(dataset1) + len(dataset2))\n",
    "    data = [dataset1[i] if i < len(dataset1) else dataset2[i - len(dataset1)] for i in indices]\n",
    "    for i in range(len(data)):\n",
    "        data[i].y = 1.0 if indices[i] < len(dataset1) else 0.0 # Binary labels for each node; 1 = URS, 0 = UPS\n",
    "        # Hence roughly speaking, 1 = more coherent, 0 = less coherent\n",
    "\n",
    "    train_data_ratio = 0.8\n",
    "    train_data, test_data = data[:int(train_data_ratio * len(data))], data[int(train_data_ratio * len(data)):]\n",
    "    num_node_features = data[0].x.shape[0] # Number of features for each node\n",
    "    return train_data, test_data, num_node_features\n",
    "\n",
    "def train_fcnn_classifier(model, criterion, optimizer, train_data, test_data, epochs = 40, patience = 3, \n",
    "                          epochs_without_improvement = 0, best_loss = float('inf')):\n",
    "    for epoch in range(epochs):\n",
    "        avg_train_loss = 0\n",
    "        for datapt in train_data:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model.forward(datapt)\n",
    "            assert isinstance(out, torch.Tensor), f\"Expected model.forward to return a tensor, but got {out}\"\n",
    "            loss = criterion(out, torch.tensor([datapt.y]))  # Adjust shape as necessary\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_train_loss += loss.item()\n",
    "        avg_train_loss /= len(train_data)\n",
    "\n",
    "        avg_test_loss = 0\n",
    "        for datapt in test_data:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model.forward(datapt)\n",
    "                loss = criterion(out, torch.tensor([datapt.y]))\n",
    "                avg_test_loss += loss.item()\n",
    "        avg_test_loss /= len(test_data)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Average Train Loss: {avg_train_loss}, Average Test Loss: {avg_test_loss}')\n",
    "        \n",
    "        # Early Stopping\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "train_data, test_data, num_node_features = generate_fcnn_data(dataset1, dataset2)\n",
    "print(num_node_features)\n",
    "model = FCNNBinary(num_node_features)\n",
    "criterion = torch.nn.BCELoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "train_fcnn_classifier(model, criterion, optimizer, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     r_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m s, a, \u001b[38;5;241m*\u001b[39margs: r_table[s, a]\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_qtable(env_name \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mspec\u001b[38;5;241m.\u001b[39mid, episodes \u001b[38;5;241m=\u001b[39m episodes, reward_function \u001b[38;5;241m=\u001b[39m r_func)\n\u001b[1;32m---> 18\u001b[0m UQS_agents \u001b[38;5;241m=\u001b[39m [generate_UQS_qagent(agent\u001b[38;5;241m.\u001b[39mq_table, \u001b[38;5;241m0.9\u001b[39m, env, episodes \u001b[38;5;241m=\u001b[39m NUM_EPS_TRAIN_R) \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m UPS_agents]\n",
      "Cell \u001b[1;32mIn[37], line 18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     15\u001b[0m     r_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m s, a, \u001b[38;5;241m*\u001b[39margs: r_table[s, a]\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_qtable(env_name \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mspec\u001b[38;5;241m.\u001b[39mid, episodes \u001b[38;5;241m=\u001b[39m episodes, reward_function \u001b[38;5;241m=\u001b[39m r_func)\n\u001b[1;32m---> 18\u001b[0m UQS_agents \u001b[38;5;241m=\u001b[39m [\u001b[43mgenerate_UQS_qagent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNUM_EPS_TRAIN_R\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m UPS_agents]\n",
      "Cell \u001b[1;32mIn[37], line 16\u001b[0m, in \u001b[0;36mgenerate_UQS_qagent\u001b[1;34m(rand_qtable, gamma, env, episodes)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m     15\u001b[0m r_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m s, a, \u001b[38;5;241m*\u001b[39margs: r_table[s, a]\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_qtable\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_function\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mr_func\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m, in \u001b[0;36mtrain_qtable\u001b[1;34m(env_name, episodes, epsilon_start, epsilon_final, epsilon_decay, reward_function, verbose, return_reward, print_every, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reward_function:\n\u001b[0;32m     25\u001b[0m     reward \u001b[38;5;241m=\u001b[39m reward_function(done, state, action, next_state)\n\u001b[1;32m---> 27\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     29\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[2], line 100\u001b[0m, in \u001b[0;36mQTableAgent.update\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     97\u001b[0m next_q_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[next_state])\n\u001b[0;32m     98\u001b[0m expected_q_value \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m next_q_value \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m done)\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m*\u001b[39m (expected_q_value \u001b[38;5;241m-\u001b[39m q_value)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "def generate_UQS_qagent(rand_qtable, gamma, env: gym.Env, episodes = 500):\n",
    "    \"\"\"\n",
    "    Train a Q-table agent based on a reward function uniformly sampled from the set of \n",
    "    possible reward functions compatible with the given random Q-table.\"\"\"\n",
    "    # Generate the reward function using the Bellman equation\n",
    "    r_table = np.zeros(rand_qtable.shape)\n",
    "    for s in range(rand_qtable.shape[0]):\n",
    "        for a in range(rand_qtable.shape[1]):\n",
    "            env.reset()\n",
    "            env.unwrapped.s = s\n",
    "            ns = env.step(a)[0]\n",
    "            r_table[s, a] = rand_qtable[s, a] - gamma * np.max(rand_qtable[ns]) #assuming greedy policy\n",
    "    \n",
    "    # Train the agent\n",
    "    r_func = lambda s, a, *args: r_table[s, a]\n",
    "    return train_qtable(env_name = env.spec.id, episodes = episodes, reward_function = r_func)\n",
    "\n",
    "UQS_agents = [generate_UQS_qagent(agent.q_table, 0.9, env, episodes = NUM_EPS_TRAIN_R) for agent in UPS_agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_UVS_qagent(rand_values, gamma, env: gym.Env, episodes = 500, lb = -1, ub = 1):\n",
    "    \"\"\"\n",
    "    Train a Q-table agent based on a reward function uniformly sampled from the set of \n",
    "    possible reward functions compatible with the given values for each state.\n",
    "    Assumes a uniform distribution between [lb, ub].\"\"\"\n",
    "    r_table = np.zeros((len(rand_values), env.action_space.n))\n",
    "    for s in range(len(rand_values)):\n",
    "        next_states = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            env.reset()\n",
    "            env.unwrapped.s = s\n",
    "            next_states[a] = env.step(a)[0]\n",
    "        #v(s) = max_a(R(s, a) + gamma * v(s'))\n",
    "        reward_ub = rand_values[s] - np.array([gamma * rand_values[int(ns)] for ns in next_states])\n",
    "        taut_probs = np.zeros(env.action_space.n)\n",
    "        for i in range(env.action_space.n):\n",
    "            all_except_i = np.delete(np.arange(env.action_space.n), i)\n",
    "            taut_probs[i] = np.prod((reward_ub[all_except_i] + 1) / 2)\n",
    "            # probability that all other rewards at action j are less than reward_ub[j]\n",
    "        taut_probs /= np.sum(taut_probs)\n",
    "        taut = np.random.choice(env.action_space.n, p = taut_probs) \n",
    "        #index of the action where the reward is equal to the maximum\n",
    "\n",
    "        rewards = np.full(env.action_space.n, float('inf'))\n",
    "        while np.any(rewards >= reward_ub): #while any of the rewards are greater than the upper bound\n",
    "            rewards = np.random.uniform(-1, 1, env.action_space.n)\n",
    "        rewards[taut] = reward_ub[taut]\n",
    "        r_table[s] = rewards\n",
    "    \n",
    "    r_func = lambda s, a, *args: r_table[s, a]\n",
    "    return train_qtable(env_name = env.spec.id, episodes = episodes, reward_function = r_func)\n",
    "\n",
    "# UVS_agents = [generate_UVS_qagent(np.random.uniform(-1, 1, env.unwrapped.s), 0.9, env, episodes = NUM_EPS_TRAIN_R) for _ in range(NUM_TRAIN_R_FUNCS)]\n",
    "# this currently takes way too long so it has been commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UQS_agents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mUQS_agents\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mq_table \u001b[38;5;241m-\u001b[39m UPS_agents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mq_table\n",
      "\u001b[1;31mNameError\u001b[0m: name 'UQS_agents' is not defined"
     ]
    }
   ],
   "source": [
    "UQS_agents[0].q_table - UPS_agents[0].q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only attaching reward to terminal states (kind of like UUS? but with the inductive biases)\n",
    "\n",
    "def det_rand_terminal(done: bool, *args, lb = -1, ub = 1, sparsity = 0.0):\n",
    "    \"\"\"\n",
    "    Create a deterministic random number generator for a given set of arguments.\n",
    "    Used to generate deterministic reward functions for the coherence classifier. \"\"\"\n",
    "    global NUM_REWARD_CALLS\n",
    "    NUM_REWARD_CALLS += 1\n",
    "    if not done:\n",
    "        return random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "    unique_seed = f\"{args}\".encode(\"utf-8\")\n",
    "    random.seed(unique_seed)\n",
    "    return random.uniform(lb, ub) if random.random() > sparsity else random.uniform(-NEAR_ZERO, NEAR_ZERO)\n",
    "\n",
    "UUS_agents = [train_qtable(env_name = env_name, episodes = NUM_EPS_TRAIN_R, \n",
    "                           reward_function = lambda *args: det_rand_terminal(*args)) for _ in range(NUM_TRAIN_R_FUNCS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0,   0,   0,  ..., 499, 499, 499],\n",
       "         [100,   0,  20,  ..., 479, 499, 499]]),\n",
       " torch.Size([2, 3000]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Turn the state and action space of Taxi-v3 into a graph\n",
    "\n",
    "from collections import defaultdict\n",
    "taxi_env = gym.make(\"Taxi-v3\")\n",
    "taxi_env.reset()\n",
    "# Initialize containers for graph data\n",
    "edges = defaultdict(list)\n",
    "edge_attr = defaultdict(list)\n",
    "\n",
    "# A helper function to encode the state into a single number (node index)\n",
    "def state_to_node(taxi_row, taxi_col, pass_loc, dest_idx):\n",
    "    # This encoding assumes specific knowledge about the Taxi-v3 state space size\n",
    "    return taxi_row * 100 + taxi_col * 20 + pass_loc * 4 + dest_idx\n",
    "    # max = 4 * 100 + 4 * 20 + 4 * 4 + 3 = 400 + 80 + 16 + 3 = 499\n",
    "\n",
    "# Iterate through all possible states and actions to construct the graph\n",
    "for taxi_row in range(5):\n",
    "    for taxi_col in range(5):\n",
    "        for pass_loc in range(5):  # 4 locations + 1 for 'in taxi'\n",
    "            for dest_idx in range(4):\n",
    "                current_state = state_to_node(taxi_row, taxi_col, pass_loc, dest_idx)\n",
    "                for action in range(taxi_env.action_space.n):\n",
    "                    # Set the environment to the current state\n",
    "                    taxi_env.unwrapped.s = current_state\n",
    "                    # Take action and observe the next state and reward\n",
    "                    next_state, reward, done, _ = taxi_env.step(action)\n",
    "                    # Add edge from current state to next state\n",
    "                    edges[current_state].append(next_state)\n",
    "                    # Optionally, use rewards as edge attributes\n",
    "                    # edge_attr[(current_state, next_state)].append(reward)\n",
    "                    taxi_env.reset()\n",
    "\n",
    "\n",
    "# Convert edges and edge attributes to tensors\n",
    "edge_index = []\n",
    "for src, dsts in edges.items():\n",
    "    for dst in dsts:\n",
    "        edge_index.append([src, dst])\n",
    "edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "edge_index, edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1])\n",
      "Epoch 1: Average Train Loss: 0.6975951328873634, Average Test Loss: 0.6983259975910187\n",
      "Epoch 2: Average Train Loss: 0.6948948062956333, Average Test Loss: 0.6966440558433533\n",
      "Epoch 3: Average Train Loss: 0.6944274820387364, Average Test Loss: 0.6957786858081818\n",
      "Epoch 4: Average Train Loss: 0.6941709764301777, Average Test Loss: 0.695288798213005\n",
      "Epoch 5: Average Train Loss: 0.6939983785152435, Average Test Loss: 0.6949822187423706\n",
      "Epoch 6: Average Train Loss: 0.693861399590969, Average Test Loss: 0.6947700291872024\n",
      "Epoch 7: Average Train Loss: 0.6937379993498325, Average Test Loss: 0.6945965200662613\n",
      "Epoch 8: Average Train Loss: 0.6936026096343995, Average Test Loss: 0.6944513291120529\n",
      "Epoch 9: Average Train Loss: 0.6934472382068634, Average Test Loss: 0.6942998260259629\n",
      "Epoch 10: Average Train Loss: 0.6932599917054176, Average Test Loss: 0.6941128522157669\n",
      "Epoch 11: Average Train Loss: 0.6930280290544033, Average Test Loss: 0.6938772439956665\n",
      "Epoch 12: Average Train Loss: 0.6927455887198448, Average Test Loss: 0.6935957819223404\n",
      "Epoch 13: Average Train Loss: 0.692417661100626, Average Test Loss: 0.6932632893323898\n",
      "Epoch 14: Average Train Loss: 0.6920402996242047, Average Test Loss: 0.692882576584816\n",
      "Epoch 15: Average Train Loss: 0.6916092857718468, Average Test Loss: 0.6924319624900818\n",
      "Epoch 16: Average Train Loss: 0.6911199890077114, Average Test Loss: 0.6919016540050507\n",
      "Epoch 17: Average Train Loss: 0.6905594564974308, Average Test Loss: 0.6912597715854645\n",
      "Epoch 18: Average Train Loss: 0.6899037569761276, Average Test Loss: 0.6904739439487457\n",
      "Epoch 19: Average Train Loss: 0.689119715988636, Average Test Loss: 0.6894990980625153\n",
      "Epoch 20: Average Train Loss: 0.6881693661212921, Average Test Loss: 0.6882845103740692\n",
      "Epoch 21: Average Train Loss: 0.6870088540017605, Average Test Loss: 0.6867828339338302\n",
      "Epoch 22: Average Train Loss: 0.6855955228209496, Average Test Loss: 0.6850064843893051\n",
      "Epoch 23: Average Train Loss: 0.6838970370590687, Average Test Loss: 0.6828623265028\n",
      "Epoch 24: Average Train Loss: 0.6818548530340195, Average Test Loss: 0.6803435981273651\n",
      "Epoch 25: Average Train Loss: 0.6794375315308571, Average Test Loss: 0.6775054693222046\n",
      "Epoch 26: Average Train Loss: 0.6766187205910683, Average Test Loss: 0.6741738438606262\n",
      "Epoch 27: Average Train Loss: 0.6733324140310287, Average Test Loss: 0.6703402072191238\n",
      "Epoch 28: Average Train Loss: 0.6695610150694847, Average Test Loss: 0.6659465581178665\n",
      "Epoch 29: Average Train Loss: 0.6652394659817219, Average Test Loss: 0.6609788596630096\n",
      "Epoch 30: Average Train Loss: 0.6603062294423581, Average Test Loss: 0.6552272409200668\n",
      "Epoch 31: Average Train Loss: 0.6547067448496818, Average Test Loss: 0.6487428963184356\n",
      "Epoch 32: Average Train Loss: 0.6483922615647316, Average Test Loss: 0.6414867758750915\n",
      "Epoch 33: Average Train Loss: 0.6413609579205513, Average Test Loss: 0.6333971619606018\n",
      "Epoch 34: Average Train Loss: 0.6335359700024128, Average Test Loss: 0.6243275076150894\n",
      "Epoch 35: Average Train Loss: 0.6248559549450874, Average Test Loss: 0.6143407136201858\n",
      "Epoch 36: Average Train Loss: 0.6152744226157665, Average Test Loss: 0.6033609807491302\n",
      "Epoch 37: Average Train Loss: 0.6047892205417156, Average Test Loss: 0.591550874710083\n",
      "Epoch 38: Average Train Loss: 0.5935118988156318, Average Test Loss: 0.5787768542766571\n",
      "Epoch 39: Average Train Loss: 0.5813402574509382, Average Test Loss: 0.56479681879282\n",
      "Epoch 40: Average Train Loss: 0.5681799709796905, Average Test Loss: 0.5500812023878098\n",
      "Epoch 41: Average Train Loss: 0.5543025769293308, Average Test Loss: 0.5345723643898964\n",
      "Epoch 42: Average Train Loss: 0.5397084858268499, Average Test Loss: 0.5181527182459831\n",
      "Epoch 43: Average Train Loss: 0.5244843598455191, Average Test Loss: 0.5007727518677711\n",
      "Epoch 44: Average Train Loss: 0.5082670856267214, Average Test Loss: 0.4830486163496971\n",
      "Epoch 45: Average Train Loss: 0.4917604118585587, Average Test Loss: 0.46433161944150925\n",
      "Epoch 46: Average Train Loss: 0.47455446384847166, Average Test Loss: 0.4451420962810516\n",
      "Epoch 47: Average Train Loss: 0.45697026774287225, Average Test Loss: 0.42563409358263016\n",
      "Epoch 48: Average Train Loss: 0.43913168720901014, Average Test Loss: 0.4059660002589226\n",
      "Epoch 49: Average Train Loss: 0.4210963321849704, Average Test Loss: 0.3858929693698883\n",
      "Epoch 50: Average Train Loss: 0.4029679665341973, Average Test Loss: 0.36618570908904075\n",
      "Epoch 51: Average Train Loss: 0.38490742575377224, Average Test Loss: 0.3464068576693535\n",
      "Epoch 52: Average Train Loss: 0.3669580852612853, Average Test Loss: 0.3270384341478348\n",
      "Epoch 53: Average Train Loss: 0.3492967592552304, Average Test Loss: 0.30799212828278544\n",
      "Epoch 54: Average Train Loss: 0.33197048828005793, Average Test Loss: 0.28975395038723945\n",
      "Epoch 55: Average Train Loss: 0.3150578156113625, Average Test Loss: 0.27210638448596003\n",
      "Epoch 56: Average Train Loss: 0.29867695160210134, Average Test Loss: 0.25510840117931366\n",
      "Epoch 57: Average Train Loss: 0.2828294176608324, Average Test Loss: 0.23885227888822555\n",
      "Epoch 58: Average Train Loss: 0.26758682494983077, Average Test Loss: 0.22341961786150932\n",
      "Epoch 59: Average Train Loss: 0.2529847246594727, Average Test Loss: 0.20873028449714184\n",
      "Epoch 60: Average Train Loss: 0.2390190836042166, Average Test Loss: 0.19493887200951576\n",
      "Epoch 61: Average Train Loss: 0.22570209829136728, Average Test Loss: 0.1819298703223467\n",
      "Epoch 62: Average Train Loss: 0.21310257455334067, Average Test Loss: 0.16968909613788127\n",
      "Epoch 63: Average Train Loss: 0.2011760462075472, Average Test Loss: 0.15822813734412194\n",
      "Epoch 64: Average Train Loss: 0.18987124962732196, Average Test Loss: 0.14750651307404042\n",
      "Epoch 65: Average Train Loss: 0.1792005633469671, Average Test Loss: 0.1375169802457094\n",
      "Epoch 66: Average Train Loss: 0.16919098771177232, Average Test Loss: 0.12819102145731448\n",
      "Epoch 67: Average Train Loss: 0.15974083188921212, Average Test Loss: 0.11951070167124271\n",
      "Epoch 68: Average Train Loss: 0.150801442284137, Average Test Loss: 0.11144692171365023\n",
      "Epoch 69: Average Train Loss: 0.14244582541286946, Average Test Loss: 0.103930607996881\n",
      "Epoch 70: Average Train Loss: 0.1345645073801279, Average Test Loss: 0.09697564840316772\n",
      "Epoch 71: Average Train Loss: 0.12719628028571606, Average Test Loss: 0.0905088072642684\n",
      "Epoch 72: Average Train Loss: 0.12027698082383723, Average Test Loss: 0.0845042835921049\n",
      "Epoch 73: Average Train Loss: 0.11379585738759487, Average Test Loss: 0.07893557958304882\n",
      "Epoch 74: Average Train Loss: 0.10770462455693633, Average Test Loss: 0.07378249261528254\n",
      "Epoch 75: Average Train Loss: 0.10200068987905979, Average Test Loss: 0.06899648727849125\n",
      "Epoch 76: Average Train Loss: 0.09665523031726479, Average Test Loss: 0.06455536969006062\n",
      "Epoch 77: Average Train Loss: 0.09163154670968651, Average Test Loss: 0.06044626738876104\n",
      "Epoch 78: Average Train Loss: 0.08692679008236155, Average Test Loss: 0.05662667918950319\n",
      "Epoch 79: Average Train Loss: 0.08251032148255036, Average Test Loss: 0.053083180263638494\n",
      "Epoch 80: Average Train Loss: 0.07837411074433476, Average Test Loss: 0.04979511229321361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.07837411074433476, 'test_loss': 0.04979511229321361}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def greedy_policy(q_table):\n",
    "    return torch.tensor(np.argmax(q_table, axis=1).reshape(-1, 1).astype(np.float32))\n",
    "def random_policy(state_dim):\n",
    "    return torch.randint(0, 6, (state_dim, 1)).float()\n",
    "def prep_qtable(q_table):\n",
    "    return torch.tensor(q_table, dtype=torch.float32)\n",
    "\n",
    "dataset1 = [Data(x = greedy_policy(agent.q_table), edge_index = edge_index, y = 1) for agent in UUS_agents]\n",
    "dataset2 = [Data(x = greedy_policy(agent.q_table), edge_index = edge_index, y = 0) for agent in URS_agents]\n",
    "# dataset2 = [Data(x = random_policy(agent.q_table.shape[0]), edge_index = edge_index, y = 0) for agent in UPS_agents]\n",
    "# ^ random_policy = UPS sampling\n",
    "print(dataset1[0].x.shape)\n",
    "\n",
    "train_data, test_data, num_node_features = generate_data(dataset1, dataset2)\n",
    "model = GraphLevelGCN(num_node_features)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "train_classifier(model, criterion, optimizer, train_data, test_data, epochs = 80, patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "def train(project_name: str, model, criterion, train_data, test_data):\n",
    "    wandb.init(project=project_name)\n",
    "    sweep_config = {\n",
    "        \"method\": \"random\",\n",
    "        \"parameters\": {\n",
    "            \"lr\": {\n",
    "                \"distribution\": \"log_uniform_values\", \n",
    "                \"min\": 1e-4,\n",
    "                \"max\": 1e-1\n",
    "            },\n",
    "            \"weight_decay\": {\n",
    "                \"values\": [5e-4, 1e-4]\n",
    "            },\n",
    "            \"epochs\": {\"values\": [10, 20, 40, 80]}\n",
    "        },\n",
    "        \"metric\": {\"goal\": \"minimize\", \"name\": \"test_loss\"}\n",
    "    }\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config[\"lr\"], weight_decay=wandb.config[\"weight-decay\"])\n",
    "    metrics = train_classifier(model, criterion, optimizer, train_data, test_data)\n",
    "    wandb.log(metrics)\n",
    "wandb.log({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 2., 3., 0., 0., 0., 2., 1.]])\n",
      "tensor([[0., 0., 3., 0., 0., 0., 0., 0., 0., 3.]])\n",
      "tensor([[0.9858]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.0830]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.transpose(dataset1[0].x, 0, -1)[0:10, 0:10]) # Example greedy policies\n",
    "print(torch.transpose(dataset2[0].x, 0, -1)[0:10, 0:10])\n",
    "print(model.forward(dataset1[0]))\n",
    "print(model.forward(dataset2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2000, Average total reward: -298.0345172586293, Epsilon: 0.03\n",
      "Episode: 4000, Average total reward: -98.47173586793397, Epsilon: 0.01\n",
      "Episode: 6000, Average total reward: -49.28064032016008, Epsilon: 0.01\n",
      "Episode: 8000, Average total reward: -19.100550275137568, Epsilon: 0.01\n",
      "Episode: 10000, Average total reward: -3.5672836418209104, Epsilon: 0.01\n",
      "Episode: 12000, Average total reward: 2.729864932466233, Epsilon: 0.01\n",
      "Episode: 14000, Average total reward: 5.246623311655828, Epsilon: 0.01\n",
      "Episode: 16000, Average total reward: 6.6213106553276635, Epsilon: 0.01\n",
      "Episode: 18000, Average total reward: 7.011505752876438, Epsilon: 0.01\n",
      "Episode: 20000, Average total reward: 7.078039019509755, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_19872\\742924666.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  taxi_data = Data(x = torch.tensor(greedy_policy(taxi_model.q_table)), edge_index = edge_index)\n"
     ]
    }
   ],
   "source": [
    "taxi_model = train_qtable(env_name = \"Taxi-v3\", episodes = 20000, verbose = True, print_every = 2000, \n",
    "                          return_reward = False)\n",
    "taxi_data = Data(x = torch.tensor(greedy_policy(taxi_model.q_table)), edge_index = edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 4., 4., 4., 3., 0., 2., 0., 0., 0., 0., 0., 1., 0., 0., 0., 5., 0.,\n",
      "         0., 0., 0., 3., 3., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         3., 0., 3., 0., 0., 0., 0., 0., 2., 0., 2., 2., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 2., 0., 1., 0., 0., 0., 0., 2., 0., 2., 2., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 2., 0., 0., 0., 2., 2., 3., 4., 0., 4., 4., 1., 2.,\n",
      "         0., 4., 0., 0., 0., 0., 3., 5., 3., 0., 0., 1., 1., 1., 2., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 2., 0., 0., 0., 1., 0., 0., 0., 0., 3., 3., 3., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 3., 0., 0., 0., 0., 0.,\n",
      "         2., 0., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0.,\n",
      "         0., 0., 2., 0., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0.,\n",
      "         0., 3., 3., 3., 1., 0., 1., 1., 3., 3., 0., 3., 3., 3., 3., 0., 3., 1.,\n",
      "         3., 3., 0., 1., 1., 1., 2., 0., 2., 2., 0., 0., 0., 0., 2., 2., 2., 0.,\n",
      "         1., 2., 0., 2., 0., 1., 1., 1., 2., 0., 2., 2., 3., 3., 0., 3., 2., 2.,\n",
      "         2., 0., 3., 2., 3., 2., 0., 3., 3., 3., 1., 0., 2., 2., 3., 3., 0., 3.,\n",
      "         2., 2., 2., 0., 3., 2., 3., 2., 0., 3., 3., 3., 1., 0., 1., 1., 3., 3.,\n",
      "         0., 3., 0., 0., 0., 0., 3., 2., 3., 0., 0., 3., 3., 3., 1., 0., 1., 1.,\n",
      "         3., 3., 0., 3., 3., 3., 3., 0., 3., 1., 3., 3., 0., 1., 1., 1., 1., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "         1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "         1., 0., 0., 3., 3., 3., 1., 0., 1., 1., 3., 3., 0., 3., 3., 3., 3., 0.,\n",
      "         3., 1., 3., 3., 0., 1., 1., 1., 2., 0., 3., 0., 4., 4., 0., 4., 2., 1.,\n",
      "         1., 0., 1., 1., 5., 1., 0., 1., 1., 1., 3., 0., 3., 1., 1., 1., 0., 1.,\n",
      "         1., 1., 1., 0., 2., 0., 2., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "         0., 1., 1., 1., 1., 0., 2., 1., 0., 2., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
      "         1., 1., 0., 1., 4., 4., 4., 0., 1., 1., 1., 5., 0., 0., 2., 1., 1., 0.,\n",
      "         1., 1., 1., 1., 0., 2., 3., 3., 3., 0., 3., 1., 3., 3.]])\n",
      "tensor([[0.6893]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(taxi_data.x.T)\n",
    "print(model.forward(taxi_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum reward: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: -9.875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [-3.45631803, -3.40184505, -3.44935417, -3.30738336,  9.61017345,\n",
       "        -4.00532755],\n",
       "       [-1.32423015, -1.4053787 , -1.69223727, -1.42928145, 14.1179476 ,\n",
       "        -3.41292327],\n",
       "       ...,\n",
       "       [-0.79663486, -0.68870221, -0.79279359, -0.79716306, -2.42284272,\n",
       "        -2.02425586],\n",
       "       [-2.18994752, -2.18745726, -2.19043425, -1.79490775, -2.2865236 ,\n",
       "        -2.63298791],\n",
       "       [-0.04946138, -0.05038011, -0.06737791,  2.01012662, -0.58588332,\n",
       "        -0.58404418]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_qtable(gym.make(\"Taxi-v3\"), taxi_model, episodes = 1000)\n",
    "taxi_model.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, parent=None, action=None, q_values=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action = action  # Action taken to reach this node\n",
    "        self.children = []\n",
    "        self.visits = 1  # Initialize to avoid division by zero\n",
    "        self.value = 0\n",
    "        self.q_values = q_values  # This should be a dictionary or similar structure\n",
    "\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def update(self, reward):\n",
    "        self.visits += 1\n",
    "        self.value += reward\n",
    "\n",
    "    def is_fully_expanded(self, env):\n",
    "        return len(self.children) == env.action_space.n\n",
    "\n",
    "    def best_child(self, c_param=1.4):\n",
    "        choices_weights = [\n",
    "            (child.value / child.visits) + c_param * np.sqrt((2 * np.log(self.visits) / child.visits))\n",
    "            for child in self.children\n",
    "        ]\n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "\n",
    "def rollout_policy(state, q_table, env):\n",
    "    # Use the Q-table to select the best action if this state has been seen\n",
    "    if state in q_table:\n",
    "        return np.argmax(q_table[state])\n",
    "    else:\n",
    "        # Otherwise, select a random action\n",
    "        return env.action_space.sample()\n",
    "\n",
    "def selection(node, env):\n",
    "    while not node.is_fully_expanded(env):\n",
    "        if not node.children:\n",
    "            return expansion(node, env)\n",
    "        else:\n",
    "            node = node.best_child()\n",
    "    return node\n",
    "\n",
    "def expansion(node, env):\n",
    "    tried_actions = [child.action for child in node.children]\n",
    "    for action in range(env.action_space.n):\n",
    "        if action not in tried_actions:\n",
    "            env.env.s = node.state  # Set environment to current node's state\n",
    "            next_state, _, _, _ = env.step(action)\n",
    "            new_node = Node(next_state, parent=node, action=action, q_values=node.q_values)\n",
    "            node.add_child(new_node)\n",
    "            return new_node\n",
    "    return node  # In case all actions were tried\n",
    "\n",
    "def simulation(node, env, max_steps=100):\n",
    "    total_reward = 0\n",
    "    current_state = node.state\n",
    "    steps = 0\n",
    "\n",
    "    while steps < max_steps:\n",
    "        action = rollout_policy(current_state, node.q_values, env)\n",
    "        env.env.s = current_state\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        current_state = next_state\n",
    "        steps += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "def backpropagation(node, reward):\n",
    "    while node is not None:\n",
    "        node.update(reward)\n",
    "        node = node.parent\n",
    "\n",
    "def mcts(root, env, iterations=1000):\n",
    "    for _ in range(iterations):\n",
    "        leaf = selection(root, env)\n",
    "        reward = simulation(leaf, env)\n",
    "        backpropagation(leaf, reward)\n",
    "\n",
    "# Example usage\n",
    "env_name = \"Taxi-v3\"\n",
    "env = gym.make(env_name)\n",
    "initial_state = env.reset()\n",
    "\n",
    "# Assume taxi_model.q_table is your pre-trained Q-table\n",
    "# It should be a dictionary where keys are states and values are arrays of Q-values for each action\n",
    "q_table = taxi_model.q_table  # Replace this with your actual Q-table\n",
    "\n",
    "root_node = Node(initial_state, q_values=q_table)\n",
    "mcts(root_node, env, iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward from the MCTS policy: -200.0\n",
      "Maximum reward: 8\n",
      "Average reward: -8.52\n"
     ]
    }
   ],
   "source": [
    "### Test MCTS\n",
    "\n",
    "def choose_action(node):\n",
    "    # Choose the child with the highest visit count\n",
    "    if node.children:\n",
    "        return max(node.children, key=lambda child: child.visits).action\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def simulate_episode_from_root(env, root_node):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    current_node = root_node\n",
    "    env.reset()\n",
    "    env.env.s = current_node.state\n",
    "    \n",
    "    while not done and current_node is not None:\n",
    "        action = choose_action(current_node)\n",
    "        if action is None:\n",
    "            # No more information in the tree; choose random action\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)  # Execute the chosen action\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Move to the next node in the tree, if it exists\n",
    "        next_node = None\n",
    "        for child in current_node.children:\n",
    "            if child.action == action:\n",
    "                next_node = child\n",
    "                break\n",
    "        current_node = next_node\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# Test the policy derived from the MCTS root node\n",
    "env = gym.make('Taxi-v3')\n",
    "average_reward = np.mean([simulate_episode_from_root(env, root_node) for _ in range(100)])\n",
    "print(f\"Average Reward from the MCTS policy: {average_reward}\")\n",
    "test_qtable(env, taxi_model, episodes = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GNNs over the environment work really well, even on USS/URS and UUS/URS (identifying \"sparsity\")\n",
    "    - Real Taxi agents are graded strongly towards USS and UUS end\n",
    "    - Looking at the q-tables, there are some noticible differences (e.g. USS q-tables tend to have lower magnitude)\n",
    "    - GNNs don't work yet on USS/URS when only the policy is passed in\n",
    "    - Started working once I put the fix in of changing the reward function of the environment *for each state*\n",
    "- Luckily the classification of a good Taxi agent (3000 eps) under USS/URS is not too high (p = 0.9865)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate tabular policy from MCTS and feed through classifier\n",
    "\n",
    "def extract_policy(root_node, env):\n",
    "    policy = np.random.randint(0, env.action_space.n, env.observation_space.n)\n",
    "    # default action is random in case the state is not in the tree\n",
    "    node_queue = [root_node]\n",
    "    \n",
    "    num_not_random = 0\n",
    "    while node_queue:\n",
    "        num_not_random += 1\n",
    "        current_node = node_queue.pop(0)\n",
    "        if current_node.is_fully_expanded(env):\n",
    "            best_action = current_node.best_child().action\n",
    "            policy[current_node.state] = best_action\n",
    "            node_queue.extend(current_node.children)\n",
    "        else:\n",
    "            # If the node isn't fully expanded, we take the best action tried so far\n",
    "            # This is rare in fully run MCTS but can happen if the tree isn't deep enough\n",
    "            if current_node.children:\n",
    "                best_action = max(current_node.children, key=lambda x: x.visits).action\n",
    "                policy[current_node.state] = best_action\n",
    "                node_queue.extend(current_node.children)\n",
    "\n",
    "    return policy, num_not_random  \n",
    "\n",
    "mcts_policy, num_not_random = extract_policy(root_node, env)\n",
    "model.forward(Data(x = torch.tensor(mcts_policy.reshape(-1, 1).astype(np.float32)), edge_index = edge_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "tensor([[0.6893]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6078]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### A la Wentworth's definition of coherence, we create policies that do and do not \"contradict\"\n",
    "# themselves, i.e. there exists a value function consistent with the policy, and pass them\n",
    "# through the classifier\n",
    "\n",
    "coherent_policy = greedy_policy(taxi_model.q_table).detach()\n",
    "incoherent_policy = greedy_policy(taxi_model.q_table).detach()\n",
    "for _ in range(100):\n",
    "    env.reset()\n",
    "    i = env.unwrapped.s # +100 for moving one row, + 20 for moving one column\n",
    "    env.step(coherent_policy[i].item())\n",
    "    j = env.unwrapped.s\n",
    "    if coherent_policy[i][0] % 2 == 0:\n",
    "        incoherent_policy[j][0] = coherent_policy[i][0] + 1\n",
    "    else:\n",
    "        incoherent_policy[j][0] = coherent_policy[i][0] - 1 # if 0, then 1; if 1, then 0\n",
    "    # point is to put incoherent_policy in a loop\n",
    "\n",
    "print((coherent_policy != incoherent_policy).nonzero().shape[0])\n",
    "print(model.forward(Data(x = coherent_policy.detach(), edge_index = edge_index)))\n",
    "print(model.forward(Data(x = incoherent_policy.detach(), edge_index = edge_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum reward: 8\n",
      "Average reward: -6.553\n",
      "Maximum reward: 8\n",
      "Average reward: -182.233\n"
     ]
    }
   ],
   "source": [
    "class PolicyAgent:\n",
    "    def __init__(self, policy, epsilon = 0.1):\n",
    "        self.policy, self.epsilon = policy, epsilon\n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            action = self.policy[state]\n",
    "        else:\n",
    "            action = random.randrange(self.action_dim)\n",
    "        return action\n",
    "    \n",
    "c_agent, ic_agent = PolicyAgent(np.array(coherent_policy.T[0])), PolicyAgent(np.array(incoherent_policy.T[0]))\n",
    "test_qtable(env, c_agent, episodes = 1000)\n",
    "test_qtable(env, ic_agent, episodes = 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
